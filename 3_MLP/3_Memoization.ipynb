{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5186eb68-92be-4da2-ac80-d64e7d99a1e7",
   "metadata": {},
   "source": [
    "### MLP Memoization \n",
    "lecture 18 dl playlist\n",
    "\n",
    "This video introduces the concept of **Memoization** and explains its crucial role in the **Backpropagation algorithm**, especially when dealing with neural networks that have **multiple hidden layers**. It also addresses the increased complexity of derivative calculations in such deep networks.\n",
    "\n",
    "#### 1. Introduction to Memoization\n",
    "\n",
    "*   **Definition**: Memoization is an **optimization technique** used primarily in computing to **speed up computer programs**.\n",
    "*   **Mechanism**: It works by **storing the results of \"expensive function calls\"** and then **returning the cached (stored) result** when the same input occurs again, rather than re-calculating it.\n",
    "*   **Benefits**:\n",
    "    *   **Speeds up program execution** and reduces computation time.\n",
    "*   **Trade-offs**:\n",
    "    *   Requires **additional memory** to store the computed results. This is often considered a worthwhile trade-off in computer science (\"space-time trade-off\").\n",
    "*   **Context**: It is a well-known technique in computer science, particularly within **dynamic programming**.\n",
    "\n",
    "#### 2. Illustrative Example: Fibonacci Sequence\n",
    "\n",
    "The video uses the calculation of the Fibonacci sequence to demonstrate the inefficiency of a naive recursive approach and the benefits of memoization.\n",
    "\n",
    "*   **Naive Recursive Approach (Highly Inefficient)**:\n",
    "    *   A simple recursive function `fib(n)` to calculate the `n`-th Fibonacci number `fib(n-1) + fib(n-2)`.\n",
    "    *   **Problem**: This code has an **exponential time complexity** (roughly 2^n), meaning computation time increases drastically with `n`. For example, `fib(38)` takes seconds, `fib(40)` minutes, and `fib(100)` could take months.\n",
    "    *   **Reason for Inefficiency**: The function **recalculates the same Fibonacci numbers multiple times** (e.g., `fib(3)` is calculated twice for `fib(5)`, and `fib(2)` is calculated multiple times).\n",
    "*   **Memoized Recursive Approach (Efficient)**:\n",
    "    *   **Solution**: Introduce a **dictionary (or hash map)** to store previously calculated Fibonacci numbers.\n",
    "    *   **Logic**:\n",
    "        1.  Before calculating `fib(n)`, **check if `n` is already in the dictionary**. If yes, return the stored value immediately.\n",
    "        2.  If not, calculate `fib(n)` recursively.\n",
    "        3.  **Store the result** for `n` in the dictionary before returning it.\n",
    "    *   **Result**: This significantly **reduces computation time**. For example, `fib(38)` and `fib(40)` (and even larger numbers) are calculated almost instantaneously.\n",
    "\n",
    "#### 3. Backpropagation in Neural Networks with Multiple Hidden Layers\n",
    "\n",
    "The video explains a concept missed in previous Backpropagation discussions: how to handle neural networks with **more than one hidden layer**.\n",
    "\n",
    "*   **Increased Complexity**: When a neural network has multiple hidden layers, calculating derivatives for weights in the **earlier layers (closer to the input)** becomes more complex.\n",
    "*   **Example Network**: A network with an input layer, two hidden layers, and an output layer (total four layers) is used.\n",
    "*   **Derivative for Later Weights (Simple Chain Rule)**:\n",
    "    *   For weights in the hidden layer closer to the output (e.g., `w_21`), the derivative `∂L/∂w_21` can be calculated directly using the **chain rule**.\n",
    "    *   `∂L/∂w_21 = (∂L/∂y_hat) * (∂y_hat/∂O_21) * (∂O_21/∂w_21)`.\n",
    "*   **Derivative for Earlier Weights (Complex Chain Rule with Multiple Paths)**:\n",
    "    *   **Problem**: When a weight in an earlier layer (e.g., `w_11`, connecting the input to the first node of the first hidden layer) changes, its effect propagates through **multiple subsequent nodes and paths** before reaching the loss function.\n",
    "    *   **Example**: A change in `w_11` affects the output of the first hidden node `O_1`. This `O_1` then feeds into *both* `O_1` (first node of second hidden layer) and `O_2` (second node of second hidden layer). Both `O_1` and `O_2` then contribute to the final `y_hat` and thus the loss `L`.\n",
    "    *   **Mathematical Approach**: When a variable (like `w_11`) affects the final output (`L`) through multiple intermediate pathways, the total derivative is the **sum of the derivatives along each of those paths**.\n",
    "    *   **Formula for `∂L/∂w_11`**: This involves two main paths, each requiring multiple chain rule applications, and then summing their results. The resulting formula is significantly more complex, involving terms like `(∂L/∂y_hat) * (∂y_hat/∂O_1) * (∂O_1/∂w_11)` plus `(∂L/∂y_hat) * (∂y_hat/∂O_2) * (∂O_2/∂w_11)` (simplified representation).\n",
    "    *   **Observation**: Calculating these derivatives requires recalculating many intermediate terms repeatedly.\n",
    "\n",
    "#### 4. The Role of Memoization in Backpropagation\n",
    "\n",
    "*   **The Challenge with Deep Networks**: As neural networks become deeper (more hidden layers), the formulas for calculating derivatives of weights in earlier layers become extraordinarily **complex**, leading to numerous **recalculations of identical intermediate derivatives**.\n",
    "*   **Memoization as a Solution**:\n",
    "    *   During backpropagation, we calculate derivatives starting from the output layer and move backward.\n",
    "    *   Many of the intermediate derivative terms that are calculated for later layers are **reused** when calculating derivatives for earlier layers.\n",
    "    *   By **storing these intermediate derivative results** (memoizing them), we can avoid recalculating them multiple times.\n",
    "    *   **Benefit**: This significantly **reduces the computational time** required for backpropagation, making the training process much faster. While it uses a bit more memory, the time savings are substantial.\n",
    "*   **Conclusion on Backpropagation**: The Backpropagation algorithm is fundamentally a combination of two powerful concepts:\n",
    "    1.  **Chain Rule (Mathematics)**: For differentiating complex composite functions.\n",
    "    2.  **Memoization (Computer Science Optimization Trick)**: For efficiently storing and reusing intermediate results.\n",
    "\n",
    "This combined approach is what makes modern deep learning libraries (like PyTorch or TensorFlow) efficient for training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d9c1aa2-9c5c-4f35-a286-38a302d458fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4224696333392304878706725602341482782579852840250681098010280137314308584370130707224123599639141511088446087538909603607640194711643596029271983312598737326253555802606991585915229492453904998722256795316982874482472992263901833716778060607011615497886719879858311468870876264597369086722884023654422295243347964480139515349562972087652656069529806499841977448720155612802665404554171717881930324025204312082516817125\n",
      "0.0022203922271728516\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "dic={0:1,1:1}\n",
    "start_time=time.time()\n",
    "def fib(n):\n",
    "    if n==0:\n",
    "        return 0\n",
    "    if n==1 :\n",
    "        return 1\n",
    "    if n not in dic.keys():\n",
    "        dic[n]=fib(n-1)+fib(n-2)\n",
    "        return dic[n]\n",
    "    else:\n",
    "        return dic[n]\n",
    "print(fib(2000))\n",
    "print(time.time()-start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
