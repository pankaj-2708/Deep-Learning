{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2db07c62-21dd-412b-a876-2e8f080b5940",
   "metadata": {},
   "source": [
    "# \"MLP Notation\"\n",
    "lec 8 campus x\n",
    "\n",
    "***\n",
    "\n",
    "### Lecture Notes: Multi-layer Perceptron (MLP) Notation\n",
    "\n",
    "**1. Introduction and Importance of Notation**\n",
    "*   This video builds on previous discussions about Multi-layer Perceptron (MLP) intuition, exploring how MLPs work and why they are effective.\n",
    "*   The most challenging aspect of understanding MLPs is often the **training algorithm, known as Backpropagation**.\n",
    "*   A common source of confusion when learning backpropagation is the large number of weights and biases in a neural network. Without a proper system for notation, it becomes difficult to distinguish between these parameters, leading to confusion during complex calculations.\n",
    "*   **The primary goals of this video are**:\n",
    "    1.  To learn how to **calculate the total number of trainable parameters** (weights and biases) in any given neural network architecture.\n",
    "    2.  To establish a **standardised notation for weights, biases, and outputs** that is commonly followed in the industry, to avoid confusion during backpropagation.\n",
    "\n",
    "**2. Neural Network Architecture Setup**\n",
    "\n",
    "<img src='https://i.ibb.co/twpX7JCM/image.png'>\n",
    "\n",
    "*   The lecture uses a specific neural network architecture for demonstration. (shown above)\n",
    "*   This architecture consists of **four layers in total**:\n",
    "    *   **Layer 0**: The Input Layer.\n",
    "    *   **Layer 1**: The first Hidden Layer.\n",
    "    *   **Layer 2**: The second Hidden Layer.\n",
    "    *   **Layer 3**: The Output Layer.\n",
    "*   The input data for this example is **four-dimensional**, meaning each input instance has four features or columns.\n",
    "\n",
    "**3. Calculating Trainable Parameters (Weights and Biases)**\n",
    "*   **Trainable parameters** are the values of weights and biases that the backpropagation algorithm will determine during the training process of the model.\n",
    "*   It is crucial to be able to calculate these from any given architecture.\n",
    "*   **For the demonstrated architecture**:\n",
    "    *   **From Input Layer (Layer 0) to Hidden Layer 1 (Layer 1)**:\n",
    "        *   Layer 0 has 4 nodes and Layer 1 has 3 nodes.\n",
    "        *   **Weights**: 4 (nodes in Layer 0) × 3 (nodes in Layer 1) = **12 weights**.\n",
    "        *   **Biases**: 3 (biases, one for each node in Layer 1).\n",
    "        *   *Subtotal for this segment*: 12 weights + 3 biases = **15 parameters**.\n",
    "    *   **From Hidden Layer 1 (Layer 1) to Hidden Layer 2 (Layer 2)**:\n",
    "        *   Layer 1 has 3 nodes and Layer 2 has 2 nodes.\n",
    "        *   **Weights**: 3 (nodes in Layer 1) × 2 (nodes in Layer 2) = **6 weights**.\n",
    "        *   **Biases**: 2 (biases, one for each node in Layer 2).\n",
    "        *   *Subtotal for this segment*: 6 weights + 2 biases = **8 parameters**.\n",
    "    *   **From Hidden Layer 2 (Layer 2) to Output Layer (Layer 3)**:\n",
    "        *   Layer 2 has 2 nodes and Layer 3 has 1 node.\n",
    "        *   **Weights**: 2 (nodes in Layer 2) × 1 (node in Layer 3) = **2 weights**.\n",
    "        *   **Biases**: 1 (bias, for the node in Layer 3).\n",
    "        *   *Subtotal for this segment*: 2 weights + 1 bias = **3 parameters**.\n",
    "    *   **Total Trainable Parameters for the entire network**: 15 + 8 + 3 = **26 parameters**. This means the backpropagation algorithm will find the values for these 26 weights and biases.\n",
    "\n",
    "**4. Notation for Biases (b)**\n",
    "*   The notation for biases is straightforward and uses two indices.\n",
    "*   **Standard Notation**: **`b_i^j`**\n",
    "    *   **`i`**: Represents the **layer number**.\n",
    "    *   **`j`**: Represents the **node number** within that layer.\n",
    "*   **Examples**:\n",
    "    *   **`b_1^1`**: Bias for the first node in Layer 1.\n",
    "    *   **`b_1^2`**: Bias for the second node in Layer 1.\n",
    "    *   **`b_2^1`**: Bias for the first node in Layer 2.\n",
    "    *   **`b_3^1`**: Bias for the first (and only) node in Layer 3.\n",
    "\n",
    "**5. Notation for Outputs (o)**\n",
    "*   The notation for outputs is identical to that for biases.\n",
    "*   **Standard Notation**: **`o_i^j`**\n",
    "    *   **`i`**: Represents the **layer number**.\n",
    "    *   **`j`**: Represents the **node number** within that layer.\n",
    "*   Any output originating from a node will follow this notation.\n",
    "*   **Examples**:\n",
    "    *   **`o_1^1`**: Output from the first node in Layer 1.\n",
    "    *   **`o_1^2`**: Output from the second node in Layer 1.\n",
    "    *   **`o_2^1`**: Output from the first node in Layer 2.\n",
    "    *   **`o_3^1`**: Output from the first (and only) node in Layer 3.\n",
    "\n",
    "eg - \n",
    "<img src='https://miro.medium.com/v2/resize:fit:1400/1*2vLiWsyesKLAfDcezIfBRQ.png'>\n",
    "\n",
    "**6. Notation for Weights (W)**\n",
    "*   The notation for weights is slightly more complex, requiring three indices.\n",
    "*   **Standard Notation**: **`W_k_i^j`**\n",
    "    *   **`k`**: Represents the **layer number that the weight is entering**. This is the layer containing the destination node.\n",
    "    *   **`i`**: Represents the **node number in the previous layer from which the weight is originating**.\n",
    "    *   **`j`**: Represents the **node number in the current layer (layer `k`) that the weight is entering**.\n",
    "*   **Examples (referencing the network diagram in the source)**:\n",
    "    *   **`W_1_1^1`**: Weight entering **Layer 1**, originating from the **1st node of the previous layer** (Layer 0), and entering the **1st node of Layer 1**.\n",
    "    *   **`W_1_4^2`**: Weight entering **Layer 1**, originating from the **4th node of the previous layer** (Layer 0), and entering the **2nd node of Layer 1**.\n",
    "    *   **`W_1_1^3`**: Weight entering **Layer 1**, originating from the **1st node of the previous layer** (Layer 0), and entering the **3rd node of Layer 1**.\n",
    "    *   **`W_2_2^2`**: Weight entering **Layer 2**, originating from the **2nd node of the previous layer** (Layer 1), and entering the **2nd node of Layer 2**.\n",
    "    *   **`W_3_1^1`**: Weight entering **Layer 3**, originating from the **1st node of the previous layer** (Layer 2), and entering the **1st node of Layer 3**.\n",
    "\n",
    "eg \n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1200/1*n5YNnh_vG2exS-YnjDPoPA.png'>\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa701451-b05f-459e-9e2e-c793c39ee4cd",
   "metadata": {},
   "source": [
    "# \"Multi Layer Perceptron | MLP Intuition\":\n",
    "\n",
    "lec 9 campus x dl playlist\n",
    "\n",
    "***\n",
    "\n",
    "### Lecture Notes: Multi-layer Perceptron (MLP) Intuition\n",
    "\n",
    "**1. Introduction: Overcoming the Perceptron's Limitation**\n",
    "*   The fundamental problem with a single Perceptron is its **inability to create non-linear decision boundaries**; it can only draw a straight line.\n",
    "*   This limitation means a Perceptron cannot capture non-linear relationships in data, as seen with complex datasets where a straight line cannot separate classes (e.g., data requiring a curved boundary).\n",
    "*   The solution is the **Multi-layer Perceptron (MLP)**, which combines multiple Perceptrons to form a larger neural network.\n",
    "*   MLPs act as **universal function approximators**, capable of creating any kind of non-linear decision boundary.\n",
    "\n",
    "**2. Perceptron (Logistic Regression) in this Context**\n",
    "*   For the purpose of this explanation, the Perceptron used has a **Sigmoid activation function** and **Log Loss**.\n",
    "*   This setup essentially makes the Perceptron behave like a **Logistic Regression model**.\n",
    "*   Instead of binary outputs (0 or 1), it provides a **probability between 0 and 1** (e.g., probability of placement).\n",
    "*   **How it works**:\n",
    "    *   Input features (e.g., CGPA, IQ) are multiplied by weights (w1, w2) and summed with a bias (`Z = w1*CGPA + w2*IQ + bias`).\n",
    "    *   This `Z` value is then passed through the Sigmoid function (`1 / (1 + e^-Z)`) to produce a probability.\n",
    "    *   The **decision boundary** is where this probability is **0.5**.\n",
    "    *   Points further away from this line will have probabilities increasingly closer to 0 or 1, forming a gradient of probabilities.\n",
    "\n",
    "**3. The Core Idea of MLP: Combining Multiple Perceptrons**\n",
    "*   To capture non-linearity, an MLP uses **more than one Perceptron** to solve the same problem.\n",
    "*   **Intuition (Abstract Idea)**: Imagine two separate Perceptrons, each creating its own linear decision boundary. The idea is to somehow \"superimpose\" these boundaries on top of each other and then \"smooth\" them out to create a more complex, non-linear boundary. This initial explanation is purely intuitive without mathematical detail.\n",
    "\n",
    "**4. Mathematical Justification: Linear Combination with Sigmoid**\n",
    "*   Consider a single data point (student) and two Perceptrons. Each Perceptron will output a probability (e.g., Perceptron 1: 0.7, Perceptron 2: 0.8).\n",
    "*   **Initial thought**: Simply add the probabilities (`0.7 + 0.8 = 1.5`).\n",
    "*   **Problem**: Probabilities must be between 0 and 1. A sum can exceed 1.\n",
    "*   **Solution**: Sum the probabilities from the individual Perceptrons, and then pass this sum through *another* **Sigmoid function**.\n",
    "    *   Example: `Sigmoid(P_Perceptron1 + P_Perceptron2)`. This new model would then output a valid probability (e.g., 0.82).\n",
    "*   This process of addition followed by a Sigmoid function mathematically achieves the \"superimposition and smoothing\" previously discussed conceptually.\n",
    "*   This is essentially creating a **linear combination of multiple Perceptrons**.\n",
    "\n",
    "**5. Adding Flexibility: Weighted Combinations and Bias**\n",
    "*   To allow different Perceptrons to have more or less influence on the final decision, **weights** can be assigned to their outputs.\n",
    "    *   Instead of `P_Perceptron1 + P_Perceptron2`, it becomes `w_new1 * P_Perceptron1 + w_new2 * P_Perceptron2`.\n",
    "    *   Example: `10 * P_Perceptron1 + 5 * P_Perceptron2` means Perceptron 1's output has double the impact.\n",
    "*   Additionally, a **bias term** can be added to this weighted sum: `Sigmoid(w_new1 * P_Perceptron1 + w_new2 * P_Perceptron2 + bias_new)`.\n",
    "*   **Key Realisation**: The entire operation of taking outputs from previous Perceptrons, applying new weights and a bias, and then passing it through a Sigmoid function, is itself the operation of **another Perceptron**.\n",
    "*   Therefore, an MLP is essentially a **combination of Perceptrons where the outputs of some Perceptrons serve as inputs to subsequent Perceptrons**.\n",
    "\n",
    "**6. MLP Architecture: Input, Hidden, and Output Layers**\n",
    "*   An MLP consists of at least three types of layers:\n",
    "    *   **Input Layer (Layer 0)**: Receives the raw features of the data (e.g., CGPA, IQ).\n",
    "    *   **Hidden Layer(s)**: Contains multiple Perceptrons (nodes). Each hidden node takes inputs from the previous layer (e.g., input layer), applies its own weights and bias, and uses an activation function (e.g., Sigmoid) to produce an output. This is where the non-linear transformations and feature learning primarily occur.\n",
    "    *   **Output Layer**: Contains one or more Perceptrons that take inputs from the last hidden layer and produce the final output of the network (e.g., probability of placement).\n",
    "\n",
    "**7. Modifying MLP Architecture for Enhanced Flexibility**\n",
    "*   Neural Network architecture defines how nodes (Perceptrons) are connected and their weights.\n",
    "*   **1. Increase Number of Nodes in Hidden Layer(s)**:\n",
    "    *   Adding more Perceptrons to a hidden layer allows the network to create more distinct linear decision boundaries internally.\n",
    "    *   This enables the MLP to capture **more complex non-linear relationships** in the data.\n",
    "*   **2. Increase Number of Nodes in Input Layer**:\n",
    "    *   This is done when the dataset has **more input features/columns** (e.g., adding \"12th marks\" to CGPA and IQ).\n",
    "    *   More input nodes change the dimensionality of the input space (e.g., from 2D lines to 3D planes for decision boundaries).\n",
    "*   **3. Increase Number of Nodes in Output Layer**:\n",
    "    *   Primarily used for **multi-class classification problems** (e.g., classifying an image as Dog, Cat, or Human).\n",
    "    *   Each output node corresponds to a specific class, providing a probability for that class.\n",
    "*   **4. Increase Number of Hidden Layers**:\n",
    "    *   This is the basis of **Deep Neural Networks**.\n",
    "    *   Multiple hidden layers allow the network to learn increasingly **abstract and hierarchical features** from the data.\n",
    "    *   Early layers might capture simple patterns, while deeper layers combine these patterns to understand more complex relationships.\n",
    "    *   This capability makes neural networks **universal function approximators**, meaning they can model almost any complex mathematical function, given enough layers, nodes, and training time.\n",
    "\n",
    "**8. TensorFlow Playground Demonstration**\n",
    "*   The demonstration in TensorFlow Playground (`playground.tensorflow.org`) illustrates these concepts:\n",
    "    *   A single Perceptron fails to converge on **XOR data** (non-linear).\n",
    "    *   A small **MLP (with two hidden nodes)** successfully learns to classify XOR data quickly.\n",
    "    *   For more complex non-linear datasets (e.g., concentric circles, spirals), MLPs can converge by adding more hidden layers and nodes.\n",
    "    *   Changing **activation functions** (e.g., to ReLU) can significantly improve training speed and convergence for complex data.\n",
    "    *   The tool visually shows how each layer transforms the decision boundaries, with early layers often starting with linear boundaries and deeper layers creating more complex, non-linear ones.\n",
    "    *   It highlights that MLPs can capture the \"essence\" of even highly complex, non-linear data.\n",
    "\n",
    "**9. Conclusion**\n",
    "*   MLPs solve the problem of non-linear data by creating **linear combinations of multiple Perceptrons**, which themselves are combined into layers.\n",
    "*   This architecture allows them to act as **universal function approximators**, capable of capturing any complex non-linear relationship within data.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efd82ea-0127-4435-b1b3-e65046fbd26f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
