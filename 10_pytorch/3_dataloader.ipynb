{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfeeb896",
   "metadata": {},
   "source": [
    "## I. Context: The Flaw in Batch Gradient Descent\n",
    "\n",
    "\n",
    "### A. Problems with Batch Gradient Descent\n",
    "1.  **Memory Inefficiency:** To perform Batch Gradient Descent, the entire dataset must be loaded into RAM simultaneously. While feasible for small datasets (e.g., 500 rows), this is impossible for large-scale problems like Image Classification with gigabytes of data.\n",
    "2.  **Slow Convergence:** Parameters are updated only once per epoch (after processing the whole dataset). This results in slow learning and convergence compared to updating parameters more frequently.\n",
    "\n",
    "### B. The Solution: Mini-Batch Gradient Descent\n",
    "The standard industry practice is **Mini-Batch Gradient Descent**. Instead of processing 1000 rows at once, the data is divided into smaller batches (e.g., 10 batches of 100 rows). The model performs a forward pass, calculates loss, and updates parameters for *each* batch sequentially.\n",
    "\n",
    "### C. Limitations of Manual Implementation\n",
    "While one can manually write loops to slice tensors into batches, this approach lacks robustness:\n",
    "*   **No Standard Interface:** It does not handle complex data fetching (e.g., images stored in folder structures).\n",
    "*   **Difficult Transformations:** There is no centralized place to apply data augmentations or transformations (e.g., resizing images, greyscaling).\n",
    "*   **Shuffling & Sampling:** Implementing efficient random shuffling and sampling requires complex manual logic.\n",
    "*   **No Parallelization:** Manual loops process data sequentially, failing to utilise multi-core CPUs for faster data loading.\n",
    "\n",
    "To solve these issues, PyTorch provides two core abstract classes: **Dataset** and **DataLoader**.\n",
    "\n",
    "---\n",
    "\n",
    "## II. The Dataset Class (`torch.utils.data.Dataset`)\n",
    "\n",
    "The **Dataset** class is responsible for defining *how* the data is loaded and *how* a single item is retrieved. It decouples the data storage logic from the training loop.\n",
    "\n",
    "To create a custom dataset, one must create a class that inherits from `torch.utils.data.Dataset` and implement three specific methods:\n",
    "\n",
    "### 1. The `__init__` Method (Constructor)\n",
    "*   **Purpose:** Defines how data is loaded from the source (e.g., reading a CSV file, defining paths to image folders).\n",
    "*   **Execution:** Runs once when the object is instantiated.\n",
    "\n",
    "### 2. The `__len__` Method\n",
    "*   **Purpose:** Returns the total number of samples (rows) in the dataset.\n",
    "*   **Utility:** This helps the DataLoader calculate how many batches will be created given a specific batch size.\n",
    "\n",
    "### 3. The `__getitem__` Method\n",
    "*   **Purpose:** The most critical method. Given an `index`, it retrieves the specific sample (features and label) from the dataset at that position.\n",
    "*   **Transformations:** This is the correct place to apply transformations. Before returning the row, one can resize images, apply normalization, or convert text to lower case.\n",
    "\n",
    "---\n",
    "\n",
    "## III. The DataLoader Class (`torch.utils.data.DataLoader`)\n",
    "\n",
    "While the Dataset class fetches individual items, the **DataLoader** class orchestrates the creation of batches. It manages shuffling, batching, and parallel processing.\n",
    "\n",
    "### A. The Workflow of DataLoader\n",
    "The internal process of a DataLoader follows these steps:\n",
    "1.  **Index Selection:** It retrieves all indices (e.g., 0 to 9).\n",
    "2.  **Sampling/Shuffling:** It uses a **Sampler** to shuffle these indices randomly (if `shuffle=True`).\n",
    "3.  **Chunking:** It groups the shuffled indices into chunks based on the defined `batch_size` (e.g., pairing indices for a batch size of 2).\n",
    "4.  **Fetching:** It passes these indices one by one to the **Dataset's `__getitem__`** method to retrieve the actual data.\n",
    "5.  **Collation:** It uses a **Collate Function** to combine the individual data items into a single batch tensor.\n",
    "\n",
    "### B. Key Parameters\n",
    "*   **`batch_size`:** The number of samples per batch.\n",
    "*   **`shuffle`:** If `True`, data is shuffled every epoch. If `False`, it remains sequential (useful for Time Series).\n",
    "*   **`num_workers`:** Enables **Parallelization**. Instead of fetching data sequentially, multiple \"workers\" (sub-processes) fetch data batches simultaneously, significantly speeding up training.\n",
    "*   **`drop_last`:** If `True`, it drops the final batch if it is incomplete (smaller than the batch size). This is useful when using Batch Normalization, which requires consistent batch sizes.\n",
    "\n",
    "---\n",
    "\n",
    "## IV. Advanced Concepts: Samplers and Collate Functions\n",
    "\n",
    "### A. The Sampler\n",
    "The Sampler determines the strategy for drawing indices from the dataset.\n",
    "*   **RandomSampler:** Shuffles indices (Standard for training).\n",
    "*   **SequentialSampler:** Keeps indices in order (Standard for validation/testing or Time Series).\n",
    "*   **Weighted/Custom Samplers:** Crucial for **Imbalanced Datasets**. For example, if Class A has 99% of data and Class B has 1%, a custom sampler can ensure every batch maintains a specific ratio of Class B to prevent model bias.\n",
    "\n",
    "### B. The Collate Function\n",
    "The `collate_fn` defines how a list of individual samples (retrieved by `__getitem__`) is merged into a batch.\n",
    "*   **Default Behavior:** Simply stacks tensors together.\n",
    "*   **Custom Usage (e.g., NLP):** If samples are sentences of variable lengths (e.g., one sentence has 4 words, another has 2), they cannot be stacked directly. A custom Collate function is required to add **Padding** (adding zeros) to match lengths before stacking.\n",
    "\n",
    "---\n",
    "\n",
    "## V. Implementation Summary\n",
    "\n",
    "The lecture concludes by refactoring the Breast Cancer training code to use these classes:\n",
    "\n",
    "1.  **Step 1:** Define a `CustomDataset` class implementing `__init__`, `__len__`, and `__getitem__`.\n",
    "2.  **Step 2:** Instantiate `train_dataset` and `test_dataset` objects.\n",
    "3.  **Step 3:** Instantiate `train_loader` and `test_loader` using `DataLoader`, specifying `batch_size` and `shuffle`.\n",
    "4.  **Step 4:** Refactor the training loop to include a nested loop:\n",
    "    *   **Outer Loop:** Iterates over Epochs.\n",
    "    *   **Inner Loop:** Iterates over the `train_loader` to fetch `batch_features` and `batch_labels` for Mini-Batch Gradient Descent.\n",
    "\n",
    "This structure ensures the code is scalable, memory-efficient, and cleaner compared to manual batching."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
