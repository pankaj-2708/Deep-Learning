{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "468102fd",
   "metadata": {},
   "source": [
    "These lecture notes cover the **Data Efficient Image Transformer (DEiT)**, a model developed by Facebook AI Research (FAIR) in 2021. The lecture builds upon the previously studied **Vision Transformer (ViT)**, addressing its primary weakness: data inefficiency.\n",
    "\n",
    "<a href=\"https://miro.com/app/board/uXjVJsY_MIU=/\">Read here</a>\n",
    "## I. Context: The Problem with Vision Transformers (ViT)\n",
    "\n",
    "While Vision Transformers (ViT) achieved state-of-the-art results, they possessed significant drawbacks compared to Convolutional Neural Networks (CNNs):\n",
    "\n",
    "1.  **Data Hunger:** The original ViT required a massive dataset (JFT-300M, containing 300 million private images) to perform well. The paper explicitly states, \"Transformers do not generalize well when trained on insufficient amounts of data\".\n",
    "2.  **Computational Cost:** The base ViT models were enormous (e.g., 600 million parameters) and required significant GPU resources and time to train.\n",
    "3.  **Lack of Inductive Bias:** CNNs possess inherent **Inductive Biases**—specifically **Locality** (assuming adjacent pixels are related) and **Translation Invariance** (recognizing an object regardless of where it appears in the image). Transformers lack these biases because they treat images as sequences of patches with global attention. Consequently, ViT requires massive amounts of data to \"learn\" these spatial relationships from scratch.\n",
    "\n",
    "**The Goal of DEiT:** To train a high-performance transformer on a standard-sized dataset (ImageNet-1k, ~1.2 million images) without requiring the massive JFT-300M dataset.\n",
    "\n",
    "## II. The Solution: Knowledge Distillation\n",
    "\n",
    "To solve the data efficiency problem, DEiT introduces **Knowledge Distillation** (specifically, a Teacher-Student model) into the Transformer training process.\n",
    "\n",
    "### A. The Concept\n",
    "*   **Teacher:** A large, pre-trained model (often a CNN like RegNet or ResNet). The teacher’s weights are frozen; it is not trained further.\n",
    "*   **Student:** The DEiT model (Vision Transformer) which is trained from scratch.\n",
    "*   **Objective:** The Student tries to replicate the predictions of the Teacher. By doing so, the Transformer (Student) inherits the robust representations and inductive biases of the CNN (Teacher) without changing its own architecture.\n",
    "\n",
    "### B. Soft vs. Hard Labels\n",
    "When the Teacher makes a prediction, it outputs a probability distribution (logits):\n",
    "1.  **Soft Labels:** The raw probabilities (e.g., Cat: 0.8, Dog: 0.1, Background: 0.1). These contain rich information, indicating that the Teacher thinks the image looks *a little bit* like a dog.\n",
    "2.  **Hard Labels:** Converting the probability to a one-hot vector (e.g., Cat: 1, Dog: 0, Background: 0).\n",
    "\n",
    "**Surprising Finding:** The DEiT paper found that **Hard Distillation** (using the Teacher's hard decisions) often worked better than Soft Distillation when the Teacher was a CNN and the Student was a Transformer.\n",
    "\n",
    "## III. DEiT Architecture: The Distillation Token\n",
    "\n",
    "DEiT is 95% similar to the standard ViT but introduces one specific architectural change to facilitate distillation.\n",
    "\n",
    "### A. The Extra Token\n",
    "Standard ViT uses a **Class Token (`[CLS]`)** to aggregate information for classification. DEiT adds a second special token called the **Distillation Token (`[DIST]`)**.\n",
    "\n",
    "1.  **Initialization:** The Distillation token is **randomly initialized**, just like the CLS token. It is *not* initialized using CNN features (which would be cheating).\n",
    "2.  **Processing:** Both tokens interact with the image patches via Self-Attention layers throughout the network.\n",
    "3.  **Convergence:** Ideally, one might expect the CLS and DIST tokens to converge to the same vector. However, the authors found the cosine similarity between them is low (0.06), meaning they learn distinct representations because they target different objectives.\n",
    "\n",
    "### B. Output Structure\n",
    "At the final layer, the model produces two separate context vectors:\n",
    "1.  **CLS Token Output:** Fed into a classification head to predict the **Ground Truth**.\n",
    "2.  **DIST Token Output:** Fed into a separate distillation head to predict the **Teacher's Label**.\n",
    "\n",
    "## IV. The Loss Function\n",
    "\n",
    "The training objective is a composite loss function that balances learning from the truth and learning from the teacher.\n",
    "\n",
    "The total loss $\\mathcal{L}$ is defined as:\n",
    "$$ \\mathcal{L} = \\alpha \\times \\mathcal{L}_{CE} + (1-\\alpha) \\times \\mathcal{L}_{Teacher} $$\n",
    "\n",
    "1.  **$\\mathcal{L}_{CE}$ (Cross Entropy):** Calculates the error between the **CLS Token's prediction** and the **Ground Truth** (True Labels).\n",
    "2.  **$\\mathcal{L}_{Teacher}$ (KL Divergence):** Calculates the error between the **DIST Token's prediction** and the **Teacher's prediction**.\n",
    "    *   This is typically calculated using **Kullback-Leibler (KL) Divergence**, which measures how one probability distribution differs from another.\n",
    "    *   If the Student makes a confident prediction that conflicts with the Teacher (e.g., Student says \"Dog\" with 100% certainty, but Teacher is unsure), the loss blows up, penalizing the Student heavily.\n",
    "\n",
    "**Key Hyperparameter:** The **Temperature ($\\tau$)** parameter is used inside the Softmax function to control the \"sharpness\" of the probability distribution. A higher temperature makes the distribution softer (flatter), while a lower temperature makes it sharper (peakier).\n",
    "\n",
    "## V. Implementation & Coding Insights\n",
    "\n",
    "The lecture concludes with a \"from scratch\" implementation in PyTorch. Key takeaways include:\n",
    "\n",
    "1.  **Teacher Model:** A pre-trained ResNet50 is used. The classification layer (last fully connected layer) is modified to match the number of classes in the current dataset (e.g., 10 for MNIST/CIFAR).\n",
    "2.  **Freezing:** It is crucial to freeze the Teacher's parameters (set `requires_grad = False`) so backpropagation only updates the Student.\n",
    "3.  **Loss Calculation:**\n",
    "    *   The model returns two logits: `cls_logits` and `dist_logits`.\n",
    "    *   `cls_logits` are compared against Ground Truth ($y$).\n",
    "    *   `dist_logits` are compared against the Teacher's output ($y_{teacher}$).\n",
    "4.  **Inference Strategy:** At prediction time (inference), you have two heads. You can:\n",
    "    *   Use only the CLS token.\n",
    "    *   Use only the DIST token.\n",
    "    *   Average both (Fusion).\n",
    "    *   *Result:* Empirical results often show that averaging both or using the DIST token alone yields higher accuracy than the CLS token alone.\n",
    "\n",
    "## VI. Summary of Impact\n",
    "\n",
    "DEiT proved that Transformers *can* be efficient. By using a CNN teacher to guide the learning process (injecting inductive bias indirectly via the loss function), DEiT achieved better accuracy than its teacher (EfficientNet) and the original ViT on ImageNet-1k, with significantly fewer parameters (84M vs 600M) and faster training times."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
