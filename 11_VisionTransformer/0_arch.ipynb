{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60070c14",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT) — Complete Notes\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Overview\n",
    "A Vision Transformer converts an image into a sequence of tokens (patch embeddings) and feeds them into a standard Transformer encoder. It relies on **three key components**:\n",
    "\n",
    "### ViT Positional Encoding = 3 Layers\n",
    "1. **Patch Embeddings**\n",
    "2. **Class Token (`[CLS]`)**\n",
    "3. **Positional Embeddings**\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Patch Embeddings\n",
    "### Goal\n",
    "Convert an image into a sequence of tokens.\n",
    "\n",
    "### Steps\n",
    "- Split image into patches of size **P × P**\n",
    "- Flatten each patch\n",
    "- Linearly project into embedding of size **D**\n",
    "\n",
    "### Output Shape\n",
    "```\n",
    "(batch, num_patches, D)\n",
    "```\n",
    "\n",
    "### Analogy\n",
    "Patch embedding ≈ Word embedding in NLP.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Class Token (`[CLS]`)\n",
    "### What it is\n",
    "A **learnable vector** of dimension **D**, prepended to the patch sequence.\n",
    "\n",
    "### Why it exists\n",
    "Transformers produce one output per token, but classification needs **one global vector**.  \n",
    "The `[CLS]` token learns to gather information from all patches through attention.\n",
    "\n",
    "### After the Transformer\n",
    "Use:\n",
    "```\n",
    "CLS_out = output[:, 0, :]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Positional Embeddings\n",
    "Transformers lack positional awareness. ViT uses **learned** positional embeddings.\n",
    "\n",
    "### Shape\n",
    "```\n",
    "(num_patches + 1, D)\n",
    "```\n",
    "\n",
    "### Why Learned Instead of Sinusoidal\n",
    "- Sinusoidal is 1D; images are 2D  \n",
    "- Learned embeddings work better for spatial patterns  \n",
    "- Improves accuracy  \n",
    "\n",
    "### Added to Tokens\n",
    "```\n",
    "tokens = patch_embeddings + positional_embeddings\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Full Transformer Input\n",
    "```\n",
    "[CLS] + patch_embeddings + positional_embeddings\n",
    "```\n",
    "\n",
    "Shape:\n",
    "```\n",
    "(batch, num_patches + 1, D)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 6. Transformer Encoder in ViT\n",
    "Same structure as NLP Transformer encoder:\n",
    "- Multi-Head Self-Attention\n",
    "- MLP\n",
    "- LayerNorm\n",
    "- Residual connections\n",
    "\n",
    "No decoder is used.\n",
    "\n",
    "---\n",
    "\n",
    "# 7. Classification Head\n",
    "Only the final CLS vector is used:\n",
    "```\n",
    "cls_vec = output[:, 0, :]\n",
    "logits = MLP(cls_vec)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 8. ViT vs NLP Transformer\n",
    "\n",
    "| Component | NLP Transformer | Vision Transformer |\n",
    "|----------|------------------|--------------------|\n",
    "| Input Tokens | Words | Image patches |\n",
    "| Token Embedding | Lookup | Linear projection |\n",
    "| Positional Encoding | Sinusoidal or learned | Always learned |\n",
    "| Structure | 1D | 2D patches flattened |\n",
    "| Special Token | CLS | CLS |\n",
    "| Resolution Changes | Easy | Need interpolation |\n",
    "\n",
    "---\n",
    "\n",
    "# 9. Why CLS Token is Required\n",
    "- Transformer outputs multiple vectors  \n",
    "- Classification requires **one vector**  \n",
    "- CLS token learns global aggregation  \n",
    "- Better than average pooling  \n",
    "\n",
    "---\n",
    "\n",
    "# 10. Minimal Summary\n",
    "- Image → patches → linear projection  \n",
    "- Add `[CLS]` token  \n",
    "- Add learned positional embeddings  \n",
    "- Pass through Transformer encoder  \n",
    "- Extract CLS output  \n",
    "- Classify\n",
    "\n",
    "This is the core of Vision Transformers."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
