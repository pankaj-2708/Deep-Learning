{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5d4acc5",
   "metadata": {},
   "source": [
    "# Limitations of CNNs That Led to Vision Transformers  \n",
    "\n",
    "---\n",
    "\n",
    "# 1. Strong Inductive Bias\n",
    "CNNs are built on assumptions like locality, translation equivariance, and hierarchical feature extraction.  \n",
    "These assumptions help when data is small but restrict flexibility and global reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Poor Global Context Modeling\n",
    "Convolutions operate on local neighborhoods (e.g., 3×3 or 5×5).  \n",
    "To gather long-range relationships, CNNs must stack many layers, use dilations, or add attention-like modules.\n",
    "\n",
    "This makes capturing global structure:\n",
    "- slow,\n",
    "- indirect,\n",
    "- computationally expensive.\n",
    "\n",
    "Transformers, in contrast, use self-attention, giving global receptive field from the first layer.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Slow Receptive Field Growth\n",
    "A CNN’s receptive field grows only as you stack more layers.  \n",
    "This is inefficient and can still fail to capture whole-image relationships.  \n",
    "ViTs eliminate this by letting every patch attend to every other patch immediately.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Inefficient Scaling\n",
    "Increasing depth/width of CNNs often yields diminishing returns.  \n",
    "Transformers scale linearly and benefit strongly from large datasets + large compute.\n",
    "\n",
    "This ability to scale up (same as GPT-like models) is a major reason ViT surpasses CNNs when pretrained on large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Heavy Architectural Engineering\n",
    "CNNs require manually designed components:\n",
    "- kernel sizes,\n",
    "- strides,\n",
    "- padding,\n",
    "- feature pyramids,\n",
    "- skip connections,\n",
    "- specialized detection/segmentation heads.\n",
    "\n",
    "ViTs simplify architecture to repeated Transformer blocks (+ patch embedding).\n",
    "\n",
    "---\n",
    "\n",
    "# 6. Weaker Transfer Across Domains\n",
    "CNNs encode vision-specific priors → less reusable across modalities.  \n",
    "Transformers unify architectures across NLP, vision, audio, and multimodal tasks.\n",
    "\n",
    "---\n",
    "\n",
    "# 7. Fixed Local Interactions\n",
    "CNN kernels learn **fixed patterns** that apply locally.  \n",
    "Transformers instead compute **dynamic token-to-token relationships** that change based on the content.\n",
    "\n",
    "---\n",
    "\n",
    "# 8. Limitations in High-Data Regimes\n",
    "When more data becomes available (ImageNet-21k, JFT-300M), CNNs hit a performance ceiling.  \n",
    "Transformers improve dramatically with scale, showing higher upper limits.\n",
    "\n",
    "---\n",
    "\n",
    "# 9. No Unified Architecture With NLP\n",
    "ViT enables consistent architecture across fields.  \n",
    "This makes multimodal models (CLIP, Flamingo, LLaVA) possible.\n",
    "\n",
    "---\n",
    "\n",
    "# 10. Translation Equivariance (Major Difference)\n",
    "\n",
    "### CNNs **have** translation equivariance  \n",
    "Translation equivariance means:\n",
    "\n",
    "**If the input image shifts, the output feature map shifts in the same way.**  \n",
    "The network’s response is preserved under translation.\n",
    "\n",
    "Why CNNs have this:\n",
    "- Convolution uses the **same kernel at every location**.\n",
    "- The kernel reacts identically no matter where a pattern appears.\n",
    "- So moving the pattern just moves the activation.\n",
    "\n",
    "This is a strong image-specific inductive bias that makes CNNs:\n",
    "- robust to object movement,\n",
    "- very effective at small/medium data settings,\n",
    "- naturally suited for spatial tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### Vision Transformers **do NOT** have translation equivariance  \n",
    "\n",
    "Why ViTs break this property:\n",
    "\n",
    "#### 1. Patchifying breaks translation structure  \n",
    "If the image shifts slightly:\n",
    "- patches are cut differently,\n",
    "- patch boundaries change,\n",
    "- the token sequence changes completely.\n",
    "\n",
    "Even 1–2 pixels of shift can create a different patch composition.\n",
    "\n",
    "#### 2. Positional embeddings remove translation symmetry  \n",
    "A positional embedding gives each patch a **fixed coordinate identity**.  \n",
    "If the image shifts, patches get different embeddings → outputs do not shift predictably.\n",
    "\n",
    "#### 3. Global self-attention mixes all tokens  \n",
    "Self-attention has no spatial consistency requirement.  \n",
    "A shift in input does NOT guarantee any structured shift in output.\n",
    "\n",
    "---\n",
    "\n",
    "### Consequence  \n",
    "**CNN:**\n",
    "- Built-in robustness to object movement.\n",
    "- No need to learn translation behavior.\n",
    "\n",
    "**ViT:**\n",
    "- Must learn translation invariance purely from data.\n",
    "- Performs worse in low-data settings.\n",
    "- Outperforms CNNs once pretrained on very large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "# CNN vs ViT — Summary Table (Updated)\n",
    "\n",
    "| Property / Limitation | CNNs | Vision Transformers (ViT) |\n",
    "|------------------------|------|----------------------------|\n",
    "| **Inductive Bias** | Strong: locality, translation equivariance | Weak; model learns everything from data |\n",
    "| **Translation Equivariance** | **Yes (built-in)** | **No (broken by patching + positional embeddings)** |\n",
    "| **Global Context** | Hard; large depth needed | Immediate global self-attention |\n",
    "| **Receptive Field** | Grows slowly | Global from first layer |\n",
    "| **Scalability** | Saturates when scaled | Scales extremely well with data/compute |\n",
    "| **Dynamic Interactions** | Fixed convolution kernels | Dynamic token-to-token attention |\n",
    "| **Architecture Complexity** | Requires manual design (kernels, pooling) | Simple repeated blocks |\n",
    "| **Transfer Learning** | Moderate; mostly vision-only | Excellent; cross-modal + large-scale |\n",
    "| **Data Efficiency** | Strong for small data | Weak for small data; needs large pretraining |\n",
    "| **Multimodal Compatibility** | Not natural | Natural (same Transformer backbone as NLP) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60070c14",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT)\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Overview\n",
    "A Vision Transformer converts an image into a sequence of tokens (patch embeddings) and feeds them into a standard Transformer encoder. It relies on **three key components**:\n",
    "\n",
    "### ViT Positional Encoding = 3 Layers\n",
    "1. **Patch Embeddings**\n",
    "2. **Class Token (`[CLS]`)**\n",
    "3. **Positional Embeddings**\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Patch Embeddings\n",
    "### Goal\n",
    "Convert an image into a sequence of tokens.\n",
    "\n",
    "### Steps\n",
    "- Split image into patches of size **P × P**\n",
    "- Flatten each patch\n",
    "- Linearly project into embedding of size **D**\n",
    "\n",
    "### Output Shape\n",
    "```\n",
    "(batch, num_patches, D)\n",
    "```\n",
    "\n",
    "### Analogy\n",
    "Patch embedding ≈ Word embedding in NLP.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Class Token (`[CLS]`)\n",
    "### What it is\n",
    "A **learnable vector** of dimension **D**, prepended to the patch sequence.\n",
    "\n",
    "### Why it exists\n",
    "Transformers produce one output per token, but classification needs **one global vector**.  \n",
    "The `[CLS]` token learns to gather information from all patches through attention.\n",
    "\n",
    "### After the Transformer\n",
    "Use:\n",
    "```\n",
    "CLS_out = output[:, 0, :]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Positional Embeddings\n",
    "Transformers lack positional awareness. ViT uses **learned** positional embeddings.\n",
    "\n",
    "### Shape\n",
    "```\n",
    "(num_patches + 1, D)\n",
    "```\n",
    "\n",
    "### Why Learned Instead of Sinusoidal\n",
    "- Sinusoidal is 1D; images are 2D  \n",
    "- Learned embeddings work better for spatial patterns  \n",
    "- Improves accuracy  \n",
    "\n",
    "### Added to Tokens\n",
    "```\n",
    "tokens = patch_embeddings + positional_embeddings\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Full Transformer Input\n",
    "```\n",
    "[CLS] + patch_embeddings + positional_embeddings\n",
    "```\n",
    "\n",
    "Shape:\n",
    "```\n",
    "(batch, num_patches + 1, D)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 6. Transformer Encoder in ViT\n",
    "Same structure as NLP Transformer encoder:\n",
    "- Multi-Head Self-Attention\n",
    "- MLP\n",
    "- LayerNorm\n",
    "- Residual connections\n",
    "\n",
    "No decoder is used.\n",
    "\n",
    "---\n",
    "\n",
    "# 7. Classification Head\n",
    "Only the final CLS vector is used:\n",
    "```\n",
    "cls_vec = output[:, 0, :]\n",
    "logits = MLP(cls_vec)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 8. ViT vs NLP Transformer\n",
    "\n",
    "| Component | NLP Transformer | Vision Transformer |\n",
    "|----------|------------------|--------------------|\n",
    "| Input Tokens | Words | Image patches |\n",
    "| Token Embedding | Lookup | Linear projection |\n",
    "| Positional Encoding | Sinusoidal or learned | Always learned |\n",
    "| Structure | 1D | 2D patches flattened |\n",
    "| Special Token | CLS | CLS |\n",
    "| Resolution Changes | Easy | Need interpolation |\n",
    "\n",
    "---\n",
    "\n",
    "# 9. Why CLS Token is Required\n",
    "- Transformer outputs multiple vectors  \n",
    "- Classification requires **one vector**  \n",
    "- CLS token learns global aggregation  \n",
    "- Better than average pooling  \n",
    "\n",
    "---\n",
    "\n",
    "# 10. Minimal Summary\n",
    "- Image → patches → linear projection  \n",
    "- Add `[CLS]` token  \n",
    "- Add learned positional embeddings  \n",
    "- Pass through Transformer encoder  \n",
    "- Extract CLS output  \n",
    "- Classify\n",
    "\n",
    "This is the core of Vision Transformers."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
