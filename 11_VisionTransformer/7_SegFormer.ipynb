{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Notes\n",
        "# https://miro.com/app/board/uXjVGaELrhQ=/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gM7PCKFEuggf"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "s1H4IvVxuxVv"
      },
      "outputs": [],
      "source": [
        "# @title Global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "8mfvrAywfR1A"
      },
      "outputs": [],
      "source": [
        "# @title Efficient self attention\n",
        "class EfficientSelfAttention(nn.Module):\n",
        "  def __init__(self,num_heads,reduction_ratio,dim):\n",
        "    super().__init__()\n",
        "    self.num_heads=num_heads\n",
        "    self.reduction_ratio=reduction_ratio\n",
        "    self.q_proj=nn.Linear(dim,dim)\n",
        "    self.kv_proj=nn.Linear(dim,dim*2)\n",
        "    self.out_proj=nn.Linear(dim,dim)\n",
        "    self.norm=nn.LayerNorm(dim)\n",
        "    self.sr=nn.Conv2d(dim,dim,kernel_size=reduction_ratio,stride=reduction_ratio)\n",
        "    self.scale=(dim//num_heads)**-0.5\n",
        "    self.out_drop=nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self,x,H,W):\n",
        "    B, N, C = x.shape\n",
        "    q=self.q_proj(x).reshape(B,N,self.num_heads,C//self.num_heads).permute(0,2,1,3)\n",
        "\n",
        "    x_=x.permute(0,2,1).reshape(B, C, H, W)\n",
        "    x_=self.sr(x_).reshape(B,C,-1).permute(0,2,1)\n",
        "    x_=self.norm(x_)\n",
        "    kv=self.kv_proj(x_).reshape(B,-1,2,self.num_heads,C//self.num_heads).permute(2,0,3,1,4)\n",
        "    k=kv[0]\n",
        "    v=kv[1]\n",
        "\n",
        "    attn=(q@k.transpose(-1,-2))*self.scale\n",
        "    attn=attn.softmax(dim=-1)\n",
        "    out=(attn@v).transpose(1,2).reshape(B,N,C)\n",
        "    out=self.out_proj(out)\n",
        "    out=self.out_drop(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "NpbQN8DAlztF"
      },
      "outputs": [],
      "source": [
        "# @title Feed Forward\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embd,hidden_features):\n",
        "    super().__init__()\n",
        "    self.fc1=nn.Linear(n_embd,hidden_features)\n",
        "    self.fc2=nn.Linear(hidden_features,n_embd)\n",
        "    self.drop=nn.Dropout(0.1)\n",
        "    self.act=nn.GELU()\n",
        "    self.conv=nn.Conv2d(hidden_features,hidden_features,kernel_size=3,stride=1,padding=1)\n",
        "\n",
        "  def forward(self,x_in,H,W):\n",
        "    B,N,C=x_in.shape\n",
        "    x=self.fc1(x_in)\n",
        "    x=x.transpose(1,2).reshape(B,-1,H,W)\n",
        "    x=self.conv(x)\n",
        "    x=x.reshape(B,-1,N).transpose(1,2)\n",
        "    x=self.act(x)\n",
        "    x=self.fc2(x)\n",
        "    x=self.drop(x)\n",
        "    return x+x_in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "VCO3yY4G5LJF"
      },
      "outputs": [],
      "source": [
        "# @title Encoder Block\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, dim,input_channels,r):\n",
        "        super().__init__()\n",
        "        self.attention=EfficientSelfAttention(num_heads=8,reduction_ratio=r,dim=dim)\n",
        "        self.ffn=FeedForward(dim,dim*4)\n",
        "        self.norm1=nn.LayerNorm(dim)\n",
        "        self.norm2=nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self,x,H,W):\n",
        "        x=x+self.attention(self.norm1(x),H,W)\n",
        "        x=x+self.ffn(self.norm2(x),H,W)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "mBEGwnKJzwNK"
      },
      "outputs": [],
      "source": [
        "# @title Overlap patch Embeddings\n",
        "class OverlapPatchEmbeddings(nn.Module):\n",
        "  def __init__(self,input_channels,embed_dim,kernel_size,stride,padding):\n",
        "    super().__init__()\n",
        "    self.norm1=nn.LayerNorm(embed_dim)\n",
        "    self.conv=nn.Conv2d(input_channels,embed_dim,kernel_size=kernel_size,stride=stride,padding=padding)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.conv(x)\n",
        "    _,_,H,W=x.shape\n",
        "    x=x.flatten(2).transpose(1,2)\n",
        "    x=self.norm1(x)\n",
        "    return x,H,W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "iGsE6IfZu1eI"
      },
      "outputs": [],
      "source": [
        "# @title Encoder\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self,embed_dims=[64, 128, 256, 512],in_chans=3,sr=[8, 4, 2, 1],depths=[3, 4, 6, 3]):\n",
        "    super().__init__()\n",
        "    self.patch_embed1= OverlapPatchEmbeddings(kernel_size=7, stride=4,padding=3, input_channels=in_chans,\n",
        "                                              embed_dim=embed_dims[0])\n",
        "    self.patch_embed2= OverlapPatchEmbeddings(kernel_size=3, stride=2,padding=1, input_channels=embed_dims[0],\n",
        "                                              embed_dim=embed_dims[1])\n",
        "    self.patch_embed3= OverlapPatchEmbeddings(kernel_size=3, stride=2,padding=1, input_channels=embed_dims[1],\n",
        "                                              embed_dim=embed_dims[2])\n",
        "    self.patch_embed4= OverlapPatchEmbeddings(kernel_size=3, stride=2,padding=1, input_channels=embed_dims[2],\n",
        "                                              embed_dim=embed_dims[3])\n",
        "\n",
        "    self.block1=nn.ModuleList([EncoderBlock(dim=embed_dims[0],input_channels=in_chans,r=sr[0]) for _ in range(depths[0])])\n",
        "    self.block2=nn.ModuleList([EncoderBlock(dim=embed_dims[1],input_channels=embed_dims[0],r=sr[1]) for _ in range(depths[1])])\n",
        "    self.block3=nn.ModuleList([EncoderBlock(dim=embed_dims[2],input_channels=embed_dims[1],r=sr[2]) for _ in range(depths[2])])\n",
        "    self.block4=nn.ModuleList([EncoderBlock(dim=embed_dims[3],input_channels=embed_dims[2],r=sr[3]) for _ in range(depths[3])])\n",
        "    self.norm1=nn.LayerNorm(embed_dims[0])\n",
        "    self.norm2=nn.LayerNorm(embed_dims[1])\n",
        "    self.norm3=nn.LayerNorm(embed_dims[2])\n",
        "    self.norm4=nn.LayerNorm(embed_dims[3])\n",
        "    self.depths=[3, 4, 6, 3]\n",
        "\n",
        "  def forward(self,x):\n",
        "    # block 1\n",
        "    B=x.shape[0]\n",
        "    outs=[]\n",
        "    x,H,W=self.patch_embed1(x)\n",
        "    for blk in self.block1:\n",
        "      x=blk(x,H,W)\n",
        "    x=self.norm1(x)\n",
        "    x=x.reshape(B,H,W,-1).permute(0,3,1,2)\n",
        "    outs.append(x)\n",
        "    # print(\"yes\")\n",
        "    # print(x.shape)\n",
        "    # block 2\n",
        "    x,H,W=self.patch_embed2(x)\n",
        "    for blk in self.block2:\n",
        "      x=blk(x,H,W)\n",
        "    x=self.norm2(x)\n",
        "    x=x.reshape(B,H,W,-1).permute(0,3,1,2)\n",
        "    outs.append(x)\n",
        "    # print(\"yes\")\n",
        "    # print(x.shape)\n",
        "\n",
        "    # block 3\n",
        "    x,H,W=self.patch_embed3(x)\n",
        "    # print(x.shape)\n",
        "    for blk in self.block3:\n",
        "      x=blk(x,H,W)\n",
        "    x=self.norm3(x)\n",
        "    x=x.reshape(B,H,W,-1).permute(0,3,1,2)\n",
        "    outs.append(x)\n",
        "    # print(\"yes\")\n",
        "\n",
        "    # block 3\n",
        "    x,H,W=self.patch_embed4(x)\n",
        "    for blk in self.block4:\n",
        "      x=blk(x,H,W)\n",
        "    x=self.norm4(x)\n",
        "    x=x.reshape(B,H,W,-1).permute(0,3,1,2)\n",
        "    outs.append(x)\n",
        "    print(\"yes\")\n",
        "    return outs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "aR52_0rIVV4E"
      },
      "outputs": [],
      "source": [
        "# @title Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_dims, decoder_dim, num_classes, out_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.proj = nn.ModuleList([\n",
        "            nn.Conv2d(embed_dims[i], decoder_dim, kernel_size=1)\n",
        "            for i in range(4)\n",
        "        ])\n",
        "\n",
        "        self.fuse = nn.Conv2d(decoder_dim * 4, decoder_dim, kernel_size=1)\n",
        "        self.cls = nn.Conv2d(decoder_dim, num_classes, kernel_size=1)\n",
        "        self.upsample = nn.Upsample(size=out_size, mode='bilinear', align_corners=False)\n",
        "\n",
        "    def forward(self, feats):\n",
        "        x = []\n",
        "        for i in range(4):\n",
        "            xi = self.proj[i](feats[i])\n",
        "            xi = self.upsample(xi)\n",
        "            x.append(xi)\n",
        "\n",
        "        x = torch.cat(x, dim=1)\n",
        "        x = self.fuse(x)\n",
        "        x = self.cls(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "7FFhj4PaeQ0v"
      },
      "outputs": [],
      "source": [
        "# @title SegFormer\n",
        "class SegFormer(nn.Module):\n",
        "    def __init__(self, embed_dims, decoder_dim, num_classes, out_size):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder(embed_dims, decoder_dim, num_classes, out_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoPMCL2ye6YC",
        "outputId": "3d27b276-2550-40a5-8565-8012f02941eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "yes\n",
            "torch.Size([1, 19, 128, 128])\n"
          ]
        }
      ],
      "source": [
        "demo_image=torch.randn(1,3,512,512)\n",
        "model=SegFormer(embed_dims=[64, 128, 256, 512], decoder_dim=256, num_classes=19, out_size=(int(512/4),int(512/4)))\n",
        "with torch.no_grad():\n",
        "  print(model(demo_image).shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
