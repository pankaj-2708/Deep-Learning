{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tr9tnzXLvSiQ"
      },
      "outputs": [],
      "source": [
        "# @title Notes\n",
        "# https://miro.com/app/board/uXjVGekFzXg=/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kY5U7xQLwyN1"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from torchvision.ops.boxes import box_area\n",
        "from scipy.optimize import linear_sum_assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qaYgNCyw8nb"
      },
      "outputs": [],
      "source": [
        "# @title Global variables\n",
        "batch_size=-1\n",
        "embed_dim=-1\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPEJzTvhxF4k"
      },
      "outputs": [],
      "source": [
        "# @title CNN Backbone(resnet 50)\n",
        "class CNNBackbone(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model=torchvision.models.resnet50(pretrained=True)\n",
        "    for param in self.model.parameters():\n",
        "      param.requires_grad=False\n",
        "    self.model=nn.Sequential(*list(self.model.children())[:-2])\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sDaY6S9xxvF",
        "outputId": "157a4cdc-94fe-4074-9031-e08c0c960617"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 184MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 2048, 7, 7])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "demo_img=torch.rand((1,3,224,224))\n",
        "backbone=CNNBackbone()\n",
        "backbone(demo_img).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuQaHck3x6a4"
      },
      "outputs": [],
      "source": [
        "# @title Positional encoding\n",
        "def positional_encoding(dim,no_of_patches):\n",
        "  # out_shape=no_of_patches*dim\n",
        "  lst=torch.arange(dim)\n",
        "  lst=[10000**(2*i/dim) for i in lst]\n",
        "  out=[]\n",
        "  for i in range(no_of_patches):\n",
        "    ang=[i/(j+1e-8) for j in lst]\n",
        "    ang=[np.sin(a) if i%2==0 else np.cos(a) for i,a in enumerate(ang)]\n",
        "    out.append(ang)\n",
        "  return torch.tensor(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-1Sb13q3Myb",
        "outputId": "153e017c-4ebc-4c28-9d5e-5a8da216c057"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3511680957.py:9: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  ang=[np.sin(a) if i%2==0 else np.cos(a) for i,a in enumerate(ang)]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([49, 2048])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pe=positional_encoding(2048,49)\n",
        "pe.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Npp0S7Co3UwJ"
      },
      "outputs": [],
      "source": [
        "# @title EncoderLayer\n",
        "class EncoderLayer(nn.Module):\n",
        "  # here n is H*W\n",
        "  def __init__(self,embed_dim,n,device):\n",
        "    super().__init__()\n",
        "    self.pos_encoding=positional_encoding(embed_dim,n).to(device)\n",
        "    self.layer_norm1=nn.LayerNorm(embed_dim)\n",
        "    self.layer_norm2=nn.LayerNorm(embed_dim)\n",
        "    self.multihead_attention=nn.MultiheadAttention(embed_dim,num_heads=8,batch_first=True)\n",
        "    self.ffn=nn.Sequential(nn.Linear(embed_dim,embed_dim*4),nn.ReLU(),nn.Linear(embed_dim*4,embed_dim))\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    q=x+self.pos_encoding.unsqueeze(0)\n",
        "    k=x+self.pos_encoding.unsqueeze(0)\n",
        "    v=x\n",
        "    out=self.multihead_attention(q,k,v)[0]\n",
        "    out=self.layer_norm1(out+x)\n",
        "    out1=self.ffn(out)\n",
        "    out1=self.layer_norm2(out+out1)\n",
        "    return out1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWW1kic8BPNM",
        "outputId": "cfe0f07e-95ba-4abf-c1ee-1a53fb135f16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3511680957.py:9: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  ang=[np.sin(a) if i%2==0 else np.cos(a) for i,a in enumerate(ang)]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 49, 2048])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "demo_enc_input=torch.rand((1,49,2048))\n",
        "enc=EncoderLayer(2048,49,device)\n",
        "enc(demo_enc_input).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMGGSYjz8f0G"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  # n is H*W\n",
        "  def __init__(self,input_channel,embed_dim,enc_layers,n):\n",
        "    super().__init__()\n",
        "    self.base_conv=nn.Conv2d(input_channel,embed_dim,kernel_size=1)\n",
        "    self.encoder_layers=nn.ModuleList([EncoderLayer(embed_dim,n,device) for _ in range(enc_layers)])\n",
        "\n",
        "  def forward(self,x):\n",
        "    # x will be of shape B,C,H,W\n",
        "    z=self.base_conv(x)\n",
        "    B,C,H,W=z.shape\n",
        "    z=z.reshape(B,C,H*W)\n",
        "    z=z.permute(0,2,1)\n",
        "    for layer in self.encoder_layers:\n",
        "      z=layer(z)\n",
        "    return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byGob7EPD8de",
        "outputId": "c0fbfa33-6eeb-4771-8842-709a2e6a835f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3511680957.py:9: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  ang=[np.sin(a) if i%2==0 else np.cos(a) for i,a in enumerate(ang)]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 49, 768])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "enc_inp=backbone(demo_img)\n",
        "enc=Encoder(2048,768,12,49)\n",
        "enc_out=enc(enc_inp)\n",
        "enc_out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aB5n9q9OESJu"
      },
      "outputs": [],
      "source": [
        "# @title DecoderLayer -> not completed will be done after the vizura video because it is so confusing\n",
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self,obj_query_dim,no_of_obj_queries,n_heads,n_dim,no_of_patches) :\n",
        "    super().__init__()\n",
        "    self.pos_embedding=positional_encoding(obj_query_dim,no_of_obj_queries).to(device)\n",
        "    self.pos_embedding_learnable=nn.Embedding(n_dim,n_dim)\n",
        "    self.pos_embedding_learnable2=nn.Embedding(n_dim,n_dim)\n",
        "    self.mha1=nn.MultiheadAttention(obj_query_dim,num_heads=n_heads,batch_first=True)\n",
        "    self.mha2=nn.MultiheadAttention(obj_query_dim,num_heads=n_heads,batch_first=True)\n",
        "    self.layernorm1=nn.LayerNorm(obj_query_dim)\n",
        "    self.layernorm2=nn.LayerNorm(obj_query_dim)\n",
        "    self.layernorm3=nn.LayerNorm(obj_query_dim)\n",
        "    self.embed_input1=torch.arange(no_of_obj_queries)\n",
        "    self.embed_input2=torch.arange(no_of_patches)\n",
        "    self.mlp=nn.Sequential(nn.Linear(obj_query_dim,obj_query_dim*4),nn.ReLU(),nn.Linear(obj_query_dim*4,obj_query_dim))\n",
        "\n",
        "  def forward(self,obj_queries,enc_output):\n",
        "    pos_embed=self.pos_embedding_learnable(self.embed_input1)\n",
        "    key=obj_queries+pos_embed.unsqueeze(0)\n",
        "    query=obj_queries+pos_embed.unsqueeze(0)\n",
        "\n",
        "    out=self.mha1(query,key,obj_queries)[0]\n",
        "    out1_normed=self.layernorm1(out+obj_queries)\n",
        "\n",
        "    # cross attn\n",
        "    query=obj_queries+self.pos_embedding_learnable2(self.embed_input2).unsqueeze(0)\n",
        "    key=enc_output+self.pos_embedding.unsqueeze(0)\n",
        "    out2=self.mha2(query,key,enc_output)[0]\n",
        "    out2=out2+out1_normed\n",
        "\n",
        "    out2_normed=self.layernorm2(out2)\n",
        "    out2_mlp=self.mlp(out2_normed)\n",
        "    out2_mlp=out2_normed+out2_mlp\n",
        "\n",
        "    out2_mlp_normed=self.layernorm3(out2_mlp)\n",
        "    return out2_mlp_normed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lkulu_r2o-go"
      },
      "outputs": [],
      "source": [
        "# demo_obj_query=torch.rand((1,10,768))\n",
        "# dec_layer=DecoderLayer(768,10,8,768,49)\n",
        "# dec_layer(demo_obj_query,enc_out).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nJ9jnMip6QV"
      },
      "outputs": [],
      "source": [
        "# @title Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rG4nhrdIBlDc"
      },
      "outputs": [],
      "source": [
        "# @title utility function\n",
        "def box_cxcywh_to_xyxy(x):\n",
        "  x_c,y_c,w,h=x.unbind(-1)\n",
        "  b=[(x_c-0.5*w),(y_c-0.5*h),(x_c+0.5*w),(y_c+0.5*h)]\n",
        "  return torch.stack(b,dim=-1)\n",
        "\n",
        "def box_xyxy_to_cxcywh(x):\n",
        "  x0, y0, x1, y1 = x.unbind(-1)\n",
        "  b = [(x0 + x1) / 2, (y0 + y1) / 2,\n",
        "       (x1 - x0), (y1 - y0)]\n",
        "  return torch.stack(b, dim=-1)\n",
        "\n",
        "def box_iou(boxes1, boxes2):\n",
        "    area1 = box_area(boxes1)\n",
        "    area2 = box_area(boxes2)\n",
        "\n",
        "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
        "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
        "\n",
        "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
        "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
        "\n",
        "    union = area1[:, None] + area2 - inter\n",
        "\n",
        "    iou = inter / union\n",
        "    return iou, union\n",
        "\n",
        "\n",
        "def generalized_box_iou(boxes1, boxes2):\n",
        "    \"\"\"\n",
        "    Generalized IoU from https://giou.stanford.edu/\n",
        "\n",
        "    The boxes should be in [x0, y0, x1, y1] format\n",
        "\n",
        "    Returns a [N, M] pairwise matrix, where N = len(boxes1)\n",
        "    and M = len(boxes2)\n",
        "    \"\"\"\n",
        "    # degenerate boxes gives inf / nan results\n",
        "    # so do an early check\n",
        "    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()\n",
        "    assert (boxes2[:, 2:] >= boxes2[:, :2]).all()\n",
        "    iou, union = box_iou(boxes1, boxes2)\n",
        "\n",
        "    lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
        "    rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
        "\n",
        "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
        "    area = wh[:, :, 0] * wh[:, :, 1]\n",
        "\n",
        "    return iou - (area - union) / area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhUf1w5bzFM4"
      },
      "outputs": [],
      "source": [
        "# @title hugarian\n",
        "class hungarianMatcher(nn.module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def forward(self,outputs,targets):\n",
        "    \"\"\" Performs the matching\n",
        "\n",
        "        Params:\n",
        "            outputs: This is a dict that contains at least these entries:\n",
        "                 \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n",
        "                 \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n",
        "\n",
        "            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n",
        "                 \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth\n",
        "                           objects in the target) containing the class labels\n",
        "                 \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n",
        "\n",
        "        Returns:\n",
        "            A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
        "                - index_i is the indices of the selected predictions (in order)\n",
        "                - index_j is the indices of the corresponding selected targets (in order)\n",
        "            For each batch element, it holds:\n",
        "                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n",
        "        \"\"\"\n",
        "    bs,num_queries=outputs['pred_logits'].shape[:2]\n",
        "    out_prob=outputs['pred_logits'].flatten(0,1).softmax(-1)  # B*Q,C (c is no of classes)\n",
        "    out_bbox=outputs['pred_boxes'].flatten(0,1)  # B*Q,4\n",
        "\n",
        "    tgt_ids=torch.cat([tgt['labels'] for tgt in targets])  # sum(gt in each batch)\n",
        "    tgt_bbox=torch.cat([tgt['bbox'] for tgt in targets]) # sum(gt in each batch),4\n",
        "\n",
        "    cost_class=-out_prob[:,tgt_ids]  # B*Q,sum(gt in each batch)\n",
        "    # l1 dist\n",
        "    cost_bbox=torch.cdist(out_bbox,tgt_bbox,p=1)  # B*Q,sum(gt in each batch)\n",
        "\n",
        "    cost_giou=-generalized_box_iou(box_cxcywh_to_xyxy(out_bbox), box_cxcywh_to_xyxy(tgt_bbox))\n",
        "\n",
        "    C=cost_bbox+cost_class+cost_giou\n",
        "    C=C.view(bs,num_queries,-1).cpu()\n",
        "\n",
        "    sizes=[len(v['boxes']) for  v in targets]\n",
        "    indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
        "    return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
