{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa95650f-cb8f-4eb7-916c-eee011c37bda",
   "metadata": {},
   "source": [
    "# Class notes\n",
    "lecture 5 campus x dl playlist\n",
    "\n",
    "### Lecture 5: Perceptron Trick - How to Train a Perceptron\n",
    "\n",
    "This video, part of the 100 Days of Deep Learning course, focuses on how to **train the weights and biases of a Perceptron**. The goal is to develop mathematical intuition and then translate it into code, including an animation for step-by-step visualisation.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Recap & Problem Statement\n",
    "\n",
    "*   The previous video covered what a Perceptron is, its difference from a neuron, and how it makes predictions.\n",
    "*   **Perceptron Prediction**: A Perceptron model calculates `w0 + w1*x1 + w2*x2` (where `w0` is the bias or intercept, and `w1, w2` are weights for inputs `x1, x2`).\n",
    "*   If this sum is **positive** (`>=0`), the prediction is typically `1` (e.g., placement occurs).\n",
    "*   If the sum is **negative** (`<0`), the prediction is `0` (e.g., no placement).\n",
    "*   The crucial missing piece was how to find the optimal values for these weights (`w0, w1, w2`) to achieve accurate predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Linearly Separable Data\n",
    "\n",
    "*   The Perceptron is suitable for **linearly separable data**, meaning you can draw a straight line (or a hyperplane in higher dimensions) to classify the data into two distinct classes.\n",
    "*   The training process involves finding this optimal line that properly classifies both labels.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Identifying Positive and Negative Regions\n",
    "\n",
    "*   For a given line equation (e.g., `2x + 3y + 5 = 0`), how do you determine which side is the \"positive region\" and which is the \"negative region\"?\n",
    "*   You evaluate the expression `2x + 3y + 5`.\n",
    "    *   If `2x + 3y + 5 > 0`, that region is considered the **positive region**.\n",
    "    *   If `2x + 3y + 5 < 0`, that region is the **negative region**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Line Transformations based on Coefficients\n",
    "\n",
    "Changes in the coefficients of the line equation (`w0 + w1*x1 + w2*x2 = 0`) result in different types of line transformations:\n",
    "\n",
    "*   **Changing `w0` (the intercept/bias 'C')**:\n",
    "    *   **Increases or decreases `w0`**: The line moves **parallel** to its original position, either upwards or downwards. For `2x + 3y + C = 0`, increasing `C` moves the line downwards, while decreasing `C` moves it upwards.\n",
    "*   **Changing `w1` (coefficient of `x`) or `w2` (coefficient of `y`)**:\n",
    "    *   These changes cause the line to **rotate** around a point.\n",
    "    *   For example, changing the coefficient of `y` in `2x + y + 5 = 0` results in rotation.\n",
    "*   The Perceptron trick uses a combination of these transformations to move the line.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. The Perceptron Trick (Intuition for Weight Update)\n",
    "\n",
    "The core idea is to move the decision boundary (the line) towards a **misclassified point**.\n",
    "\n",
    "*   **Adding '1' to Coordinates**: To apply transformations, the Perceptron concept suggests adding a `1` as an additional coordinate (e.g., for a point `(x, y)`, it becomes `(x, y, 1)`) and using a similar representation for line coefficients. This allows the bias term (`w0`) to be treated similarly to other weights.\n",
    "*   **Adjusting Coefficients**: When a point is misclassified, the line's coefficients (`w0, w1, w2`) are adjusted.\n",
    "    *   If a **negative point** (actual output `0`) is in the **positive region** (predicted output `1`), you need to move the line such that this point falls into the negative region. The transformation involves **adding** the coordinates (multiplied by a learning rate) to the current weights.\n",
    "    *   If a **positive point** (actual output `1`) is in the **negative region** (predicted output `0`), you need to move the line such that this point falls into the positive region. The transformation involves **subtracting** the coordinates (multiplied by a learning rate) from the current weights.\n",
    "*   **Learning Rate (`alpha`)**: Instead of making huge transformations in one go, a **small number called the learning rate** (e.g., `0.01` or `0.001`) is used. All coordinate changes are multiplied by this learning rate to make small, incremental adjustments to the weights. This ensures smoother convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Perceptron Training Algorithm\n",
    "\n",
    "The training process involves an iterative loop to adjust weights.\n",
    "\n",
    "**a. Model Representation:**\n",
    "*   The model can be expressed as `w0*x0 + w1*x1 + w2*x2 = 0`, where `x0` is always `1` (to account for the bias `w0`).\n",
    "*   This can be represented as a **dot product** of the input vector `X = [x0, x1, x2]` and the weight vector `W = [w0, w1, w2]`.\n",
    "*   Prediction (`y_hat`) is `1` if `dot_product(X, W) >= 0`, and `0` if `dot_product(X, W) < 0`.\n",
    "\n",
    "**b. Initial (Conceptual) Algorithm:**\n",
    "1.  **Initialise weights** (`w0, w1, w2`) with random values.\n",
    "2.  Loop for a fixed number of **epochs** (e.g., 1000 times).\n",
    "3.  **Randomly select a data point** (student) from the training data.\n",
    "4.  **Check for misclassification**:\n",
    "    *   If a **negative point** (actual `0`) is classified as **positive** (model predicts `1`):\n",
    "        *   Update `W_new = W_old + learning_rate * X_old`.\n",
    "    *   If a **positive point** (actual `1`) is classified as **negative** (model predicts `0`):\n",
    "        *   Update `W_new = W_old - learning_rate * X_old`.\n",
    "    *   If correctly classified, no change to weights.\n",
    "\n",
    "**c. Simplified Algorithm (for Implementation):**\n",
    "To simplify the code, a single update rule can be used that incorporates both misclassification scenarios and handles correct classifications without explicit `if` conditions:\n",
    "\n",
    "*   **Prediction (y_hat)**:\n",
    "    *   `dot_product = w0 + w1*x1 + w2*x2` (using `x0=1` for `w0`).\n",
    "    *   `y_hat = 1` if `dot_product >= 0`, else `y_hat = 0`.\n",
    "*   **Weight Update Rule**:\n",
    "    *   `W_new = W_old + learning_rate * (actual_y - predicted_y) * X`\n",
    "\n",
    "    This single rule works as follows:\n",
    "    *   **Correct Prediction (e.g., `actual_y = 1`, `predicted_y = 1` OR `actual_y = 0`, `predicted_y = 0`)**:\n",
    "        *   `(actual_y - predicted_y)` will be `0`.\n",
    "        *   `W_new = W_old + learning_rate * 0 * X = W_old`. **No change** in weights, which is correct.\n",
    "    *   **False Negative (e.g., `actual_y = 1`, `predicted_y = 0`)**:\n",
    "        *   `(actual_y - predicted_y)` will be `(1 - 0) = 1`.\n",
    "        *   `W_new = W_old + learning_rate * 1 * X = W_old + learning_rate * X`. The weights are adjusted to make the output more positive.\n",
    "    *   **False Positive (e.g., `actual_y = 0`, `predicted_y = 1`)**:\n",
    "        *   `(actual_y - predicted_y)` will be `(0 - 1) = -1`.\n",
    "        *   `W_new = W_old + learning_rate * (-1) * X = W_old - learning_rate * X`. The weights are adjusted to make the output more negative.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Code Implementation Details\n",
    "\n",
    "The video demonstrates the implementation of the simplified Perceptron algorithm:\n",
    "\n",
    "*   **`perceptron` Function**: Takes input data `X` and output `Y` (where `Y` contains `0`s and `1`s) and returns the learned weights and intercept.\n",
    "*   **Initialisation**:\n",
    "    *   A `weights` array is created, typically initialised with ones (e.g., ``). The first element is for the intercept (`w0`), the second for `w1`, and the third for `w2`.\n",
    "    *   A `learning_rate` (e.g., `0.01`) and `epochs` (e.g., `1000`) are defined.\n",
    "*   **Data Preparation**: For `X = [x1, x2]`, a column of ones (`x0=1`) is added to `X` to account for the intercept term, effectively making `X = [1, x1, x2]`.\n",
    "*   **Training Loop**:\n",
    "    *   Iterates for the specified number of `epochs`.\n",
    "    *   In each iteration, a **random student index** is selected.\n",
    "    *   The **dot product** of the selected student's `X` vector and the current `weights` vector is calculated to get the `output`.\n",
    "    *   A **step function** converts this `output` into a binary `prediction` (`0` or `1`).\n",
    "    *   The **weight update formula** `weights = weights + learning_rate * (Y[random_index] - prediction) * X[random_index]` is applied.\n",
    "*   **Visualisation**: The code also visualises how the line moves with each update, showing it gradually converging to correctly classify the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "470b445f-abc9-47e4-8f5b-ba9805a947bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self,lr,epochs):\n",
    "        self.lr=lr\n",
    "        self.epochs=epochs\n",
    "        self.weights=None\n",
    "\n",
    "    def activate(self,i):\n",
    "        \n",
    "        return 1 if i>0 else 0\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        X=np.insert(X,0,1,axis=1)\n",
    "        self.weights=np.ones(X.shape[1])\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            i=np.random.randint(0,len(X))\n",
    "            y_hat=self.activate(X[i].T@self.weights)\n",
    "            self.weights=self.weights+self.lr*X[i]*(y[i]-y_hat)\n",
    "        return self.weights\n",
    "        \n",
    "    def predict(self,X):\n",
    "        X=np.insert(X,0,1,axis=1)\n",
    "        return [ self.activate(i)  for i in X @ self.weights ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c948cf-5e8c-4839-9e5c-a457d7bd9bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
