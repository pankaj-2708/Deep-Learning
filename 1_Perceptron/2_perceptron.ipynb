{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a25f46-5abc-42b4-9b2d-f75c0091899d",
   "metadata": {},
   "source": [
    "# Lecture notes \n",
    "class 6 dl playlist campusx\n",
    "# Perceptron Loss Functions\n",
    "\n",
    "## 1. Introduction to Perceptron\n",
    "\n",
    "*   A **Perceptron is fundamentally a mathematical model** based on a biological neuron.\n",
    "*   It processes inputs (e.g., CGPA, IQ) with associated weights and a bias.\n",
    "*   The core operation is a **dot product** of inputs and weights, summed with the bias, producing a value `z`.\n",
    "*   This `z` value then goes through a **step activation function** for binary classification:\n",
    "    *   If `z >= 0`, output is `1`.\n",
    "    *   If `z < 0`, output is `0`.\n",
    "*   **Geometric Intuition**: A Perceptron represents a line (in 2D), a plane (in 3D), or a hyperplane (in higher dimensions), separating data into two regions (positive and negative) for binary classification.\n",
    "*   Like other machine learning algorithms, Perceptrons have two phases:\n",
    "    1.  **Prediction**: Generating an output based on inputs.\n",
    "    2.  **Training**: Calculating optimal weights and biases to improve prediction accuracy.\n",
    "*   The **Perceptron Rule** was introduced as a preliminary, \"makeshift\" (jugaad) technique for training, where the line adjusts if a point is misclassified.\n",
    "\n",
    "## 2. Problems with the Perceptron Rule\n",
    "\n",
    "While the Perceptron Rule often works, it has significant drawbacks:\n",
    "\n",
    "*   **No Guarantee of Best Values**: It cannot definitively confirm that the obtained weights (`w1`, `w2`) and bias (`b`) represent the **absolute best line** for classification. Different runs might yield different satisfactory lines.\n",
    "*   **Lack of Quantification**: It does not provide a quantitative measure of **how good** the classification is. If a point is correctly classified, the line doesn't change, regardless of how close or far it is from the boundary.\n",
    "*   **Potential Convergence Issues**: In some rare scenarios, particularly with random point selection, the model might fail to converge to an optimal solution (e.g., if it repeatedly picks correctly classified points, the line won't move).\n",
    "*   Described as a \"jugaad\" that works **98% of the time**, but the remaining 2% (1% convergence issue, 1% uncertainty about optimality) are critical.\n",
    "\n",
    "## 3. Introduction to Loss Functions\n",
    "\n",
    "To overcome the Perceptron Rule's limitations, **Loss Functions** are used.\n",
    "\n",
    "*   **Definition**: A loss function is a method to **quantify** how well or poorly a machine learning model is performing on a given problem.\n",
    "*   For Perceptron, the loss function is typically a mathematical function of the weights (`w1`, `w2`) and bias (`b`).\n",
    "*   **Output**: For any given line (defined by `w1`, `w2`, `b`), the loss function outputs a **single number representing the error**.\n",
    "*   **Goal of Training**: The objective is to find the specific values of `w1`, `w2`, and `b` that **minimise this loss function's output**. This minimum value signifies the \"best\" line.\n",
    "*   **Examples of other Loss Functions**:\n",
    "    *   **Mean Squared Error (MSE)** for Linear Regression.\n",
    "    *   **Log Loss** for Logistic Regression.\n",
    "    *   **Hinge Loss** for Support Vector Machines (SVM).\n",
    "*   It is also possible to design **custom loss functions**.\n",
    "\n",
    "## 4. Developing a Loss Function for Perceptron\n",
    "\n",
    "Let's consider how to design a loss function for the Perceptron:\n",
    "\n",
    "*   **Simple Approach (Ineffective)**: Counting the **number of misclassified points**. This is too simplistic because all misclassifications are treated equally, regardless of how far a point is from the decision boundary.\n",
    "*   **Improved Approach**: Summing the **perpendicular distances of misclassified points** from the line. This accounts for the *magnitude* of the error (larger distance implies a greater error).\n",
    "*   **Perceptron's Practical Approach**: Instead of complex perpendicular distance calculations, Perceptron uses a value that is **directly proportional to the distance**. This is achieved by substituting the coordinates of a point into the line's equation (essentially a dot product operation). This method is computationally simpler than calculating exact distances.\n",
    "\n",
    "## 5. Perceptron's Actual Loss Function (Hinge Loss Variant)\n",
    "\n",
    "According to scikit-learn documentation for the Perceptron in SGD (Stochastic Gradient Descent), the loss function used is a variant of **Hinge Loss**.\n",
    "\n",
    "*   The loss function for Perceptron is given by:\n",
    "    $L(\\mathbf{w}, b) = \\frac{1}{N} \\sum_{i=1}^{N} \\max(0, -y_i \\cdot f(x_i))$\n",
    "    *   **$N$**: Number of data points (rows).\n",
    "    *   **$y_i$**: The **actual output label** for the $i$-th data point (e.g., +1 for one class, -1 for the other).\n",
    "    *   **$f(x_i)$**: Represents the output of the dot product for the $i$-th data point: $w_1x_{i1} + w_2x_{i2} + b$. This is often denoted as `z`.\n",
    "    *   The $\\max(0, \\cdot)$ term ensures that the loss is only incurred for misclassified points. If the term inside is negative or zero, the loss for that point is zero.\n",
    "\n",
    "*   **Mathematical Goal**: The training process aims to find the values of $w_1, w_2,$ and $b$ that **minimise** this loss function. This is denoted as $\\text{argmin}(\\mathbf{w}, b)$ of the loss function.\n",
    "\n",
    "## 6. Geometric Intuition of the Perceptron Loss Function\n",
    "\n",
    "Let's break down the term $\\max(0, -y_i \\cdot f(x_i))$ to understand its geometric meaning:\n",
    "\n",
    "*   **`y_i` (Actual Label)**:\n",
    "    *   If a point belongs to the positive class, $y_i = +1$.\n",
    "    *   If a point belongs to the negative class, $y_i = -1$.\n",
    "*   **`f(x_i)` (Model's Raw Output)**:\n",
    "    *   If the point falls on the positive side of the line, $f(x_i)$ is positive.\n",
    "    *   If the point falls on the negative side of the line, $f(x_i)$ is negative.\n",
    "\n",
    "We can analyse two main scenarios:\n",
    "\n",
    "1.  **Correctly Classified Point**:\n",
    "    *   If $y_i = +1$ and $f(x_i) > 0$ (positive class point on the positive side of the line).\n",
    "        *   Then $y_i \\cdot f(x_i)$ will be positive.\n",
    "        *   So, $-y_i \\cdot f(x_i)$ will be negative.\n",
    "        *   $\\max(0, \\text{negative number}) = \\mathbf{0}$.\n",
    "    *   If $y_i = -1$ and $f(x_i) < 0$ (negative class point on the negative side of the line).\n",
    "        *   Then $y_i \\cdot f(x_i)$ will be positive (negative times negative).\n",
    "        *   So, $-y_i \\cdot f(x_i)$ will be negative.\n",
    "        *   $\\max(0, \\text{negative number}) = \\mathbf{0}$.\n",
    "    *   **Conclusion**: **Correctly classified points contribute zero to the total loss**.\n",
    "\n",
    "2.  **Misclassified Point**:\n",
    "    *   If $y_i = +1$ but $f(x_i) < 0$ (positive class point on the negative side of the line).\n",
    "        *   Then $y_i \\cdot f(x_i)$ will be negative.\n",
    "        *   So, $-y_i \\cdot f(x_i)$ will be positive.\n",
    "        *   $\\max(0, \\text{positive number}) = \\text{positive number}$.\n",
    "    *   If $y_i = -1$ but $f(x_i) > 0$ (negative class point on the positive side of the line).\n",
    "        *   Then $y_i \\cdot f(x_i)$ will be negative.\n",
    "        *   So, $-y_i \\cdot f(x_i)$ will be positive.\n",
    "        *   $\\max(0, \\text{positive number}) = \\text{positive number}$.\n",
    "    *   **Conclusion**: **Misclassified points contribute a positive, non-zero value to the total loss**. The magnitude of this contribution is proportional to the point's \"distance\" from the decision boundary (calculated via the dot product).\n",
    "\n",
    "## 7. Training Perceptron using Gradient Descent\n",
    "\n",
    "To minimise the loss function and find the optimal weights and bias, **Gradient Descent** (an optimisation algorithm) is used.\n",
    "\n",
    "*   **Gradient Descent Algorithm**:\n",
    "    1.  **Initialisation**: Randomly initialise $w_1, w_2,$ and $b$.\n",
    "    2.  **Loop for a fixed number of epochs** (iterations):\n",
    "        *   For each data point $x_i, y_i$:\n",
    "            *   Calculate $f(x_i) = w_1x_{i1} + w_2x_{i2} + b$.\n",
    "            *   Check the misclassification condition: If $-y_i \\cdot f(x_i) > 0$ (i.e., the point is misclassified), then update the parameters.\n",
    "            *   **Update Rule**: The parameters are updated in the direction opposite to the gradient of the loss function. This helps move towards the minimum loss.\n",
    "                *   $w_1 \\leftarrow w_1 + \\text{learning\\_rate} \\times \\frac{\\partial L}{\\partial w_1}$\n",
    "                *   $w_2 \\leftarrow w_2 + \\text{learning\\_rate} \\times \\frac{\\partial L}{\\partial w_2}$\n",
    "                *   $b \\leftarrow b + \\text{learning\\_rate} \\times \\frac{\\partial L}{\\partial b}$\n",
    "            *   The `learning_rate` (e.g., 0.1) controls the step size in each update.\n",
    "\n",
    "*   **Partial Derivatives of the Perceptron Loss Function**:\n",
    "    *   For **misclassified points** (where $-y_i \\cdot f(x_i) > 0$):\n",
    "        *   $\\frac{\\partial L}{\\partial w_1} = -\\frac{1}{N} \\sum_{i=1}^{N} y_i x_{i1}$\n",
    "        *   $\\frac{\\partial L}{\\partial w_2} = -\\frac{1}{N} \\sum_{i=1}^{N} y_i x_{i2}$\n",
    "        *   $\\frac{\\partial L}{\\partial b} = -\\frac{1}{N} \\sum_{i=1}^{N} y_i$\n",
    "    *   For correctly classified points, the derivatives are 0.\n",
    "\n",
    "## 8. Perceptron's Flexibility: Different Behaviours\n",
    "\n",
    "The Perceptron is a **highly flexible mathematical model** due to its design. By changing its **activation function** and **loss function**, it can be adapted to solve various types of problems. This flexibility demonstrates that the same underlying model can perform different tasks.\n",
    "\n",
    "Here's a summary of how Perceptron components can be combined for different machine learning algorithms:\n",
    "\n",
    "| Algorithm Type            | Activation Function      | Loss Function               | Output                                   | Problem Type                 |\n",
    "| :------------------------ | :----------------------- | :-------------------------- | :--------------------------------------- | :--------------------------- |\n",
    "| **Perceptron**            | **Step Function**        | **Hinge Loss**              | Binary (1 or -1)                         | Binary Classification        |\n",
    "| **Logistic Regression**   | **Sigmoid Function**     | **Log Loss (Binary Cross-Entropy)** | Probabilities (0 to 1)                   | Binary Classification        |\n",
    "| **Softmax Regression**    | **Softmax Function**     | **Categorical Cross-Entropy** | Probabilities for each class             | Multi-Class Classification   |\n",
    "| **Linear Regression**     | **Linear (or None)**     | **Mean Squared Error**      | A continuous number                      | Regression                   |\n",
    "\n",
    "*   **Key takeaway**: The Perceptron provides a foundational mathematical model. By selectively altering its activation and loss functions, while consistently using **Stochastic Gradient Descent (SGD)** for parameter optimisation, it can be re-purposed for diverse tasks, including different types of classification and regression problems. This adaptability is crucial for building more complex neural networks later on.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6797a747-68dd-4a20-8970-28b3bcfb3619",
   "metadata": {},
   "source": [
    "# \"Problem with Perceptron\":\n",
    "\n",
    "lec 7 dl playist dscampusx\n",
    "***\n",
    "\n",
    "### Lecture Notes: The Problem with Perceptron\n",
    "\n",
    "**1. The Core Problem of Perceptron: Linearity Limitation**\n",
    "*   The primary reason Perceptrons did not become highly famous for deep learning is their **inability to work with non-linear data**.\n",
    "*   Perceptrons are only capable of operating on **linear data**.\n",
    "*   Even with extensive training time, a Perceptron will **never converge** if the data is non-linear, because its decision boundary is inherently linear.\n",
    "\n",
    "**2. Practical Demonstration of the Problem**\n",
    "\n",
    "The video practically demonstrates this limitation using code and a visual tool to show that a Perceptron cannot truly work with non-linear data.\n",
    "\n",
    "**2.1. Demonstration using Python Code (`sklearn` Perceptron)**\n",
    "*   Three distinct datasets were created for demonstration: **AND, OR, and XOR**.\n",
    "*   **AND Data:**\n",
    "    *   The output is `1` only when both inputs are `1`; otherwise, the output is `0`.\n",
    "    *   **Result:** The Perceptron successfully classified the AND data, easily dividing the two classes with a **clear linear boundary**.\n",
    "*   **OR Data:**\n",
    "    *   The output is `1` if any input is `1`; the output is `0` only when both inputs are `0`.\n",
    "    *   **Result:** The Perceptron successfully classified the OR data, separating the classes with a **single linear line**.\n",
    "*   **XOR Data:**\n",
    "    *   The output is `0` when both inputs are the same (both `0` or both `1`); the output is `1` when inputs are different (one `0` and one `1`).\n",
    "    *   **Result:** When the Perceptron was run on the XOR data, it **failed to find any clear-cut boundary**. This visually confirms that a single straight line cannot separate the classes in XOR data, highlighting the Perceptron's limitation with non-linear relationships.\n",
    "\n",
    "**2.2. Demonstration using TensorFlow Playground**\n",
    "*   This is a powerful web-based tool (`playground.tensorflow.org`) that allows users to create and train neural networks on various datasets without writing extensive code.\n",
    "*   **Linearly Separable Data (Example):**\n",
    "    *   A simple, linearly separable dataset was chosen where both classes are clearly separated.\n",
    "    *   **Result:** The Perceptron (simulated as a single-layer network with no hidden layers) quickly found a dividing line and provided correct results.\n",
    "*   **Non-linear Data (XOR Dataset):**\n",
    "    *   The XOR dataset (visualised as crosses and circles in a specific arrangement) was loaded.\n",
    "    *   **Result:** Even after running the simulation for a significant amount of time (many epochs), the Perceptron **could not separate the two classes**. The output did not converge, visually confirming the Perceptron's inability to handle non-linear data even when given ample training time.\n",
    "\n",
    "**3. Conclusion and Future Implications**\n",
    "*   The practical demonstrations clearly show that a Perceptron can only capture **linear relationships**.\n",
    "*   This fundamental limitation of Perceptrons with non-linear data was the primary reason for the **necessity of Multi-layer Perceptrons**.\n",
    "*   The next steps involve moving on to Multi-layer Perceptrons, where the concepts will become even more interesting.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5044e55b-aaff-4edc-8782-7fad6e57419c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
