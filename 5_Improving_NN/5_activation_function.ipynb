{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3afcea3a-75f4-43d3-aab9-94eeb6f2a79e",
   "metadata": {},
   "source": [
    "### Activation Functions in Neural Networks\n",
    "\n",
    "**1. What are Activation Functions?**\n",
    "\n",
    "*   An activation function acts as a **mathematical gate** between the input received by a neuron and the output it produces.\n",
    "*   In an Artificial Neural Network (ANN), each neuron performs a **weighted sum of its inputs**, adds a bias, and then passes this scalar value through a function, which is referred to as the activation function or transfer function.\n",
    "*   It **decides whether a neuron will be activated or not, and if so, to what extent**.\n",
    "\n",
    "**2. Why are Activation Functions Necessary?**\n",
    "\n",
    "*   **To introduce non-linearity:** If activation functions are not applied, the neural network will only be able to capture **linear data patterns**, behaving like a linear regression or logistic regression model. It would perform like a single line.\n",
    "*   **To solve non-linear problems:** Activation functions are crucial for performing non-linear regression and solving classification problems where data is not linearly separable.\n",
    "*   Without non-linear activation functions, the relationship between the input and output would be a first-degree polynomial (linear), making the model incapable of learning complex, non-linear relationships.\n",
    "*   The **Universal Approximation Theorem** states that non-linear activation functions allow a network with at least one hidden layer to approximate any continuous function.\n",
    "\n",
    "**3. Ideal Qualities of an Activation Function**\n",
    "\n",
    "An ideal activation function should possess the following qualities:\n",
    "\n",
    "*   **Non-linear:** Essential for capturing non-linear patterns in data.\n",
    "*   **Differentiable:** It must be possible to calculate its derivative, which is critical for **gradient descent** and the **backpropagation algorithm** used to update network weights during training.\n",
    "*   **Computationally Inexpensive:** Derivatives should be simple, easy, and fast to calculate to prevent slow training.\n",
    "*   **Zero-Centered:** The output (activation) of the function should have a **mean close to zero**. This normalisation helps the neural network converge faster. If the activations are not zero-centered (e.g., all positive or all negative), training can become slower and more restricted.\n",
    "*   **Non-Saturating:** A saturating function \"squeezes\" its input into a small, fixed range (e.g., 0-1 or -1 to 1). This can lead to the **Vanishing Gradient Problem**, where gradients become extremely small, preventing effective weight updates and stopping training. A non-saturating function, like ReLU in its positive region, does not suffer from this issue.\n",
    "\n",
    "**4. Types of Activation Functions**\n",
    "\n",
    "### A. Sigmoid Activation Function\n",
    "\n",
    "*   **Description:** The Sigmoid function produces an **S-shaped curve**.\n",
    "*   **Formula:** `sigmoid(x) = 1 / (1 + e^-x)`.\n",
    "*   **Output Range:** **0 to 1**. For very large positive inputs, the output approaches 1; for very large negative inputs, it approaches 0. If x is 0, the output is 0.5.\n",
    "*   **Derivative:** The derivative is at its maximum (0.25) when x is 0. For x values outside the range of approximately -3 to 3, the derivative becomes very close to zero.\n",
    "\n",
    "*   **Advantages:**\n",
    "    *   **Probabilistic Interpretation:** The 0-1 output range allows it to be interpreted as a probability, making it suitable for the **output layer in binary classification problems**.\n",
    "    *   **Non-linear:** It is a non-linear function, capable of capturing non-linear data patterns.\n",
    "    *   **Differentiable:** It is differentiable everywhere, making derivative calculation straightforward for backpropagation.\n",
    "\n",
    "*   **Disadvantages:**\n",
    "    *   **Saturating Function:** It squeezes inputs into the 0-1 range, making it a saturating function.\n",
    "        *   **Vanishing Gradient Problem:** This saturation leads to **vanishing gradients**. When the input `x` (weighted sum) is very large or very small, the derivative becomes almost zero. Consequently, weight updates during backpropagation become negligible, effectively stopping the training process. This is the **primary reason it is rarely used in hidden layers today**.\n",
    "    *   **Non-Zero Centered Output:** The output values are all positive (between 0 and 1), meaning the mean is not zero.\n",
    "        *   This results in **slower training** and convergence problems. When all activations are positive, the gradients for weights in subsequent layers will either all be positive or all be negative. This restricts the directions in which weights can be updated, forcing the optimisation algorithm to take a \"zigzag\" path, which increases training time.\n",
    "    *   **Computationally Expensive:** The exponential calculations in its formula make it computationally more intensive compared to simpler functions.\n",
    "\n",
    "### B. Tanh (Hyperbolic Tangent) Activation Function\n",
    "\n",
    "*   **Description:** The Tanh function also has an **S-shaped curve**, similar to Sigmoid.\n",
    "*   **Formula:** `tanh(x) = (e^x - e^-x) / (e^x + e^-x)`.\n",
    "*   **Output Range:** **-1 to 1**.\n",
    "*   **Derivative:** The derivative formula is `1 - tanh(x)^2`. The maximum value of its derivative is 1.\n",
    "\n",
    "*   **Advantages:**\n",
    "    *   **Non-linear:** It is a non-linear function capable of capturing complex relationships.\n",
    "    *   **Differentiable:** It is differentiable, allowing for easy derivative calculation.\n",
    "    *   **Zero-Centered Output:** Unlike Sigmoid, Tanh's output is **zero-centered** (ranging from -1 to 1), with both negative and positive activations. This helps in faster training and convergence compared to Sigmoid, as it alleviates the gradient restriction problem.\n",
    "\n",
    "*   **Disadvantages:**\n",
    "    *   **Saturating Function:** Like Sigmoid, Tanh is a saturating function.\n",
    "        *   **Vanishing Gradient Problem:** It still suffers from the **Vanishing Gradient Problem** for very large or very small inputs, as its derivative also approaches zero in those regions. This fundamental issue was not resolved by Tanh.\n",
    "    *   **Computationally Expensive:** It involves exponential calculations, making it computationally more expensive.\n",
    "\n",
    "### C. ReLU (Rectified Linear Unit) Activation Function\n",
    "\n",
    "*   **Description:** ReLU is the most widely used activation function in hidden layers today.\n",
    "*   **Formula:** `ReLU(x) = max(0, x)`.\n",
    "    *   For **negative input values (x < 0)**, the output is **0**.\n",
    "    *   For **positive input values (x >= 0)**, the output is **x**.\n",
    "\n",
    "*   **Advantages:**\n",
    "    *   **Non-linear:** Although it appears piecewise linear, ReLU is a non-linear function. Combining multiple ReLUs can create complex non-linear patterns.\n",
    "    *   **Non-Saturating (in positive region):** For positive inputs, ReLU does not saturate. This significantly mitigates the **Vanishing Gradient Problem**, as the gradient for positive inputs is constant (1), preventing it from becoming zero.\n",
    "    *   **Computationally Inexpensive:** Its simple formula (a simple comparison and output) makes it extremely **computationally efficient** for both forward and backward passes. No exponentials are involved.\n",
    "    *   **Faster Convergence:** Its non-saturating nature and computational efficiency lead to **faster training and convergence** compared to Sigmoid and Tanh.\n",
    "\n",
    "*   **Disadvantages:**\n",
    "    *   **Not Completely Differentiable:** ReLU is not differentiable at x = 0 (the point where the slope changes abruptly). However, this is typically handled in coding by assigning a derivative of 0 or 1 at x = 0.\n",
    "    *   **Not Zero-Centered:** Like Sigmoid, ReLU's output is not zero-centered (all activations are non-negative).\n",
    "        *   This issue can be addressed by using techniques like **Batch Normalisation**, which normalises the outputs of layers before passing them to the next layer.\n",
    "    *   **Dying ReLU Problem:** This is a problem where neurons can become \"inactive\" and stop learning. (This will be discussed in a subsequent video).\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91166bb3-86e6-4194-a000-87d2f3528361",
   "metadata": {},
   "source": [
    "### Activation Functions in Neural Networks (Part 2)\n",
    "\n",
    "**5. The Dying ReLU Problem**\n",
    "\n",
    "Despite its advantages, the Rectified Linear Unit (ReLU) activation function, commonly used in hidden layers, has a significant drawback known as the **Dying ReLU Problem**.\n",
    "\n",
    "*   **What is a \"Dead Neuron\"?**\n",
    "    *   Sometimes, if a ReLU activation function is used, certain neurons can output zero for **any input**.\n",
    "    *   This neuron effectively \"dies\" because it stops learning; its output does not change based on the input.\n",
    "    *   A dead neuron is permanently dead and will always output zero. It's as if the neuron is no longer part of the neural network.\n",
    "\n",
    "*   **Impact of the Dying ReLU Problem:**\n",
    "    *   If **more than 50%** of neurons experience this problem, the model's performance significantly degrades.\n",
    "    *   The model struggles to capture patterns and high-level representations in the data, leading to a **low-level representation** that is not effective.\n",
    "    *   In the worst-case scenario, if 100% of neurons die, the network essentially ceases to exist and learns nothing, making the application of a neural network pointless.\n",
    "\n",
    "*   **Why Does Dying ReLU Occur? (Mathematical Intuition)**\n",
    "    *   Consider a simple setup with two neurons, an input, a hidden unit, and an output layer.\n",
    "    *   The output of a ReLU neuron is `max(0, Z1)`, where `Z1 = W1*X1 + W2*X2 + B1` (weighted sum plus bias).\n",
    "    *   The problem arises when this weighted sum, `Z1`, becomes **negative**.\n",
    "    *   If `Z1 < 0`, then the ReLU output is 0, and more critically, its **derivative with respect to Z1 is also 0**.\n",
    "    *   During **backpropagation**, to update the weights (e.g., W1, W2), the update rule involves the **derivative of the loss with respect to the weights**. This calculation requires the derivative of the activation function with respect to `Z1`.\n",
    "    *   If this derivative (`dL/dZ1`) is 0, then the entire gradient for `W1` and `W2` becomes 0.\n",
    "    *   As a result, `W1` and `W2` (and potentially the bias `B1`) will **not be updated** in subsequent training cycles (`W_new = W_old - learning_rate * gradient`).\n",
    "    *   Since the weights are not updating, the neuron cannot learn, effectively becoming \"dead\".\n",
    "\n",
    "*   **Reasons for `Z1` Becoming Negative:**\n",
    "    *   **High Learning Rate:** If the learning rate is set too high, during weight updates, the weights (e.g., W1, W2) can become **very negative**. This can cause `Z1` (W1*X1 + W2*X2 + B1) to become negative in subsequent cycles.\n",
    "    *   **High Negative Bias:** If the bias term (`B1`) becomes very negative (either initialised as such or driven negative by a high learning rate during updates), it can pull the entire `Z1` sum into the negative region, regardless of the positive inputs.\n",
    "\n",
    "*   **Why is it Permanent?**\n",
    "    *   Once `Z1` becomes negative, the neuron's output is 0, and its gradient is 0.\n",
    "    *   Since the gradient is 0, the weights associated with that neuron (W1, W2) **will not update**.\n",
    "    *   Even if the input data (X1, X2) changes, the small, normalised input values (typically between 0 and 1) are usually **insufficient to overcome a very negative bias or very negative weights** and make `Z1` positive again.\n",
    "    *   Therefore, the neuron **cannot recover** and remains permanently dead.\n",
    "\n",
    "*   **Solutions to the Dying ReLU Problem:**\n",
    "    *   **Set Low Learning Rates:** Reduce the learning rate to prevent weights from becoming excessively negative.\n",
    "    *   **Set Positive Biases:** Initialise biases with a small positive value (e.g., 0.01).\n",
    "    *   **Use ReLU Variants:** Instead of standard ReLU, use its variants that are designed to mitigate this problem.\n",
    "\n",
    "**6. ReLU Variants**\n",
    "\n",
    "ReLU variants modify the function's behaviour for negative inputs to prevent the gradient from becoming zero. These fall into two categories: **linear variants** and **non-linear variants**.\n",
    "\n",
    "### A. Leaky ReLU (Linear Variant)\n",
    "\n",
    "*   **Formula:**\n",
    "    *   `f(x) = x` if `x >= 0`\n",
    "    *   `f(x) = 0.01x` if `x < 0`\n",
    "*   **Description:** For negative inputs, Leaky ReLU does not output zero, but a **small, non-zero fraction** (typically 0.01) of the input.\n",
    "*   **Derivative:**\n",
    "    *   `1` for `x >= 0`\n",
    "    *   `0.01` for `x < 0`\n",
    "    *   This ensures that a small gradient always flows through the network, even for negative inputs, preventing neurons from dying.\n",
    "\n",
    "*   **Advantages:**\n",
    "    *   **No Dying ReLU Problem:** The non-zero gradient for negative inputs ensures weights can still be updated.\n",
    "    *   **Non-Saturating:** It's unbounded on both sides, avoiding saturation.\n",
    "    *   **Computationally Inexpensive:** No exponential terms, making it fast to compute.\n",
    "    *   **Close to Zero-Centred:** Provides both negative and positive outputs, leading to a mean activation closer to zero than standard ReLU.\n",
    "\n",
    "*   **Disadvantage:**\n",
    "    *   The fixed slope of `0.01` for negative inputs is an arbitrary choice.\n",
    "\n",
    "### B. Parametric ReLU (PReLU) (Linear Variant)\n",
    "\n",
    "*   **Formula:**\n",
    "    *   `f(x) = x` if `x >= 0`\n",
    "    *   `f(x) = αx` if `x < 0`\n",
    "*   **Description:** PReLU is similar to Leaky ReLU, but the slope `α` for negative inputs is a **learnable parameter**.\n",
    "    *   The model learns the optimal value of `α` during training, adjusting it like other network weights.\n",
    "\n",
    "*   **Advantages:**\n",
    "    *   All advantages of Leaky ReLU.\n",
    "    *   **Increased Flexibility:** Allows the network to learn the best negative slope for the given data, potentially leading to better performance than a fixed `0.01`.\n",
    "\n",
    "*   **Disadvantage:**\n",
    "    *   No specific disadvantage mentioned apart from Leaky ReLU's general properties.\n",
    "\n",
    "### C. Exponential Linear Unit (ELU) (Non-Linear Variant)\n",
    "\n",
    "*   **Formula:**\n",
    "    *   `f(x) = x` if `x >= 0`\n",
    "    *   `f(x) = α(e^x - 1)` if `x < 0`\n",
    "    *   `α` is a hyperparameter, typically chosen between `0.1` and `0.3`.\n",
    "*   **Description:** For positive inputs, it behaves like ReLU. For negative inputs, it smoothly approaches `-α` exponentially.\n",
    "*   **Derivative:**\n",
    "    *   `1` for `x >= 0`\n",
    "    *   `αe^x` for `x < 0` (which is `f(x) + α`)\n",
    "\n",
    "*   **Advantages:**\n",
    "    *   **Better Performance:** Experiments have shown ELU often outperforms ReLU on various datasets.\n",
    "    *   **Close to Zero-Centred:** Outputs tend to be closer to zero mean, which aids faster convergence.\n",
    "    *   **No Dying ReLU Problem:** Provides non-zero gradients for negative inputs.\n",
    "    *   **Continuously Differentiable:** The function and its derivative are continuous everywhere, which can be beneficial for optimisation.\n",
    "\n",
    "*   **Disadvantage:**\n",
    "    *   **Computationally Expensive:** The exponential term (`e^x`) makes it more computationally intensive than ReLU or Leaky ReLU.\n",
    "\n",
    "### D. Scaled Exponential Linear Unit (SELU) (Non-Linear Variant)\n",
    "\n",
    "*   **Formula:**\n",
    "    *   `f(x) = λx` if `x >= 0`\n",
    "    *   `f(x) = λ(α(e^x - 1))` if `x < 0`\n",
    "    *   SELU is essentially ELU scaled by a fixed factor `λ`. The values for `λ` (approximately `1.0507`) and `α` (approximately `1.67326`) are pre-defined and fixed.\n",
    "*   **Description:** This function aims to normalise the activations of layers automatically.\n",
    "*   **Derivative:**\n",
    "    *   `λ` for `x >= 0`\n",
    "    *   `λ(αe^x)` for `x < 0`\n",
    "\n",
    "*   **Advantages:**\n",
    "    *   **Self-Normalising:** SELU is designed to normalise activations across layers automatically. This implies that the mean of activations tends towards zero and the variance towards one, which leads to **faster convergence** and **better generalisation** without explicit normalisation techniques like Batch Normalisation.\n",
    "    *   Includes all benefits of ELU.\n",
    "\n",
    "*   **Disadvantages:**\n",
    "    *   **Relatively New and Less Adopted:** As it is a recent development, it is not yet widely used in the industry due to less extensive research and understanding compared to more established functions.\n",
    "    *   **Complex Theory:** The original paper detailing SELU is quite complex, which might hinder its broader adoption.\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
