{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2575c92e-a50c-40ae-b67a-21dbd79569c2",
   "metadata": {},
   "source": [
    "## **Optimizers in Deep Learning: Adam (Adaptive Moment Estimation)**\n",
    "\n",
    "### **1. Definition and Significance**\n",
    "\n",
    "*   **Adam** stands for **Adaptive Moment Estimation**.\n",
    "*   It is currently considered the **most powerful optimization technique**.\n",
    "*   Adam is the most famous and widely used optimization technique.\n",
    "*   It is the standard choice for optimising Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), or Recurrent Neural Networks (RNN).\n",
    "\n",
    "### **2. Core Principles: Combining Two Key Ideas**\n",
    "\n",
    "Adam's strength comes from borrowing and combining successful concepts from preceding optimizers. The algorithm is based on combining two main ideas:\n",
    "\n",
    "1.  **Momentum:** The concept of using past gradients for the current update to accelerate descent.\n",
    "2.  **Learning Rate Decay/Adaptive Scaling:** The concept derived from AdaGrad and RMSProp, which allows the learning rate to be adapted for each parameter, overcoming issues related to sparse data.\n",
    "\n",
    "**Historical Context of Predecessors:**\n",
    "\n",
    "*   Standard techniques like Batch Gradient Descent are often too slow.\n",
    "*   **Momentum** improved speed but caused oscillations around the minimum.\n",
    "*   **NAG (Nesterov Accelerated Gradient)** introduced dampening to reduce these oscillations.\n",
    "*   **AdaGrad** was developed to handle sparse data/columns, allowing direct descent on stretched loss contours.\n",
    "*   However, AdaGrad's flaw was that the learning rate decayed too rapidly, causing updates to become negligible, potentially leading to a failure to reach the global minimum.\n",
    "*   **RMSProp** solved this problem by preventing the learning rate from becoming too small, ensuring complete convergence.\n",
    "\n",
    "By combining the **Momentum** concepts (like NAG) and the adaptive **Learning Rate Decay** concepts (like RMSProp), Adam generates a powerful hybrid algorithm.\n",
    "\n",
    "### **3. Mathematical Formulation**\n",
    "\n",
    "The full Adam algorithm involves calculating estimates of the first moment (mean of the gradients, $M_T$) and the second moment (uncentred variance of the squared gradients, $V_T$).\n",
    "\n",
    "#### **A. Calculating the Moments**\n",
    "\n",
    "Adam uses an Exponentially Weighted Average (similar to RMSProp) to calculate $M_T$ and $V_T$, meaning more weight is given to recent gradients.\n",
    "\n",
    "*   **First Moment ($M_T$):** This term handles the **Momentum** component.\n",
    "    $$\\mathbf{M_T} = \\beta_1 \\times M_{T-1} + (1 - \\beta_1) \\times (\\text{Gradient of Weight Vector})$$\n",
    "*   **Second Moment ($V_T$):** This term handles the **Adaptive Learning Rate** component, similar to how $V_T$ is used in RMSProp and AdaGrad.\n",
    "    $$\\mathbf{V_T} = \\beta_2 \\times V_{T-1} + (1 - \\beta_2) \\times (\\text{Gradient})^2$$\n",
    "\n",
    "#### **B. Recommended Hyperparameters**\n",
    "\n",
    "*   The general value for $\\beta_1$ is **0.9**.\n",
    "*   The general value for $\\beta_2$ is **0.99**.\n",
    "*   These beta values are configurable parameters that can be changed.\n",
    "*   The main hyperparameter that often requires less manual tuning in Adam is the learning rate, as it is handled automatically.\n",
    "\n",
    "#### **C. Bias Correction**\n",
    "\n",
    "*   Bias correction is applied after calculating $M_T$ and $V_T$.\n",
    "*   **Purpose:** $M_T$ and $V_T$ both start with initial zero values ($M_0 = 0, V_0 = 0$). This initial zero value introduces a bias in the beginning of the optimisation process which must be offset or changed.\n",
    "\n",
    "#### **D. Weight Update Formula**\n",
    "\n",
    "The final weight vector update formula includes the learning rate ($\\alpha$), the bias-corrected moment terms ($\\hat{M}_T$ and $\\hat{V}_T$), and a small constant ($\\epsilon$):\n",
    "$$W_{T+1} = W_T - \\alpha \\times \\frac{\\hat{M}_T}{\\sqrt{\\hat{V}_T} + \\epsilon}$$\n",
    "*(Note: $\\alpha$ is the Learning Rate; the source provides the key components and their order of calculation but the final weight update formula is shown to the user as a whole, including $W_{T+1}$ on the left-hand side).*\n",
    "\n",
    "### **4. Performance and Best Practices**\n",
    "\n",
    "*   Adam's animation demonstrates both the characteristics of **Momentum** and **Learning Rate Decay** behaviour.\n",
    "*   Adam performs exceptionally well when working with **complex neural networks** where **non-convex optimization** is occurring.\n",
    "*   It often converges faster than its predecessors.\n",
    "*   **Starting Point:** Adam is recommended as a **good starting point** for deep learning projects.\n",
    "*   **Alternative Optimizers:** If Adam does not yield satisfactory results, the next logical choices for testing are usually **RMSProp** or, occasionally, **Momentum**.\n",
    "*   There is no single \"clear answer\" for which optimizer is universally best; results depend on the specific data, necessitating hyperparameter tuning and testing.\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
