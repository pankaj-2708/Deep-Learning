{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "097757f0-f204-4fc8-92f2-2b4e6b8fd296",
   "metadata": {},
   "source": [
    "## **Optimizers in Deep Learning: RMSProp**\n",
    "\n",
    "### **1. Introduction and Definition**\n",
    "\n",
    "*   **RMSProp** stands for **Root Mean Square Prop**.\n",
    "*   It is an optimization technique that serves as an **improvement over AdaGrad** (Adaptive Gradient Algorithm).\n",
    "\n",
    "### **2. Motivation: The Problem with AdaGrad**\n",
    "\n",
    "RMSProp addresses a major disadvantage of the AdaGrad algorithm:\n",
    "\n",
    "*   **Use Case Context:** AdaGrad is typically used when dealing with **sparse data** (data where some columns contain many zero values). Sparse data results in an **elongated contour plot** of the loss function.\n",
    "*   **AdaGrad's Initial Role:** Standard optimizers (like standard Gradient Descent or Momentum) struggle with these stretched contours, leading to slow, inefficient movements. AdaGrad was introduced to solve this specific problem.\n",
    "*   **AdaGrad's Failure to Converge:** Despite its initial success, AdaGradâ€™s primary disadvantage is that it **reduces the learning rate (LR)**. After a certain point, the learning rate becomes so small that the resulting updates are very minor.\n",
    "*   **Result of Small Updates:** Due to extremely small updates, the AdaGrad algorithm may fail to converge completely, stopping prematurely and failing to reach the global minimum.\n",
    "\n",
    "### **3. Mathematical Basis of AdaGrad's Failure**\n",
    "\n",
    "The root cause of AdaGrad's convergence failure is how it calculates the divisor used in the update rule:\n",
    "\n",
    "*   **Learning Rate Division:** The learning rate for a parameter (e.g., $B$) is divided by a term, $V_T$.\n",
    "*   **$V_T$ Calculation:** In AdaGrad, $V_T$ is calculated as the **sum of the squares of all past gradients**. This includes every gradient from the start up to the current epoch.\n",
    "*   **The Issue:** Over many epochs, $V_T$ becomes **very large**. When a large $V_T$ divides the learning rate, the resulting overall update term becomes **very, very small** (almost negligible), which halts movement in that direction.\n",
    "\n",
    "### **4. RMSProp Mechanism and Mathematics**\n",
    "\n",
    "RMSProp's core change is designed to prevent $V_T$ from growing excessively large.\n",
    "\n",
    "*   **The Key Change:** RMSProp stops considering the entire history of gradients. Instead, it uses an **Exponentially Weighted Average** (or Exponentially Decaying Average) when calculating $V_T$.\n",
    "*   **Formula for $V_T$ (RMSProp):**\n",
    "    $$V_T = \\beta \\times V_{T-1} + (1 - \\beta) \\times (\\text{Gradient})^2$$\n",
    "*   **Beta ($\\beta$) Value:** The value for $\\beta$ is generally set to **0.95**, although it can be changed.\n",
    "*   **Impact of Exponential Averaging:**\n",
    "    1.  This method ensures that **more value is given to recent gradients**, while older gradients are \"forgotten\" (their influence is significantly down-weighted).\n",
    "    2.  As steps progress (epochs 1, 2, 3...), the gradient value from the earliest epoch (epoch 1) is multiplied by multiple terms that are less than one, making its contribution progressively smaller compared to the most recent gradient.\n",
    "    3.  $V_T$ is thereby prevented from \"shooting up\" (becoming overly large).\n",
    "    4.  Since $V_T$ remains reasonably sized, the learning rate **does not become too small**, and updates continue, allowing the algorithm to **converge completely**.\n",
    "\n",
    "### **5. Performance and Disadvantages**\n",
    "\n",
    "*   **Convex vs. Non-Convex Problems:**\n",
    "    *   In **convex optimization problems** (e.g., Linear Regression), AdaGrad and RMSProp behave similarly, and AdaGrad also converges to the global minimum.\n",
    "    *   In **non-convex optimization problems** involving complex neural networks, AdaGrad struggles to reach the global minimum, but **RMSProp converges successfully**.\n",
    "*   **Empirical Success:** RMSProp is empirically proven to be **one of the best optimization techniques** for neural networks.\n",
    "*   **Historical Context:** Before the introduction of Adam, RMSProp was the optimizer most commonly used for neural networks.\n",
    "*   **Current Status:** RMSProp still competes with Adam today and is used when Adam does not produce sufficient results.\n",
    "*   **Disadvantages:** RMSProp is stated to have **no disadvantages**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc515faa-7b88-47e0-bd4e-71ccc4ab0e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop, Adagrad\n",
    "\n",
    "model.compile(\n",
    "    optimizer=RMSprop(learning_rate=0.001, rho=0.9),  \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adagrad(learning_rate=0.01, epsilon=1e-07),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
