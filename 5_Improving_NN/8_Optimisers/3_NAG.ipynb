{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eadd44d0-46e9-4ceb-898a-24a766dcf7d1",
   "metadata": {},
   "source": [
    "## Lecture Notes: SGD with Momentum & Nesterov Accelerated Gradient (NAG)\n",
    "\n",
    "### 5. Nesterov Accelerated Gradient (NAG)\n",
    "\n",
    "#### 5.1. Introduction to NAG\n",
    "*   **Nesterov Accelerated Gradient (NAG)** is an optimisation technique that represents a **small but significant upgrade to Momentum**.\n",
    "*   It generally **performs better than Momentum**.\n",
    "*   The primary goal of NAG is to **reduce the oscillations** observed in Momentum, thereby speeding up convergence.\n",
    "\n",
    "#### 5.2. Intuition and Geometric Interpretation of NAG (Look-Ahead Gradient)\n",
    "*   NAG's core idea is to be more **\"intelligent\"** or **\"foresighted\"** about where to calculate the gradient.\n",
    "*   Instead of calculating the gradient at the current position (`W_t-1`), NAG **first applies the momentum term** to project to a **\"look-ahead\" point**.\n",
    "*   The gradient is then calculated at this **projected look-ahead point**, and this look-ahead gradient is used to update the weights.\n",
    "*   **Analogy**: Imagine a ball rolling down a hill. Momentum takes a step based on its current speed and the slope right where it is. NAG, however, first predicts where the momentum *alone* would take it next, then it calculates the slope *at that predicted future position*, and uses *that* slope to guide its step. This allows it to \"anticipate\" the terrain and adjust its path sooner, reducing overshooting.\n",
    "*   This \"look-ahead\" mechanism helps in **damping the oscillations** that are characteristic of standard Momentum.\n",
    "\n",
    "#### 5.3. Mathematical Implementation of NAG\n",
    "NAG modifies the Momentum update by calculating the gradient at a predicted future position:\n",
    "1.  **Calculate Look-Ahead Point (`W_lookahead`)**:\n",
    "    *   `W_lookahead = W_t-1 - β * V_t-1`\n",
    "    *   This step projects the current weights by applying only the accumulated momentum (velocity) from the previous step.\n",
    "2.  **Calculate Velocity (`V_t`) using Look-Ahead Gradient**:\n",
    "    *   `V_t = β * V_t-1 + η * ∇L(W_lookahead)`\n",
    "    *   Notice that the gradient `∇L` is calculated at `W_lookahead` (the future position), not `W_t-1` (the current position).\n",
    "3.  **Update Weights (`W_t`)**:\n",
    "    *   `W_t = W_t-1 - V_t`\n",
    "    *   The final weight update is still applied from the current position using the newly calculated velocity.\n",
    "\n",
    "#### 5.4. Advantages of NAG\n",
    "*   **Reduced Oscillations**: By anticipating the future gradient, NAG is better at slowing down as it approaches the minimum, significantly reducing the overshooting and oscillations seen in Momentum.\n",
    "*   **Faster Convergence**: Due to reduced oscillations, NAG can converge faster to the minimum than Momentum, especially on complex loss surfaces.\n",
    "*   It is an improvement upon the basic Momentum technique.\n",
    "\n",
    "#### 5.5. Disadvantage of NAG\n",
    "*   **Risk of Getting Stuck in Local Minima**: The damping of oscillations, while generally beneficial, can be a disadvantage in certain scenarios. If the loss landscape has shallow local minima, NAG's reduced momentum might prevent it from gaining enough speed to \"jump over\" these minima, potentially causing it to get stuck. Momentum, with its higher oscillations, might have sufficient momentum to escape such points. In such cases, other optimisers might be more suitable.\n",
    "\n",
    "### 6. Implementation in Keras\n",
    "\n",
    "Both SGD with Momentum and NAG can be easily implemented in Keras using the `SGD` class.\n",
    "```python\n",
    "import tensorflow as tf\n",
    "tf.keras.optimisers.SGD(learning_rate=0.01,momentum=0,nestrov=False,name=\"SGD\",**kwargs)\n",
    "```\n",
    "\n",
    "*   **For standard SGD**:\n",
    "    *   Set `momentum = 0`.\n",
    "    *   Set `nesterov = False`.\n",
    "\n",
    "*   **For SGD with Momentum**:\n",
    "    *   Provide a value for `momentum` (e.g., `0.9`).\n",
    "    *   Set `nesterov = False`.\n",
    "\n",
    "*   **For Nesterov Accelerated Gradient (NAG)**:\n",
    "    *   Provide a value for `momentum` (e.g., `0.9`).\n",
    "    *   Set `nesterov = True`.\n",
    "\n",
    "***\n",
    "\n",
    "Would you like to review the mathematical formulations of Momentum and NAG again, perhaps with an emphasis on their differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be924b9e-1ad3-48ce-a43d-b79422f82ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
