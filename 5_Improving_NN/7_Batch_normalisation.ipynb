{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d51bcc28-26a0-4e3a-a4e4-3737c2d1585a",
   "metadata": {},
   "source": [
    "## Deep Learning: Batch Normalisation\n",
    "\n",
    "### 1. Introduction to Batch Normalisation\n",
    "\n",
    "**Batch Normalisation** is an **algorithmic method** designed to **speed up neural network training** and enhance its **stability**. This technique was introduced in 2015 and has since become widely adopted in neural networks.\n",
    "\n",
    "The core idea involves **normalising the activation vectors from hidden layers** by using the mean and variance calculated from the current training mini-batch. This normalisation step is typically applied **right before or right after a non-linear activation function**. By normalising these activations (aiming for a mean of 0 and standard deviation of 1), it effectively provides \"normalised inputs\" to subsequent layers, analogous to how input data is generally pre-processed.\n",
    "\n",
    "### 2. Why Use Batch Normalisation? (Necessity and Benefits)\n",
    "\n",
    "Batch Normalisation addresses several critical challenges in deep learning training:\n",
    "\n",
    "#### a) Analogy with Input Normalisation for Faster and More Stable Training\n",
    "\n",
    "*   **Problem with Unnormalised Inputs**: When input features have vastly different scales (e.g., CGPA on a 1-10 scale, IQ on a 1-100 scale), using unnormalised data can lead to a **cost function contour that is \"stretched\" or non-uniform**. This means the contour appears elongated in some directions and compressed in others.\n",
    "*   **Consequences**: A stretched cost function contour **prevents the use of high learning rates** because it can cause the optimisation algorithm to \"overshoot\" the minimum in the stretched direction, leading to **slow and unstable training**. Optimisers must take small steps, resulting in longer convergence times.\n",
    "*   **Solution**: Normalising inputs (typically to a mean of 0 and standard deviation of 1) transforms the cost function contour into a more **uniform and spherical shape**. This allows for **stable and faster convergence** even with higher learning rates, as the optimiser can take larger, more efficient steps towards the minimum.\n",
    "*   **Batch Norm's Extension**: The intuition is that if normalising the initial network inputs is beneficial, then normalising the *outputs* of hidden layers (which serve as inputs to subsequent layers) should similarly make training faster and more stable.\n",
    "\n",
    "#### b) Addressing Internal Covariate Shift (The Primary Motivation)\n",
    "\n",
    "*   **Covariate Shift**: This occurs when the **distribution of input columns changes** between the training and testing phases. Even if the relationship between inputs (X) and outputs (Y) remains the same, a model trained on one input distribution might perform poorly on a new distribution, necessitating re-training. An example is training a model to distinguish red roses from other flowers, then testing it with roses of various colours; the input distribution has shifted.\n",
    "*   **Internal Covariate Shift (ICS)**: This is a specific type of covariate shift that occurs *within* a neural network during training.\n",
    "    *   **Definition**: It is \"the change in the distribution of network activations due to the changing input parameters during training\".\n",
    "    *   **Mechanism**: As a neural network trains, the weights of earlier layers are constantly updated. These updates modify the *outputs* of those layers, which then become the *inputs* for subsequent layers. Consequently, the **distribution of inputs to later layers is continuously shifting**, making it difficult for these deeper layers to learn stable mappings. This instability means that a layer constantly has to adapt to new input distributions from the preceding layers.\n",
    "    *   **Consequences of ICS**: Leads to **unstable training** and **slow convergence**. It also forces the use of **lower learning rates** and demands **very careful weight initialisation**.\n",
    "*   **Batch Norm's Solution to ICS**: By normalising the activations of *every hidden layer* (specifically, their weighted sums before activation) to have a mean of 0 and a standard deviation of 1, Batch Normalisation **ensures a consistent input distribution for subsequent layers**. This \"stable ground\" allows the rest of the network to learn more effectively, thereby **reducing the impact of ICS** and leading to faster, more stable learning.\n",
    "\n",
    "#### c) Other Significant Advantages\n",
    "\n",
    "1.  **More Stable Training**: Batch Normalisation allows for a **wider range of hyperparameter values** (e.g., learning rates, weight initialisation schemes) without destabilising the training process.\n",
    "2.  **Faster Training**: It enables the use of **higher learning rates**, which results in faster convergence and requires **fewer epochs** to achieve optimal accuracy.\n",
    "3.  **Regularisation Effect**: Batch Normalisation can act as a **weak regulariser**, helping to **reduce overfitting**. This occurs because the mean and standard deviation are calculated per batch, introducing a slight randomness or noise into the activations. This noise discourages the model from becoming overly reliant on specific input patterns present in a single batch. However, it is **not a strong regulariser** like dropout and should not replace dedicated regularisation techniques.\n",
    "4.  **Reduced Importance of Weight Initialisation**: Batch Normalisation makes the training process **less sensitive to the initialisation of weights**. By normalising activations, it effectively improves the shape of the cost function contour, making it easier for the network to find the optimal solution regardless of the initial weight values.\n",
    "\n",
    "### 3. How Batch Normalisation Works (During Training)\n",
    "\n",
    "Batch Normalisation is applied on a **layer-by-layer basis** and is primarily used with **mini-batch gradient descent**. It is an optional technique; you can choose which layers to apply it to.\n",
    "\n",
    "For a given layer, the process typically involves two main stages for each neuron:\n",
    "\n",
    "#### Stage 1: Normalising the Weighted Sum (Z)\n",
    "\n",
    "1.  **Calculate Weighted Sum (Z)**: For each neuron in the layer, first calculate the weighted sum of its inputs (`Z = WX + B`).\n",
    "2.  **Mini-Batch Processing**: Batch Normalisation operates on a **mini-batch** of data points. If the mini-batch size is `m` (e.g., 4 points), you get `m` weighted sums (`Z`) for each neuron.\n",
    "3.  **Calculate Batch Mean (μ_B)**: Compute the mean of these `m` weighted sums for the current neuron across the mini-batch.\n",
    "    *   Formula: `μ_B = (1/m) * Σ Z_i` (where `i` goes from 1 to `m`).\n",
    "4.  **Calculate Batch Variance (σ_B²)**: Compute the variance of these `m` weighted sums for the current neuron across the mini-batch.\n",
    "    *   Formula: `σ_B² = (1/m) * Σ (Z_i - μ_B)²`.\n",
    "5.  **Normalise Z**: Use the calculated `μ_B` and `σ_B` to normalise each `Z_i` in the mini-batch.\n",
    "    *   Formula: `Z_normalised_i = (Z_i - μ_B) / √(σ_B² + ε)`.\n",
    "    *   An epsilon (`ε`) term is added to the denominator to **prevent division by zero**.\n",
    "    *   After this step, the `Z_normalised` values will have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "#### Stage 2: Scaling and Shifting with Learnable Parameters (Gamma and Beta)\n",
    "\n",
    "1.  **Scaling and Shifting**: The normalised `Z` values (`Z_normalised`) are then scaled by a learnable parameter `γ` (gamma) and shifted by another learnable parameter `β` (beta).\n",
    "    *   Formula: `Z_output = γ * Z_normalised + β`.\n",
    "    *   Initially, `γ` is typically 1, and `β` is 0 in Keras.\n",
    "2.  **Flexibility**: These `γ` and `β` parameters provide **flexibility** to the neural network. While normalisation brings values to a mean of 0 and std dev 1, the network might sometimes prefer a different distribution for optimal learning. `γ` and `β` allow the network to **undo the normalisation** if it's not beneficial, or to scale and shift the normalised values to a more optimal distribution.\n",
    "3.  **Learnable Parameters**: `γ` and `β` are **learnable parameters** that are updated during backpropagation, just like weights (W) and biases (B). Each neuron has its **own independent `γ` and `β` parameters**.\n",
    "4.  **Activation**: Finally, the `Z_output` (after scaling and shifting) is passed through the chosen activation function (e.g., ReLU, Tanh) to produce the neuron's activation.\n",
    "\n",
    "### 4. How Batch Normalisation Works (During Testing/Prediction)\n",
    "\n",
    "During training, `μ_B` and `σ_B²` are calculated from the current mini-batch. However, during testing or prediction, you typically **only have a single input point** (not a mini-batch), making it impossible to calculate batch-specific mean and variance.\n",
    "\n",
    "To address this, Batch Normalisation uses **Exponentially Weighted Moving Averages (EWMA)**:\n",
    "\n",
    "1.  **EWMA Tracking During Training**: Throughout the training process, the algorithm maintains **moving averages of the batch means (μ_EWMA)** and **moving averages of the batch variances (σ_EWMA²)** across all mini-batches and epochs. These are non-learnable parameters.\n",
    "2.  **Using EWMA for Testing**: When the model is deployed for testing or prediction, instead of calculating batch-specific means and variances, it uses the **final, aggregated `μ_EWMA` and `σ_EWMA²` values** collected during training.\n",
    "3.  **Inference Calculation**: For a new test input `X_test`, the normalisation becomes:\n",
    "    *   `Z_normalised = (Z_test - μ_EWMA) / √(σ_EWMA² + ε)`\n",
    "    *   `Z_output = γ * Z_normalised + β`\n",
    "    *   The `γ` and `β` parameters are the **final learned values** from training.\n",
    "\n",
    "In summary, during training, Batch Normalisation uses **batch-specific statistics** (μ_B, σ_B²), and during testing, it uses **global statistics** (μ_EWMA, σ_EWMA²) estimated from the training set.\n",
    "\n",
    "A Batch Normalisation layer typically involves **four parameters per neuron**:\n",
    "*   **Learnable Parameters**: `γ` (gamma) and `β` (beta).\n",
    "*   **Non-learnable Parameters**: `μ_EWMA` (moving average of mean) and `σ_EWMA²` (moving average of variance).\n",
    "Therefore, for a hidden layer with 3 units, a Batch Normalisation layer would have `3 * 4 = 12` parameters, with 6 being learnable and 6 non-learnable.\n",
    "\n",
    "### 5. Exponentially Weighted Moving Averages (EWMA)\n",
    "\n",
    "When training with mini-batches, the **batch mean** and **batch variance** fluctuate.  \n",
    "To stabilize inference, Batch Normalization keeps **running averages** using **Exponentially Weighted Moving Averages (EWMA)**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔹 Formula for EWMA\n",
    "\n",
    "- **Running Mean**  \n",
    "```\n",
    "μ_t = momentum * μ_(t-1) + (1 - momentum) * μ_batch\n",
    "```\n",
    "\n",
    "- **Running Variance**  \n",
    "```\n",
    "σ²_t = momentum * σ²_(t-1) + (1 - momentum) * σ²_batch\n",
    "```\n",
    "\n",
    "where:\n",
    "- `μ_(t-1), σ²_(t-1)` → previous running estimates  \n",
    "- `μ_batch, σ²_batch` → mean/variance of the current batch  \n",
    "- `momentum` → smoothing factor (default in Keras = 0.99)  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Intuition\n",
    "- `momentum = 0.9` → **90% old value + 10% new value**  \n",
    "- `momentum = 0.99` → updates are **slower** → smoother estimates  \n",
    "- Helps reduce noise from mini-batches and makes inference stable  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 In Keras\n",
    "```python\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "bn = BatchNormalization(momentum=0.99, epsilon=1e-3)\n",
    "```\n",
    "\n",
    "- `momentum=0.99` →  \n",
    "  ```\n",
    "  μ_t = 0.99 * μ_(t-1) + 0.01 * μ_batch\n",
    "  ```\n",
    "\n",
    "---\n",
    "### 6. Keras Implementation\n",
    "\n",
    "Implementing Batch Normalisation in Keras is straightforward:\n",
    "\n",
    "*   You simply add a `BatchNormalization` layer to your model using `model.add(BatchNormalization())`.\n",
    "*   It is typically placed **after a convolutional or dense layer and before the activation function**, although placing it after the activation is also an option. The source specifically mentions applying it \"right before or right after non-linear function\".\n",
    "*   **Example Code Structure**:\n",
    "    ```python\n",
    "    model = Sequential()\n",
    "    model.add(Dense(3, activation='relu', input_dim=2)) # First hidden layer\n",
    "    model.add(BatchNormalization()) # Batch Normalization layer\n",
    "    model.add(Dense(2, activation='relu')) # Second hidden layer\n",
    "    model.add(BatchNormalization()) # Batch Normalization layer\n",
    "    model.add(Dense(1, activation='sigmoid')) # Output layer\n",
    "    ```\n",
    "*   When a `BatchNormalization` layer is added, Keras automatically manages the learnable `gamma` and `beta` parameters, as well as the non-learnable `moving_mean` and `moving_variance` parameters (the EWMAs).\n",
    "*   Experiments show that models using `BatchNormalization` achieve **higher accuracy** and converge to that accuracy **much faster** (in fewer epochs) compared to models without it.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339cd595-b3a8-40cd-a529-c14cad6b97a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
