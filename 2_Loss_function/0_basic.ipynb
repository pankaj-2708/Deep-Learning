{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf6af9a7-a2d2-467e-8f4a-45dcdb42ce40",
   "metadata": {},
   "source": [
    "### Lecture Notes: Loss Functions in Deep Learning\n",
    "\n",
    "lecture for 14 dl\n",
    "#### 1. Introduction to Loss Functions\n",
    "*   **Definition**: A loss function is a mechanism for evaluating how well an algorithm's model is performing on a given dataset. It helps determine if the algorithm is working well or poorly.\n",
    "*   **Nature**: Loss functions are mathematical functions that depend on the parameters of the machine learning algorithm.\n",
    "*   **Performance Indication**:\n",
    "    *   A **large output value** from the loss function means the algorithm is performing poorly.\n",
    "    *   A **small output value** from the loss function means the algorithm is performing well.\n",
    "*   **Adjusting Parameters**: By adjusting the algorithm's parameters (e.g., weights and biases in a neural network), the value of the loss function changes. The goal is to find parameter values that minimise this loss function.\n",
    "\n",
    "#### 2. Importance of Loss Functions\n",
    "*   **The \"You Can't Improve What You Can't Measure\" Principle**: Loss functions are crucial because they quantify the error or \"mistake\" of the model, providing a measurable metric for improvement.\n",
    "*   **Guiding the Algorithm**: Loss functions act as the \"eye of the algorithm\". They tell the machine learning algorithm:\n",
    "    *   Where to go.\n",
    "    *   What parameter values to set.\n",
    "    *   How much to adjust parameters to reduce error.\n",
    "*   **Role in Deep Learning Model Training (Backpropagation)**:\n",
    "    *   **Forward Propagation**: An input (e.g., student's CGPA and IQ) is fed into the neural network with initial random weights and biases. The network makes a prediction (output).\n",
    "    *   **Loss Calculation**: A chosen loss function calculates the error between the predicted output and the true value for that single input.\n",
    "    *   **Backpropagation & Gradient Descent**: Based on this loss value, **gradient descent** is used to adjust the weights and biases of the neural network.\n",
    "    *   **Iteration**: This process is repeated for multiple inputs (or batches of inputs) across the entire dataset for multiple epochs.\n",
    "    *   **Goal**: The ultimate aim is to minimise the loss function's value, which signifies that the model is well-trained and has learned the optimal weights and biases.\n",
    "\n",
    "#### 3. Loss Function vs. Cost Function\n",
    "*   **Loss Function**:\n",
    "    *   Calculated for a **single training example**.\n",
    "    *   Measures the error for one prediction.\n",
    "    *   Sometimes referred to as an \"error function\".\n",
    "*   **Cost Function**:\n",
    "    *   Calculated over an **entire batch** or the **entire training dataset**.\n",
    "    *   It is typically the average of the loss functions across all training examples in a batch or the dataset.\n",
    "    *   For example, in Mean Squared Error (MSE), the loss for a single point is `(y_true - y_pred)^2`, while the cost for `N` points is `(1/N) * sum((y_true_i - y_pred_i)^2)`.\n",
    "\n",
    "#### 4. Types of Loss Functions in Deep Learning\n",
    "Loss functions vary depending on the type of problem being solved.\n",
    "\n",
    "*   **For Regression Problems**:\n",
    "    *   **Mean Squared Error (MSE)**\n",
    "    *   **Mean Absolute Error (MAE)**\n",
    "    *   **Huber Loss**\n",
    "*   **For Classification Problems**:\n",
    "    *   **Binary Cross Entropy** (for binary classification)\n",
    "    *   **Categorical Cross Entropy** (for multi-class classification)\n",
    "    *   **Hinge Loss** (often used in Support Vector Machines)\n",
    "    *   **Sparse Categorical Cross Entropy** (for multi-class classification with integer-encoded labels)\n",
    "*   **For Other Problem Types**:\n",
    "    *   **Autoencoders**: KL Divergence\n",
    "    *   **Generative Adversarial Networks (GANs)**: Discriminator Loss, Max GAN Loss\n",
    "    *   **Object Detection**: Focal Loss\n",
    "    *   **Embeddings**: Triplet Loss\n",
    "*   **Custom Loss Functions**: Researchers can create their own loss functions, and frameworks like Keras allow for the implementation of custom loss functions.\n",
    "\n",
    "#### 5. Detailed Discussion of Key Loss Functions\n",
    "\n",
    "##### 5.1. Mean Squared Error (MSE)\n",
    "*   **Also Known As**: Squared Loss, L2 Loss.\n",
    "*   **Application**: Commonly used in **regression problems**.\n",
    "*   **Formula (for a single data point)**: `(y_true - y_pred)^2`.\n",
    "    *   `y_true`: The actual (true) value.\n",
    "    *   `y_pred`: The predicted value by the model.\n",
    "*   **Why Squaring?**\n",
    "    *   Ensures all error values are positive, preventing positive and negative errors from cancelling each other out when summed.\n",
    "    *   **Magnifies larger errors**: Errors are penalised quadratically; an error of 2 units becomes 4 units squared, and an error of 4 units becomes 16 units squared. This means points further from the true value have a disproportionately higher impact on the loss.\n",
    "    *   **Impact on Weight Updates**: Outlier points with large errors cause more drastic updates to weights and biases, guiding the model strongly towards these points.\n",
    "*   **Advantages**:\n",
    "    *   **Easy to interpret**.\n",
    "    *   **Always differentiable**: The loss function graph is smooth and convex, making it ideal for gradient descent algorithms.\n",
    "    *   **Single global minimum**: There is only one unique set of weights and biases for which the MSE is minimised, simplifying optimisation.\n",
    "*   **Disadvantages**:\n",
    "    *   **Squared Unit**: The unit of the error is squared (e.g., if the output is in 'LPA', the error is in 'LPA squared'), which can be less intuitive to understand.\n",
    "    *   **Not Robust to Outliers**: Due to the quadratic penalty, outliers heavily influence the model, pulling the regression line significantly towards them, potentially leading to suboptimal predictions for the majority of the data.\n",
    "*   **Deep Learning Implementation**: When using MSE for regression, the output layer of the neural network should have a **linear activation function**.\n",
    "\n",
    "##### 5.2. Mean Absolute Error (MAE)\n",
    "*   **Also Known As**: L1 Loss.\n",
    "*   **Application**: Another common loss function for **regression problems**.\n",
    "*   **Formula (for a single data point)**: `|y_true - y_pred|`.\n",
    "*   **Comparison to MSE**: Replaces the squaring operation with an absolute value operation.\n",
    "*   **Advantages**:\n",
    "    *   **Intuitive and easy to understand**.\n",
    "    *   **Same Unit as Target**: The unit of the error is the same as the target variable (e.g., if the output is 'LPA', the error is in 'LPA'), making it more directly interpretable.\n",
    "    *   **Robust to Outliers**: MAE penalises errors linearly, not quadratically. This means outliers do not cause disproportionately large penalties, resulting in less drastic adjustments to the model's parameters and a more robust fit to the general data trend.\n",
    "*   **Disadvantages**:\n",
    "    *   **Not Differentiable at Zero**: The graph of MAE (loss vs. weights) has a sharp point at zero, making it non-differentiable at that specific point. This requires the use of **subgradients** in gradient descent, which can slightly increase computational complexity.\n",
    "*   **Recommendation**: Use MAE when your dataset contains **outliers**.\n",
    "\n",
    "##### 5.3. Huber Loss\n",
    "*   **Application**: Used in **regression problems** when there is a significant mixture of normal points and outliers.\n",
    "*   **Idea**: A hybrid loss function that combines the benefits of MSE and MAE.\n",
    "    *   Behaves like **MSE for small errors** (points that are not outliers).\n",
    "    *   Behaves like **MAE for large errors** (points that are outliers).\n",
    "*   Formula\n",
    "   <img src='https://pbs.twimg.com/media/FNXUFtaWQAQSPyy.jpg'>\n",
    "*   **Parameter (Delta, Î´)**: A threshold parameter (hyperparameter) determines what constitutes a \"small\" error (MSE-like behaviour) versus a \"large\" error (MAE-like behaviour). Its value can be tuned.\n",
    "*   **Benefit**: Provides a balance, robustly handling outliers without completely ignoring smaller errors.\n",
    "\n",
    "##### 5.4. Binary Cross Entropy (BCE)\n",
    "*   **Also Known As**: Log Loss.\n",
    "*   **Application**: Used for **binary classification problems**, where there are exactly two classes (e.g., Yes/No, 0/1, placement/no placement).\n",
    "*   **Formula (for a single data point)**: `-y_true * log(y_pred) - (1 - y_true) * log(1 - y_pred)`.\n",
    "    *   `y_true`: The actual target value (0 or 1).\n",
    "    *   `y_pred`: The predicted probability of the positive class (between 0 and 1).\n",
    "*   **Deep Learning Implementation**: The output layer of the neural network must use a **sigmoid activation function**.\n",
    "*   **Advantages**:\n",
    "    *   **Differentiable**: Enables easy application of gradient descent.\n",
    "*   **Disadvantages**:\n",
    "    *   Can have **multiple local minima**.\n",
    "    *   **Less intuitive** to understand compared to MSE/MAE.\n",
    "\n",
    "##### 5.5. Categorical Cross Entropy (CCE)\n",
    "*   **Application**: Used for **multi-class classification problems**, where there are three or more classes (e.g., 'Yes', 'No', 'Maybe').\n",
    "*   **Formula (for a single data point)**: `-sum(y_true_j * log(y_pred_j))` for `j=1 to k` classes.\n",
    "    *   `y_true_j`: The true label (1 for the correct class, 0 for others in one-hot encoding).\n",
    "    *   `y_pred_j`: The predicted probability for class `j`.\n",
    "*   **Deep Learning Architecture**:\n",
    "    *   The output layer should have **as many neurons as there are classes**.\n",
    "    *   The activation function for the output layer neurons must be **Softmax**. Softmax outputs probabilities for each class that sum up to 1.\n",
    "*   **Data Preparation**: The true target labels must be **one-hot encoded**. For example, if there are three classes ('Yes', 'No', 'Maybe'), 'Yes' might be ``, 'No' as ``, and 'Maybe' as ``.\n",
    "\n",
    "##### 5.6. Sparse Categorical Cross Entropy (SCCE)\n",
    "*   **Application**: Also used for **multi-class classification problems**.\n",
    "*   **Key Difference from CCE**: Instead of requiring one-hot encoded true labels, SCCE expects the true labels to be **integer encoded**. For example, 'Yes' could be 1, 'No' could be 2, and 'Maybe' could be 3.\n",
    "*   **Deep Learning Architecture**: The architecture (number of output neurons and Softmax activation) remains the same as CCE.\n",
    "*   **Advantages**:\n",
    "    *   **Faster**: Especially beneficial when dealing with a very large number of classes, as it avoids the computational overhead of one-hot encoding the labels for each batch.\n",
    "    *   **Memory Efficient**: Reduces memory usage by storing integer labels instead of sparse one-hot vectors.\n",
    "\n",
    "#### 6. Summary and Recommendations\n",
    "*   **Regression Problems**:\n",
    "    *   Use **MSE** if there are no significant outliers in your data.\n",
    "    *   Use **MAE** if your data contains outliers, as it is more robust.\n",
    "    *   Use **Huber Loss** if your data has a mixture of normal points and outliers, offering a balance.\n",
    "*   **Classification Problems**:\n",
    "    *   Use **Binary Cross Entropy** for binary classification (two classes).\n",
    "    *   Use **Categorical Cross Entropy** for multi-class classification when true labels are one-hot encoded.\n",
    "    *   Use **Sparse Categorical Cross Entropy** for multi-class classification when true labels are integer encoded, especially with many classes for speed and memory efficiency.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac81107-b1f4-4950-abae-13a016ad829c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
