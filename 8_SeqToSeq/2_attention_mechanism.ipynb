{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b996697c-a347-41c6-ae77-c6dbaee50791",
   "metadata": {},
   "source": [
    "## I. The Problem with Vanilla Encoder-Decoder Architecture\n",
    "\n",
    "The Encoder-Decoder architecture, typically built using LSTMs (Long Short-Term Memory) or GRUs, aims to solve asynchronous Seq2Seq problems, such as Machine Translation, where input and output lengths do not correspond.\n",
    "\n",
    "### 1. The Bottleneck: Static Context Vector\n",
    "The core function of the Encoder is to process the input sentence step-by-step and **compress the entire input information** into a single fixed-length vector, known as the **Context Vector**. This vector serves as the summary or representation of the input sentence, which is then passed to the Decoder.\n",
    "\n",
    "### 2. The Flaws\n",
    "This reliance on a single Context Vector creates two significant problems, especially with longer inputs:\n",
    "\n",
    "*   **Memory Loss (Encoder Overload):** If the input sentence is long (e.g., greater than 25 words), the Context Vector is put under too much responsibility to summarize the entire sequence. This can lead to the model \"forgetting\" information from the start of the sequence, resulting in degraded translation quality.\n",
    "*   **Static Representation (Decoder Inefficiency):** At every time step of the Decoder, the entire input sentence's summary (the Context Vector) is provided. However, to translate a single word (e.g., \"light\"), the Decoder only needs to pay attention to a **specific word or set of words** from the input sequence (e.g., \"lights\"). The static, unchanging representation makes decoding difficult.\n",
    "\n",
    "## II. The Solution: The Attention Mechanism\n",
    "\n",
    "The goal of the Attention Mechanism is to provide the Decoder with **dynamic** information, allowing it to \"focus\" on the most relevant parts of the input sequence for generating the current output word. This mimics how human beings translate, by creating an \"attention region\" around the relevant text while translating *on the go*.\n",
    "\n",
    "### A. Architectural Change: The Three Inputs\n",
    "The introduction of Attention changes the input requirements for the Decoder at a given time step $i$.\n",
    "\n",
    "| Architecture | Inputs Required at Decoder Time Step $i$ | Components |\n",
    "| :--- | :--- | :--- |\n",
    "| **Vanilla Encoder-Decoder** | Two inputs: $Y_{i-1}$ and $S_{i-1}$ | $Y_{i-1}$ (Previous Output/Teacher Forced Label) and $S_{i-1}$ (Previous Decoder Hidden State). |\n",
    "| **Attention-Based E-D** | Three inputs: $Y_{i-1}$, $S_{i-1}$, and $C_i$ | **$C_i$ (The Attention Input/Context Vector for step $i$)** is added. |\n",
    "\n",
    "### B. Defining the Attention Input ($C_i$)\n",
    "The Attention Input, $C_i$, is a **vector**. Its purpose is to inform the Decoder which Encoder hidden states ($H_j$) are most useful at the current time step $i$.\n",
    "\n",
    "*   The dimension of the Attention Input $C_i$ **must be exactly the same** as the dimension of the Encoder Hidden States ($H_j$).\n",
    "*   To consolidate the potentially multiple useful hidden states into a single vector ($C_i$), a **Weighted Sum** is performed.\n",
    "\n",
    "## III. The Mathematics of Attention\n",
    "\n",
    "### 1. Calculating $C_i$ (The Weighted Sum)\n",
    "The Attention Input ($C_i$) is calculated by assigning a **weight ($\\alpha$)** to every Encoder Hidden State ($H_j$) and summing the weighted values.\n",
    "\n",
    "*   The equation for the Attention Input $C_i$ at Decoder time step $i$ is:\n",
    "    $$C_i = \\sum_{j} \\alpha_{i,j} H_j$$\n",
    "*   **Notation:**\n",
    "    *   $H_j$: The hidden state vector of the Encoder at time step $j$.\n",
    "    *   $\\alpha_{i,j}$: The weight (scalar) assigned to $H_j$.\n",
    "    *   $C_i$: The resulting Attention Input vector (which maintains the dimension of $H_j$).\n",
    "\n",
    "If a specific weight, say $\\alpha_4$, dominates (e.g., $0.6$), then the corresponding Encoder Hidden State ($H_4$) will have the greatest influence (say) in creating $C_1$.\n",
    "\n",
    "### 2. Calculating the Weights ($\\alpha_{i,j}$)\n",
    "\n",
    "The core challenge is determining the **Alignment Scores ($\\alpha_{i,j}$)**, which quantify the role of Encoder time step $j$ in producing the output at Decoder time step $i$.\n",
    "\n",
    "*   **Dependencies:** Any given score $\\alpha_{i,j}$ (e.g., $\\alpha_{2,1}$) must be a function of two pieces of information:\n",
    "    1.  $H_j$ (The Encoder hidden state itself).\n",
    "    2.  $S_{i-1}$ (The previous hidden state of the Decoder).\n",
    "    *   The use of $S_{i-1}$ is logical because the model needs to know the similarity score ($ \\alpha_{i,j} $) based on the translation that has *already occurred* up to step $i-1$.\n",
    "\n",
    "*   **The Function ($F$):** The relationship between these inputs ($H_j$ and $S_{i-1}$) and the output weight ($\\alpha_{i,j}$) is defined by a mathematical function $F$. Researchers chose to use a **small Artificial Neural Network (ANN)** (Feed-Forward Neural Network) to approximate this function.\n",
    "\n",
    "*   **ANN Training:** This small neural network is trained using $H_j$ and $S_{i-1}$ as inputs, and its output is $\\alpha_{i,j}$. Since the weights and biases of this ANN are trainable parameters, the entire system—the Encoder, the Decoder, and the Attention ANN—are trained simultaneously via **Backpropagation Through Time (BPTT)**.\n",
    "\n",
    "## IV. Empirical Validation and Advanced Details\n",
    "\n",
    "### 1. Improved Performance (BLEU Score)\n",
    "The effectiveness of the Attention Mechanism is empirically proven by plotting the sentence length against the **BLEU score** (a metric used to measure translation quality).\n",
    "\n",
    "*   In models without Attention, the BLEU score drops sharply once the sentence length exceeds approximately **30 words**.\n",
    "*   In the Attention-Based model, the BLEU score remained **stable** even beyond 30 words, demonstrating that the mechanism successfully mitigated the memory loss problem.\n",
    "\n",
    "### 2. Visualization of Attention\n",
    "Since the system calculates 16 different alpha scores (if the input and output are 4 words each, $4 \\times 4$), researchers can plot these weights to create an **alignment grid**. This grid visually confirms that the model is correctly mapping specific output words (e.g., French) to the corresponding input words (e.g., English), proving that the dynamic focus is working.\n",
    "\n",
    "### 3. Bidirectional LSTMs\n",
    "In the original research paper on the Attention Mechanism, the researchers used **Bidirectional LSTMs (Bi-LSTMs)** in the **Encoder** only. Bi-LSTMs allow the network to access both the past and future context of an input word, further improving the quality of the Context Vector ($H_j$) passed to the Decoder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
