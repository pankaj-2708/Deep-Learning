{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9610fc3",
   "metadata": {},
   "source": [
    "# Bahdanau Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ed5bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this implementation might not work due to compatibility issues but concept is correct look at this blog for working code https://blog.paperspace.com/seq-to-seq-attention-mechanism-keras/\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c1c156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [\n",
    "    \"hello there\",\n",
    "    \"how are you\",\n",
    "    \"i am fine\",\n",
    "]\n",
    "\n",
    "target_texts = [\n",
    "    \"hola allí\",\n",
    "    \"cómo estás\",\n",
    "    \"estoy bien\",\n",
    "]\n",
    "\n",
    "# for decoder we need to add starting symbol in inputs and ouput symbol in outputs\n",
    "target_texts_in = [\"<SOS> \" + t for t in target_texts]\n",
    "target_texts_out = [t + \" <EOS>\" for t in target_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e85955d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# encoder tokenizer\n",
    "encoder_tokenizer=Tokenizer(filters='')\n",
    "encoder_tokenizer.fit_on_texts(input_texts)\n",
    "encoder_vocab=len(encoder_tokenizer.word_index)+1\n",
    "\n",
    "# decoder tokenizer\n",
    "decoder_tokenizer=Tokenizer(filters='')\n",
    "decoder_tokenizer.fit_on_texts(target_texts_in+target_texts_out)\n",
    "decoder_vocab=len(decoder_tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9166e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_sequences=encoder_tokenizer.texts_to_sequences(input_texts)\n",
    "decoder_input_sequences=decoder_tokenizer.texts_to_sequences(target_texts_in)\n",
    "decoder_output_sequences=decoder_tokenizer.texts_to_sequences(target_texts_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aedcd086",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_len = max(len(s) for s in encoder_input_sequences)\n",
    "max_decoder_len = max(len(s) for s in decoder_input_sequences)\n",
    "\n",
    "encoder_input_sequences = pad_sequences(encoder_input_sequences,maxlen=max_encoder_len,padding='post')\n",
    "decoder_input_sequences = pad_sequences(decoder_input_sequences,maxlen=max_decoder_len,padding='post')\n",
    "decoder_output_sequences = pad_sequences(decoder_output_sequences,maxlen=max_decoder_len,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cef6893",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(target_texts)\n",
    "embedding_dim = 5\n",
    "units = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f4baa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bahdanau Attention\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,units):\n",
    "        super().__init__()\n",
    "        self.W1=tf.keras.layers.Dense(units)\n",
    "        self.W2=tf.keras.layers.Dense(units)\n",
    "        self.V=tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self,hidden_state,encoder_ouputs):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden_state, 1)\n",
    "        \n",
    "        score=self.V(tf.nn.tanh(self.W1(encoder_ouputs)+self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        attention_weights=tf.nn.softmax(score, axis=1)\n",
    "        context_vector=attention_weights*encoder_ouputs\n",
    "        context_vector= tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector,attention_weights\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9466db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, units):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.LSTM(units, return_state=True)\n",
    "        self.attention = BahdanauAttention(units)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, dec_input, hidden_state, cell_state, encoder_outputs):\n",
    "        embedded=self.embedding(dec_input)\n",
    "        \n",
    "        context,attn_weights=self.attention(hidden_state,encoder_outputs)\n",
    "        context=tf.expand_dims(context,axis=1)\n",
    "        \n",
    "        lstm_input=tf.concat([context,embedded],axis=-1)\n",
    "        \n",
    "        output,h,c=self.lstm(lstm_input,initial_state=[hidden_state,cell_state])\n",
    "        \n",
    "        output=self.dense(output)\n",
    "        return output,h,c,attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a73d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_input=tf.keras.layers.Input(shape=(None,))\n",
    "enc_emb=tf.keras.layers.Embedding(encoder_vocab,embedding_dim)(encoder_input)\n",
    "\n",
    "encoder_lstm=tf.keras.layers.LSTM(units,return_state=True,return_sequences=True)\n",
    "encoder_ouputs,state_h,state_c=encoder_lstm(enc_emb)\n",
    "encoder_states=[state_h,state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72b5584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "decoder_input=tf.keras.layers.Input(shape=(None,))\n",
    "\n",
    "decoder_layer=DecoderWithAttention(decoder_vocab,embedding_dim,units)\n",
    "\n",
    "all_outputs=[]\n",
    "hidden,cell=state_h,state_c\n",
    "\n",
    "for t in range(max_decoder_len):\n",
    "    dec_in_t=tf.expand_dims(decoder_input[:,t],1)\n",
    "    output, hidden, cell, _=decoder_layer(dec_in_t,hidden,cell,encoder_ouputs)\n",
    "    \n",
    "    all_outputs.append(output)\n",
    "    \n",
    "decoder_outputs = tf.concat(all_outputs, axis=1)\n",
    "training_model = tf.keras.Model([encoder_input, decoder_input], decoder_outputs)\n",
    "training_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c40836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infernece\n",
    "encoder_model = tf.keras.Model(encoder_input, [encoder_ouputs,state_h,state_c])\n",
    "\n",
    "# decoder \n",
    "dec_token_input = tf.keras.Input(shape=(1,), name=\"dec_token_input\")\n",
    "dec_h_input = tf.keras.Input(shape=(units,), name=\"dec_h_input\")\n",
    "dec_c_input = tf.keras.Input(shape=(units,), name=\"dec_c_input\")\n",
    "enc_out_input = tf.keras.Input(shape=(None, units), name=\"enc_out_input\")\n",
    "\n",
    "dec_output, dec_h, dec_c, attn = decoder_layer(\n",
    "    dec_token_input,\n",
    "    dec_h_input,\n",
    "    dec_c_input,\n",
    "    enc_out_input\n",
    ")\n",
    "\n",
    "decoder_model = tf.keras.Model(\n",
    "    [dec_token_input, dec_h_input, dec_c_input, enc_out_input],\n",
    "    [dec_output, dec_h, dec_c, attn]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4b4cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq,max_len=20):\n",
    "    # Encode source sentence\n",
    "    enc_out, h, c = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Initialize with <start>\n",
    "    dec_token = decoder_tokenizer.word_index['<sos>']\n",
    "    result = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        # Run one decoder step\n",
    "        output, h, c, attn = decoder_model.predict(\n",
    "            [dec_token, h, c, enc_out]\n",
    "        )\n",
    "\n",
    "        # Pick highest probability token\n",
    "        token_id = np.argmax(output[0, 0, :])\n",
    "        result.append(token_id)\n",
    "\n",
    "        if token_id == decoder_tokenizer.word_index['<eos>']:\n",
    "            break\n",
    "\n",
    "        # Feed the predicted token next\n",
    "        dec_token = np.array([[token_id]])\n",
    "\n",
    "    return result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
