{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28178952",
   "metadata": {},
   "source": [
    "## I. Context: The Role of Cross-Attention in the Decoder\n",
    "\n",
    "The Transformer architecture is split into the Encoder and the Decoder. The lecture notes confirm that the study of the **Encoder** (including Embeddings, Positional Encoding, Self-Attention, Multi-Head Attention, and Normalization) is complete. The study of the **Decoder** is now progressing, following the analysis of Masked Self-Attention.\n",
    "\n",
    "The Decoder architecture contains three main blocks, and the second block is a Multi-Head Attention variant called **Cross-Attention**. Cross-Attention is a \"very crucial aspect of the Transformer architecture\" required to properly understand the Decoder.\n",
    "\n",
    "## II. Definition and Necessity of Cross-Attention\n",
    "\n",
    "### A. The Definition\n",
    "**Cross-Attention** is a mechanism used in Transformer architectures, particularly for **Sequence-to-Sequence (Seq2Seq)** tasks such as translation or summarization.\n",
    "\n",
    "Its primary function is to **allow a model to focus on different parts of the input sequence when generating an output sequence**.\n",
    "\n",
    "### B. The Context of Sequence Generation\n",
    "In a Machine Translation example (English to Hindi), the Decoder's task is to generate the Hindi output sentence step-by-step.\n",
    "\n",
    "The decision of what the next word should be depends on two critical pieces of information:\n",
    "\n",
    "1.  **What has been generated till now by the Decoder** (The output sequence).\n",
    "2.  **What is the central idea or summary received from the Encoder** (The input sequence).\n",
    "\n",
    "### C. The Two Attention Problems\n",
    "Cross-Attention is required to solve the second problem (relating the input and output sequences):\n",
    "\n",
    "1.  **Self-Attention's Role (Solving Problem 1):** The relationship between the words already generated in the **output sequence** (e.g., how \"खाना\" relates to \"मुझे\" and \"आइसक्रीम\") is solved by **Self-Attention** (specifically, Masked Self-Attention).\n",
    "2.  **Cross-Attention's Role (Solving Problem 2):** The relationship between the **output sequence** and the **input sequence** (e.g., how the Hindi word \"आइसक्रीम\" relates to the English word \"ice cream,\" or \"पसंद\" relates to \"like\") must be calculated. This mechanism, which draws similarity between items in **two different sequences**, is called **Cross-Attention**.\n",
    "\n",
    "## III. Architectural and Conceptual Differences (Self-Attention vs. Cross-Attention)\n",
    "\n",
    "Conceptually, Cross-Attention is very similar to Self-Attention, but there are structural differences in the input, internal processing, and output.\n",
    "<img src=\"./images/3.png\">\n",
    "\n",
    "### A. Difference in Input\n",
    "\n",
    "| Feature | Self-Attention | Cross-Attention |\n",
    "| :--- | :--- | :--- |\n",
    "| **Input Sequences** | Takes a **single sequence** (e.g., only the English sentence \"We are friends\"). | Takes **two sequences**: the input sequence (Encoder output) and the output sequence (Decoder input). |\n",
    "\n",
    "### B. Difference in Processing (Q, K, V Generation)\n",
    "\n",
    "The major difference lies in how the Query (Q), Key (K), and Value (V) vectors are generated:\n",
    "\n",
    "1.  **Source of Query (Q):** The Query vectors are derived **only from the Output Sequence** (the Decoder's current sequence, e.g., the Hindi words).\n",
    "    *   For a Hindi word, the embedding is dotted only with the $W_Q$ matrix to produce a Query vector.\n",
    "2.  **Source of Key (K) and Value (V):** The Key and Value vectors are derived **only from the Input Sequence** (the Encoder's final output, e.g., the English words).\n",
    "    *   For an English word, the embedding is dotted with both the $W_K$ and $W_V$ matrices to produce a Key vector and a Value vector.\n",
    "\n",
    "The calculation within the Cross-Attention block uses three matrices, $W_Q, W_K,$ and $W_V$. In the overall Transformer diagram, the Query vectors come from the layer below within the Decoder, while the Key and Value vectors come from the Encoder.\n",
    "\n",
    "### C. Similarity Calculation and Output\n",
    "\n",
    "The subsequent steps mirror Self-Attention:\n",
    "\n",
    "1.  **Similarity Scores:** The similarity scores are calculated by taking the dot product of the **Output Sequence's Query vectors** (Q) with the **Input Sequence's Key vectors** (K).\n",
    "2.  **Attention Weights:** These scores are normalized (via Softmax/Scaling) to create attention weights, resulting in a matrix that shows how closely similar every input word is to every output word (e.g., \"dosht\" is highly similar to \"friends\").\n",
    "3.  **Contextual Output:** These weights are used to calculate a **weighted sum** of the **Value vectors ($V$)**.\n",
    "4.  **Final Output:** The output consists of **Contextual Embeddings**. The number of output vectors is always equal to the number of words in the **Output Sequence** (the Hindi sentence).\n",
    "\n",
    "<img src=\"./images/4.png\">\n",
    "\n",
    "## IV. Output Difference and Conceptual Parallel\n",
    "\n",
    "### A. Output Comparison\n",
    "\n",
    "| Mechanism | Output Calculation | Conceptual Goal |\n",
    "| :--- | :--- | :--- |\n",
    "| **Self-Attention** | Output Contextual Embedding of **V** (CE V) is the weighted sum of **V's, R's, and F's** embeddings. | Generates context based on **internal** word relationships within one sequence. |\n",
    "| **Cross-Attention** | Output Contextual Embedding of **हम** (CE हम) is the weighted sum of **V's, R's, and F's** embeddings. | Generates context based on the **external** relationship between the output sequence and the input sequence. |\n",
    "\n",
    "### B. Mimicking Traditional Attention\n",
    "Cross-Attention is conceptually very similar to older **RNN-based attention mechanisms** like **Bahdanau Attention and Luong Attention**.\n",
    "\n",
    "*   In those models, a **Context Vector ($C_i$)** was calculated at every time step by taking a weighted sum of all Encoder Hidden States ($H_j$).\n",
    "*   Cross-Attention mimics this by using the Decoder's Query vectors and the Encoder's Key and Value vectors to attend over the input sequence. This is why Cross-Attention is sometimes referred to as **Encoder-Decoder Attention**.\n",
    "\n",
    "## V. Applications of Cross-Attention\n",
    "\n",
    "Cross-Attention is widely used in systems that involve mapping one sequence to another:\n",
    "\n",
    "1.  **Machine Translation and Summarization:** The primary use case.\n",
    "2.  **Question Answering:** Where the query is one sequence and the input text is the second.\n",
    "3.  **Multimodal Data Processing:** Used when the input and output are different types of sequences (modalities):\n",
    "    *   **Image Captioning:** Image (Input Modality) to Text (Output Modality).\n",
    "    *   **Text to Image Generation Systems:** Text (Input Modality) to Image (Output Modality).\n",
    "    *   **Text to Speech Systems:** Text (Input Modality) to Speech/Audio (Output Modality).\n",
    "\n",
    "Cross-Attention is considered a very important concept for understanding LLMs and Generative AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19dcd9c",
   "metadata": {},
   "source": [
    "\n",
    "# How Encoder Output (T × 512) Is Used in the Transformer Decoder\n",
    "\n",
    "## 1. Encoder Output\n",
    "The encoder produces a tensor of shape:\n",
    "\n",
    "```\n",
    "encoder_output: (batch, T_enc, d_model)\n",
    "```\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "(batch, 20, 512)\n",
    "```\n",
    "\n",
    "- `T_enc` = input sequence length  \n",
    "- `d_model` = 512 (hidden size)\n",
    "\n",
    "This tensor is passed directly to every decoder layer.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Decoder Structure\n",
    "A decoder layer contains:\n",
    "\n",
    "1. Masked Self-Attention  \n",
    "2. Cross-Attention (uses encoder output)  \n",
    "3. Feed-Forward Network (FFN)\n",
    "\n",
    "Only cross-attention uses the encoder output.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. How Cross-Attention Uses (T × 512)\n",
    "\n",
    "### Query (Q)\n",
    "From decoder hidden states:\n",
    "\n",
    "```\n",
    "query: (batch, T_dec, d_model)\n",
    "```\n",
    "\n",
    "### Key (K) and Value (V)\n",
    "Computed from encoder output using linear layers:\n",
    "\n",
    "```\n",
    "key   = Dense(d_model)(encoder_output)   → (batch, T_enc, 512)\n",
    "value = Dense(d_model)(encoder_output)   → (batch, T_enc, 512)\n",
    "```\n",
    "\n",
    "So K and V include one vector per input timestep.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Attention Computation\n",
    "\n",
    "### Step 1: Attention Scores\n",
    "```\n",
    "scores = softmax( Q ⋅ K^T )\n",
    "```\n",
    "\n",
    "Shapes:\n",
    "\n",
    "| Tensor | Shape |\n",
    "|--------|--------|\n",
    "| Q | (batch, T_dec, 512) |\n",
    "| K | (batch, T_enc, 512) → transposed |\n",
    "| QKᵀ | (batch, T_dec, T_enc) |\n",
    "\n",
    "Each decoder timestep attends over all encoder positions.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Weighted Sum with Values\n",
    "```\n",
    "attn_output = scores ⋅ V\n",
    "```\n",
    "\n",
    "Shapes:\n",
    "\n",
    "| Tensor | Shape |\n",
    "|--------|--------|\n",
    "| scores | (batch, T_dec, T_enc) |\n",
    "| V | (batch, T_enc, 512) |\n",
    "| output | (batch, T_dec, 512) |\n",
    "\n",
    "This becomes the result of the cross-attention block.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Summary\n",
    "- Encoder output `(T_enc × 512)` goes to the decoder unchanged.  \n",
    "- Keys and values are produced from this encoder output.  \n",
    "- Every decoder query vector attends to all encoder vectors.  \n",
    "- Result is always `(T_dec × 512)` at each decoder layer.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
