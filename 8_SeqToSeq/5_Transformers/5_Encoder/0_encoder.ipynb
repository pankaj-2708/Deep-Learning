{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3582228d",
   "metadata": {},
   "source": [
    "## I. Context and Prerequisites\n",
    "\n",
    "Before delving into the Encoder architecture, it is essential to understand the core components developed to overcome the limitations of prior models (like RNNs/LSTMs):\n",
    "\n",
    "1.  **Self-Attention:** The foundational mechanism for generating **dynamic, contextual embeddings** by calculating similarity scores within a single sequence.\n",
    "2.  **Multi-Head Attention (MHA):** An enhancement of Self-Attention that allows the model to capture **multiple perspectives** of the input data simultaneously, leading to richer embeddings.\n",
    "3.  **Positional Encoding (PE):** A technique using trigonometric functions to **encode word order information** into the embeddings, compensating for Self-Attentionâ€™s lack of sequence awareness.\n",
    "4.  **Layer Normalization (LN):** A normalization technique applied across the feature dimension (horizontally) to **stabilize and accelerate training**, especially when dealing with variable-length sequential data that requires padding.\n",
    "\n",
    "## II. The Transformer Encoder Architecture\n",
    "\n",
    "The Transformer architecture, represented by a famous, complex diagram, is fundamentally broken down into two main parts: the Encoder and the Decoder.\n",
    "<img src=\"./images/en1.png\">\n",
    "\n",
    "### 1. Stacking Encoder Blocks\n",
    "The Encoder part is constructed by **stacking multiple identical Encoder blocks**.\n",
    "\n",
    "*   The original \"Attention Is All You Need\" paper used **six Encoder blocks** and **six Decoder blocks**.\n",
    "*   Since all six Encoder blocks are architecturally identical, understanding the operations inside **one single block** is sufficient to understand the entire Encoder.\n",
    "*   The output of the first Encoder block serves as the input for the second, and this chaining continues until the output of the sixth block is passed to the Decoder.\n",
    "\n",
    "### 2. Components of a Single Encoder Block\n",
    "<img src=\"./images/en2.png\">\n",
    "\n",
    "Each Encoder block is composed of two primary internal sub-blocks:\n",
    "\n",
    "1.  A **Self-Attention block** (implemented as Multi-Head Attention).\n",
    "2.  A **Feed-Forward Neural Network (FFNN) block**.\n",
    "\n",
    "The full block structure also includes two instances of the **Add & Norm** operation, which incorporates **Residual Connections** (or skip connections) and **Layer Normalization**.\n",
    "\n",
    "## III. The Journey of an Input Sentence (The \"How\" Part)\n",
    "\n",
    "The transformation of an example input sentence, \"How are you,\" is tracked step-by-step through the first Encoder block.\n",
    "\n",
    "### A. Input Block Processing (Embedding + PE)\n",
    "<img src=\"./images/inp1.png\">\n",
    "<img src=\"./images/inp3.png\">\n",
    "\n",
    "Before entering the first Encoder block, the input sentence undergoes three essential operations:\n",
    "\n",
    "1.  **Tokenization:** The sentence is broken down into tokens (How, are, you).\n",
    "2.  **Text Vectorization (Embedding):** Each token is converted into a numerical vector via an **embedding layer**. In the original paper, these are **512-dimensional vectors**.\n",
    "3.  **Positional Encoding (PE):** A corresponding 512-dimensional **positional vector** is generated for each word's position.\n",
    "4.  **Addition:** The positional vector is **added** to its corresponding embedding vector. This results in three new 512-dimensional input vectors ($x_1, x_2, x_3$) that now contain both semantic and positional information.\n",
    "\n",
    "### B. Stage 1: Multi-Head Attention (MHA)\n",
    "\n",
    "<img src=\"./images/en3.png\">\n",
    "\n",
    "The $x_1, x_2, x_3$ vectors enter the MHA block.\n",
    "\n",
    "*   **Purpose:** The MHA processes these inputs to create **contextually aware embeddings** by relating each word to all others in the sequence.\n",
    "*   **Output:** The MHA produces three new 512-dimensional vectors ($z_1, z_2, z_3$).\n",
    "\n",
    "### C. Stage 2: Add & Norm (Post-MHA)\n",
    "This stage combines the original input with the MHA output and normalizes the result.\n",
    "\n",
    "<img src=\"./images/en3.png\">\n",
    "\n",
    "1.  **Residual Connection (\"Add\"):** The original input vectors ($x_1, x_2, x_3$) are **copied/bypassed** around the MHA block and added to the MHA output ($z_1, z_2, z_3$), yielding three new vectors ($z'_1, z'_2, z'_3$).\n",
    "2.  **Layer Normalization (\"Norm\"):** Layer Normalization is applied to the resulting vectors ($z'$) to stabilize the training process. This results in three normalized vectors ($z_{1\\text{norm}}, z_{2\\text{norm}}, z_{3\\text{norm}}$).\n",
    "\n",
    "### D. Stage 3: Feed-Forward Network (FFNN)\n",
    "<img src=\"./images/ff1.png\">\n",
    "\n",
    "The normalized vectors ($z_{\\text{norm}}$) are then fed into the FFNN block.\n",
    "\n",
    "1.  **Input:** The vectors are treated as a $3 \\times 512$ matrix entering the network.\n",
    "2.  **Architecture:** The FFNN is a **two-layer** neural network:\n",
    "    *   **Layer 1 (Expansion):** Contains **2048 neurons** and uses the **Rectified Linear Unit (ReLU)** activation function. This layer increases the dimensionality of the vectors from 512 to 2048.\n",
    "    *   **Layer 2 (Contraction):** Contains **512 neurons** and uses a **Linear activation function**. This layer reduces the dimensionality back down to 512.\n",
    "3.  **Purpose:** The main reason for using the FFNN is to introduce **non-linearities** (via ReLU) to the data, which is necessary because the core Self-Attention operation is largely linear.\n",
    "\n",
    "### E. Stage 4: Add & Norm (Post-FFNN)\n",
    "<img src=\"./images/ff1.png\">\n",
    "\n",
    "The FFNN output ($y_1, y_2, y_3$) undergoes the final normalization step.\n",
    "\n",
    "1.  **Residual Connection (\"Add\"):** The input to the FFNN ($z_{\\text{norm}}$) is bypassed and **added** to the FFNN output ($y$), resulting in $y'$ vectors.\n",
    "2.  **Layer Normalization (\"Norm\"):** Layer Normalization is applied to these vectors, producing the final output for the first Encoder block ($y_{1\\text{norm}}, y_{2\\text{norm}}, y_{3\\text{norm}}$).\n",
    "\n",
    "### F. Final Output\n",
    "The final normalized output vectors are then passed as the input to the **next Encoder block**. The dimension of the data remains consistent throughout the entire process, starting and ending as $3 \\times 512$.\n",
    "\n",
    "### G. Full encoder architecture in 2 diagrams\n",
    "\n",
    "<img src=\"./images/all1.png\">\n",
    "<img src=\"./images/all2.png\">\n",
    "\n",
    "## IV. Conceptual Rationale (The \"Why\" Questions)\n",
    "\n",
    "The lecture notes address the conceptual necessity of two complex features: Residual Connections and the Feed-Forward Network.\n",
    "\n",
    "### 1. Why Use Residual Connections? (Skip Connections)\n",
    "The exact reason for using Residual Connections in the Transformer is **not clearly provided** in the original research paper. However, empirical evidence shows they are **critical** for performance.\n",
    "\n",
    "**Speculated Reasons:**\n",
    "*   **Stable Training:** In deep networks (like the stack of six Encoder blocks), residual connections help mitigate the **vanishing gradient problem** by providing an alternate path for the gradient to flow, ensuring parameter updates continue throughout the network.\n",
    "*   **Feature Preservation:** They allow the network to send the **original, untransformed features** forward. This acts as a safeguard; if a transformation (MHA or FFNN) degrades the feature quality, the original, \"good\" features can still be utilized.\n",
    "\n",
    "### 2. Why Use the Feed-Forward Network?\n",
    "The FFNN, particularly its weight matrices ($W_1$ and $W_2$), constitutes **two-thirds of the Transformer's total parameters**.\n",
    "\n",
    "**Main Reason (Generic Answer):**\n",
    "*   **Introducing Non-Linearity:** Self-Attention involves mostly linear operations (dot products). The FFNN, especially through the **ReLU activation** in the first layer, introduces necessary **non-linearities** that allow the model to capture more complex patterns in the data.\n",
    "\n",
    "**Alternative Theory (Active Research):**\n",
    "*   Recent research suggests that the FFNN layers operate as **key-value memories**. These \"memories\" are believed to store textual patterns learned during the training process, indicating a more complex role than simply introducing non-linearity.\n",
    "\n",
    "### 3. Why Stack Multiple Encoder Blocks?\n",
    "*   **High Representation Power:** Human language is inherently complex. To achieve a satisfactory understanding of language, the model requires very high **representation power**.\n",
    "*   **Depth is Key:** In Deep Learning, stacking multiple layers (going \"deep\") increases the model's ability to extract hidden patterns and create a deeper representation of the data.\n",
    "*   **Empirical Best Result:** The creators chose six blocks because this number yielded the **best empirical results** in their experiments, although this number is configurable depending on the specific application.\n",
    "\n",
    "***\n",
    "*Note:* The parameters (weights and biases) of the Multi-Head Attention and the Feed-Forward Network are **unique** for each of the six Encoder blocks. Although the architecture is copied (identical blocks), the learned parameters themselves are different for every block in the stack."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
