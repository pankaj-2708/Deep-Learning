{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645ed875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input,Embedding,Add,Dense,Attention,LayerNormalization,Multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e06b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_angles(pos,embed_dim):\n",
    "    a=[i for i in range(embed_dim)]\n",
    "    return [np.cos(pos/10000**(2*i/embed_dim)) if i%2==1 else np.sin(pos/10000**(2*i/embed_dim)) for i in range(embed_dim)]\n",
    "\n",
    "def positional_encoding(x):\n",
    "    # shape of x will be batch_size,timestamp,embed_dim\n",
    "    shape=tf.shape(x)\n",
    "    pos=shape[1]\n",
    "    embed_dim=shape[2]\n",
    "    ans=[]\n",
    "    for i in range(shape[0]):\n",
    "        temp_ans=[]\n",
    "        for j in range(pos):\n",
    "            angles=generate_angles(j,embed_dim)\n",
    "            temp_ans.append(angles)\n",
    "        ans.append(temp_ans)\n",
    "    return Add()[x,ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f999d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length is the seq_len , depth is the embeding dim\n",
    "def positional_encoding(length, depth):\n",
    "    positions=np.arange(length)[:,np.newaxis]  # seq_len , 1\n",
    "    \n",
    "    depths=np.arange(depth)[np.newaxis , : ]  # 1, depth\n",
    "    \n",
    "    angle_rates=1/np.power(10000,(2 * depths // 2)) / depth\n",
    "    angles=positions*angle_rates    # pos , depth \n",
    "    angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "    angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "    \n",
    "    return tf.cast(angles,tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61f180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.layer):\n",
    "    def __init__(self,vocab_size,d_model):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.embedding=Embedding(vocab_size,d_model,mask_zero=True)\n",
    "        # here pos_encoding are generated with length (seq_length) = 2048 \n",
    "        self.pos_encoding=positional_encoding(length=2048,depth=d_model)\n",
    "        \n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "       return self.embedding.compute_mask(*args, **kwargs)\n",
    "   \n",
    "    def call(self,x):\n",
    "        # x is input its shape will be  batch_size,seq_len\n",
    "        length=tf.shape(x)[1]\n",
    "        x=self.embedding(x)\n",
    "        # now x will be of shape batch_size,seq_len,embed_dim\n",
    "        \n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        \n",
    "        x=x+self.positional_encoding[tf.newaxis,:length,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e8f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  i am writing a base attention then inherting it because it will reduce code part when we have to add cross attention in decoder \n",
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model ,**kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b7dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self,x):\n",
    "        \n",
    "        # If query, key, value are the same in MultiHeadAttention keras internally multiply them with weight matrix to convert them to respect vectors as per paper\n",
    "        attn_output=self.mha(query=x,value=x,key=x)\n",
    "        \n",
    "        x=self.add([x,attn_output])\n",
    "        x=self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cc4982",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__ (self,d_model,dff,dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model),\n",
    "        tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "    def call(self,x):\n",
    "        out=self.seq(x)\n",
    "        x=self.add([x,out])\n",
    "        return self.layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd44e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "        \n",
    "        self.ffn = FeedForward(d_model,dff)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.ffn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87b49e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "d_model = 512\n",
    "input_vocab_size=-1\n",
    "\n",
    "# Encoder\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads,\n",
    "               dff, vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.pos_embedding = PositionalEmbedding(\n",
    "        vocab_size=vocab_size, d_model=d_model)\n",
    "        \n",
    "        self.enc_layers = [\n",
    "        EncoderLayer(d_model=d_model,\n",
    "                     num_heads=num_heads,\n",
    "                     dff=dff,\n",
    "                     dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x):\n",
    "        # `x` is token-IDs shape: (batch, seq_len)\n",
    "        x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "        \n",
    "        # Add dropout.\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "\n",
    "        return x  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    \n",
    "    \n",
    "# keras also has a encoder layer -> https://keras.io/keras_hub/api/modeling_layers/transformer_encoder/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
