{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28174b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input,Embedding,Add,Dense,Attention,LayerNormalization,Multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5efa4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length is the seq_len , depth is the embeding dim\n",
    "def positional_encoding(length, depth):\n",
    "    positions=np.arange(length)[:,np.newaxis]  # seq_len , 1\n",
    "    \n",
    "    depths=np.arange(depth)[np.newaxis , : ]  # 1, depth\n",
    "    \n",
    "    angle_rates=1/np.power(10000,(2 * depths // 2)) / depth\n",
    "    angles=positions*angle_rates    # pos , depth \n",
    "    angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "    angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "    \n",
    "    return tf.cast(angles,tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d35edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.layer):\n",
    "    def __init__(self,vocab_size,d_model):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.embedding=Embedding(vocab_size,d_model,mask_zero=True)\n",
    "        # here pos_encoding are generated with length (seq_length) = 2048 \n",
    "        self.pos_encoding=positional_encoding(length=2048,depth=d_model)\n",
    "        \n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "       return self.embedding.compute_mask(*args, **kwargs)\n",
    "   \n",
    "    def call(self,x):\n",
    "        # x is input its shape will be  batch_size,seq_len\n",
    "        length=tf.shape(x)[1]\n",
    "        x=self.embedding(x)\n",
    "        # now x will be of shape batch_size,seq_len,embed_dim\n",
    "        \n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        \n",
    "        x=x+self.positional_encoding[tf.newaxis,:length,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ef4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e254ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(BaseAttention):\n",
    "  def call(self, x, context):\n",
    "    attn_output, attn_scores = self.mha(\n",
    "        query=x,\n",
    "        key=context,\n",
    "        value=context,\n",
    "        return_attention_scores=True)\n",
    "   \n",
    "    # Cache the attention scores for plotting later.\n",
    "    self.last_attn_scores = attn_scores\n",
    "\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74158b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x,\n",
    "        use_causal_mask = True)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e37ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),\n",
    "      tf.keras.layers.Dense(d_model),\n",
    "      tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.layer_norm(x) \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b521a",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43e39b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.self_attention = GlobalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.self_attention(x)\n",
    "    x = self.ffn(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248cc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads,\n",
    "               dff, vocab_size, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(\n",
    "        vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "    self.enc_layers = [\n",
    "        EncoderLayer(d_model=d_model,\n",
    "                     num_heads=num_heads,\n",
    "                     dff=dff,\n",
    "                     dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self, x):\n",
    "    # `x` is token-IDs shape: (batch, seq_len)\n",
    "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    \n",
    "    # Add dropout.\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x)\n",
    "\n",
    "    return x  # Shape `(batch_size, seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cdcd5e",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7b36fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,num_heads,d_model,dff,dropout_rate=0.1):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        \n",
    "        self.cross_attention=CrossAttention(num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "        \n",
    "        self.masked_attention=CausalSelfAttention(num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "        \n",
    "        self.ff=FeedForward(d_model, dff, dropout_rate)\n",
    "        \n",
    "    def call(self,x,encoder_x):\n",
    "        x=self.masked_attention(x)\n",
    "        x=self.cross_attention(x,encoder_x)\n",
    "        x=self.ff(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15b753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,num_layers,vocab_size,num_heads,d_model,dff,dropout_rate=0.1):\n",
    "        super(Decoder,self).__init__()\n",
    "        \n",
    "        self.positional_embedding=PositionalEmbedding(vocab_size,d_model)\n",
    "        \n",
    "        self.decoder_layers=[DecoderLayer(num_heads=num_heads,d_model=d_model,dff=dff,dropout_rate=dropout_rate) for i in range(num_layers)]\n",
    "        \n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self,x,encoder_x):\n",
    "        pe=self.positional_embedding(x)\n",
    "        \n",
    "        pe=self.dropout(pe)\n",
    "        \n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            pe=decoder_layer(x,encoder_x)\n",
    "            \n",
    "        self.output_layer(pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6a4749",
   "metadata": {},
   "source": [
    "# Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5667a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,input_vocab_size,target_vocab_size,num_layers,num_heads,d_model,dff,dropout_rate=0.1):\n",
    "        super.__init__()\n",
    "        \n",
    "        self.encoder=Encoder(num_layers=num_layers,num_heads=num_heads,d_model=d_model,vocab_size=input_vocab_size,dff=dff,dropout_rate=dropout_rate)\n",
    "        \n",
    "        self.decoder=Decoder(num_layers=num_layers,vocab_size=target_vocab_size,num_heads=num_heads,d_model=d_model,dff=dff,dropout_rate=dropout_rate)\n",
    "        \n",
    "        self.output_layer=Dense(target_vocab_size,activation=\"softmax\")\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        context,x=inputs\n",
    "        \n",
    "        context=self.encoder(context)\n",
    "        x=self.decoder(x,context)\n",
    "        \n",
    "        out=self.output_layer(x)\n",
    "        try:\n",
    "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "      # b/250038731\n",
    "            del out._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18544357",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07135211",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=-1,\n",
    "    target_vocab_size=-1,\n",
    "    dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8812315",
   "metadata": {},
   "source": [
    "# Learning Rate\n",
    "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the original Transformer [paper](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "$$\\Large{lrate = d_{model}^{-0.5} * \\min(step{\\_}num^{-0.5}, step{\\_}num \\cdot warmup{\\_}steps^{-1.5})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c5daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self,d_model,warmup_steps):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model=tf.cast(d_model,tf.float32)\n",
    "        self.warmup_steps=warmup_steps\n",
    "        \n",
    "    def __call__(self,step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        lrate=tf.math.rsqrt(self.d_model)*tf.math.minimum(tf.math.sqrt(step),step*(self.warmup_steps ** -1.5))\n",
    "        return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907a28bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d9ddab",
   "metadata": {},
   "source": [
    "# LOSS and ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f274ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label,pred):\n",
    "    mask=label!=pred\n",
    "    loss_object=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction=\"None\")\n",
    "    loss=loss_object(label,pred)\n",
    "    \n",
    "    mask=tf.cast(mask,dtype=loss.dtype)\n",
    "    loss*=mask\n",
    "    \n",
    "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "\n",
    "def masked_accuracy(label,pred):\n",
    "    mask=label!=pred\n",
    "    pred=tf.argmax(pred,axis=2)\n",
    "    label=tf.cast(label,pred.dtype)\n",
    "    \n",
    "    match=label==mask\n",
    "    match=match & mask\n",
    "    \n",
    "    match = tf.cast(match, dtype=tf.float32)    \n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    \n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc865302",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db300b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(optimizer=optimizer,loss=masked_loss,metrics=[masked_accuracy])\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ef605d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.fit(train_batches,epochs=EPOCHS,validation_data=val_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7040430",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5925653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "  def __init__(self, tokenizers, transformer):\n",
    "    self.tokenizers = tokenizers\n",
    "    self.transformer = transformer\n",
    "\n",
    "  def __call__(self, sentence, max_length=MAX_TOKENS):\n",
    "    # The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.\n",
    "    assert isinstance(sentence, tf.Tensor)\n",
    "    if len(sentence.shape) == 0:\n",
    "      sentence = sentence[tf.newaxis]\n",
    "\n",
    "    sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\n",
    "\n",
    "    encoder_input = sentence\n",
    "\n",
    "    # As the output language is English, initialize the output with the\n",
    "    # English `[START]` token.\n",
    "    start_end = self.tokenizers.en.tokenize([''])[0]\n",
    "    start = start_end[0][tf.newaxis]\n",
    "    end = start_end[1][tf.newaxis]\n",
    "\n",
    "    # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
    "    # dynamic-loop can be traced by `tf.function`.\n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "    output_array = output_array.write(0, start)\n",
    "\n",
    "    for i in tf.range(max_length):\n",
    "      output = tf.transpose(output_array.stack())\n",
    "      predictions = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "      # Select the last token from the `seq_len` dimension.\n",
    "      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "\n",
    "      predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "      # Concatenate the `predicted_id` to the output which is given to the\n",
    "      # decoder as its input.\n",
    "      output_array = output_array.write(i+1, predicted_id[0])\n",
    "\n",
    "      if predicted_id == end:\n",
    "        break\n",
    "\n",
    "    output = tf.transpose(output_array.stack())\n",
    "    # The output shape is `(1, tokens)`.\n",
    "    text = tokenizers.en.detokenize(output)[0]  # Shape: `()`.\n",
    "\n",
    "    tokens = tokenizers.en.lookup(output)[0]\n",
    "\n",
    "    # `tf.function` prevents us from using the attention_weights that were\n",
    "    # calculated on the last iteration of the loop.\n",
    "    # So, recalculate them outside the loop.\n",
    "    self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "    attention_weights = self.transformer.decoder.last_attn_scores\n",
    "\n",
    "    return text, tokens, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163b54f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(tokenizers, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b692c36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExportTranslator(tf.Module):\n",
    "  def __init__(self, translator):\n",
    "    self.translator = translator\n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
    "  def __call__(self, sentence):\n",
    "    (result,\n",
    "     tokens,\n",
    "     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0891ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = ExportTranslator(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17759bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(translator, export_dir='translator')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
