{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd675be7",
   "metadata": {},
   "source": [
    "The main setup for this analysis is a simple English-to-Hindi **Machine Translation** task, where the already trained Transformer model is queried with the English sentence, \"We are friends\".\n",
    "\n",
    "## I. Context: Training vs. Inference Behavior\n",
    "\n",
    "The crucial difference between the two stages is defined by the behavior of the Decoder.\n",
    "\n",
    "1.  **Encoder Behavior:** The **Encoder** behaves exactly the same during both the training stage and the inference stage. Therefore, no new discussion is needed regarding the Encoder’s operation.\n",
    "2.  **Decoder Behavior:** The **Decoder** behaves differently:\n",
    "    *   **Training Time:** The Decoder behaves in a **Non-Autoregressive** way, taking all output tokens simultaneously (due to techniques like Teacher Forcing).\n",
    "    *   **Inference Time:** The Decoder behaves in an **Autoregressive** way, predicting the output sequentially, one word at a time, over multiple time steps.\n",
    "\n",
    "## II. Transformer Inference: The Autoregressive Process\n",
    "\n",
    "The inference process begins with the Encoder processing the input, followed by the Decoder generating the output word by word.\n",
    "\n",
    "### A. Encoder Execution (One Time Operation)\n",
    "The input sentence, \"We are friends,\" is processed by the Encoder, which follows the same steps as during training:\n",
    "\n",
    "1.  The sentence is broken into tokens (V, are, friends).\n",
    "2.  Embeddings and Positional Encoding are calculated.\n",
    "3.  The vectors pass through the six stacked Encoder blocks, performing Multi-Head Attention, Feed-Forward Networks, and Add & Norm operations.\n",
    "4.  **Output:** The Encoder generates a set of **Context Vectors** (one for each input token) that represent the English sentence's meaning. These vectors, which are fixed throughout the entire inference process, are sent to the Decoder.\n",
    "\n",
    "### B. Decoder Execution (Step-by-Step Prediction)\n",
    "The Decoder processes the output sequentially, with each step generating one word.\n",
    "\n",
    "#### 1. Time Step 1: Predicting the First Word\n",
    "The processing starts by sending a single, special token into the Decoder:\n",
    "\n",
    "*   **Input:** The **Start of Sentence (SOS)** token is sent into the Decoder.\n",
    "*   **Vectorization:** The SOS token is vectorized by the embedding layer (e.g., 512-dimensional vector), and Positional Encoding is added.\n",
    "*   **Decoder Block Flow:** The resulting vector ($x_1$) passes through the first Decoder block, which contains Masked Self-Attention, Cross-Attention, and the Feed-Forward Network, followed by Add & Norm layers.\n",
    "    *   **MMHA:** Self-Attention is performed, though with only one token, it calculates the token's similarity to itself.\n",
    "    *   **Cross-Attention (CA):** The Decoder's $x_1$ vector (Query) interacts with the **Encoder's output** (Key and Value vectors from \"V,\" \"are,\" and \"friends\").\n",
    "*   **Stacking:** The output of the first Decoder block passes through five more identical blocks (six in total), although the parameter values in each block are different.\n",
    "*   **Final Output:** The final 512-dimensional vector is passed to the **Linear Layer**, which uses weights corresponding to the Hindi vocabulary count ($V$), followed by a **Softmax Layer**. The model picks the word with the highest probability (e.g., \"हम\"). This word is the first output token.\n",
    "\n",
    "#### 2. Time Step 2: Predicting Subsequent Words\n",
    "The Decoder is autoregressive, meaning its subsequent input includes the word predicted in the previous step.\n",
    "\n",
    "*   **Input:** The input for the second time step is now **two tokens**: the **SOS** token and the previously predicted word (\"हम\").\n",
    "*   **Vectorization:** Both tokens are embedded and receive Positional Encoding vectors (for Position 0 and Position 1), yielding two input vectors ($x_1, x_2$).\n",
    "*   **Decoder Block Flow (Parallel Processing):** The two vectors are processed simultaneously through the six stacked Decoder blocks.\n",
    "*   **Masked Self-Attention (MMHA) is Essential:** Although both tokens are known, **masking is still performed** during inference. This prevents the subsequent token (\"हम\") from contributing to the context of the preceding token (SOS), ensuring consistency with the training process. Masking is critical because the exact same work done on the training data must be done on the query data to maintain prediction quality and prevent a \"data shift\".\n",
    "*   **Cross-Attention:** The two new normalized vectors (Queries) interact with the fixed Encoder output (Keys and Values), generating two new contextualized vectors.\n",
    "*   **Final Output Selection (The Crucial Step):** After passing through the six blocks, two final vectors ($y_{f1\\text{norm}}, y_{f2\\text{norm}}$) are obtained.\n",
    "    *   The first vector ($y_{f1\\text{norm}}$) corresponds to the SOS token, and its output has already been determined (it resulted in \"हम\").\n",
    "    *   **Only the vector corresponding to the newest word** ($y_{f2\\text{norm}}$, which corresponds to \"हम\") is sent to the final Linear + Softmax layer to generate the next prediction.\n",
    "    *   The model picks the word with the highest probability (e.g., \"दोस्त\").\n",
    "\n",
    "#### 3. General Steps and Stopping Condition\n",
    "The process iterates:\n",
    "\n",
    "*   **Input Growth:** For every subsequent time step, the number of input tokens increases by one (e.g., Time Step 3 input: SOS, \"हम,\" \"दोस्त\").\n",
    "*   **Vector Selection:** At the end of the six Decoder blocks, the output vector corresponding to all previous tokens are ignored, and **only the final vector** (corresponding to the most recently predicted word) is sent to the output layer.\n",
    "*   **Stopping:** The inference process stops when the Decoder predicts the **End of Sentence (EOS)** token, finalizing the translation (e.g., \"हम दोस्त हैं\")."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
