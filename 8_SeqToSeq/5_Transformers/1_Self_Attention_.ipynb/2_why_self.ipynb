{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b76038f",
   "metadata": {},
   "source": [
    "\n",
    "## I. Why Self-Attention is an Attention Mechanism\n",
    "\n",
    "The term \"Attention\" applies to the mechanism because the mathematical steps and conceptual goal are identical to traditional attention models like **Luong Attention** or **Bahdanau Attention**.\n",
    "\n",
    "### 1. Recap of Traditional Attention (Luong/Bahdanau)\n",
    "Traditional attention was developed to solve the **memory loss** problem in the original **Encoder-Decoder (E-D) architecture**. The E-D model compressed the entire input sentence into a single **Context Vector** ($h_4$), which could not effectively summarize sentences longer than $\\sim 30$ words.\n",
    "\n",
    "Attention solved this by:\n",
    "1.  Providing a **new, dynamic Context Vector ($C_i$)** to the Decoder at every output time step $i$.\n",
    "2.  $C_i$ is a **Weighted Sum** of all Encoder Hidden States ($H_j$):\n",
    "    $$C_i = \\sum_{j} \\alpha_{i,j} H_j$$\n",
    "3.  The **Alignment Scores ($\\alpha_{i,j}$)**, which act as the weights, are calculated by normalizing (using Softmax) the raw similarity scores ($e_{i,j}$).\n",
    "4.  In **Luong Attention**, these raw scores ($e_{i,j}$) are obtained by taking the **dot product** of the Decoder's Hidden State ($S_i$) and the Encoder's Hidden State ($H_j$).\n",
    "\n",
    "### 2. Conceptual Similarity in Self-Attention\n",
    "Self-Attention (SA) calculates a contextual embedding for a target word ($Y_{turn}$) using an analogous three-step process:\n",
    "\n",
    "*   **Weighted Sum:** $Y_{turn}$ is calculated as a weighted sum of the **Value vectors ($V$)** of all words in the sentence.\n",
    "*   **Similarity Scores:** The raw similarity scores ($S_{i,j}$) needed for the weights are calculated by taking the **dot product** of the target word's **Query ($Q$) vector** and the **Key ($K$) vector** of every other word (including itself).\n",
    "*   **Normalization:** These scores are normalized using **Softmax** to yield the final weights ($W_{ij}$).\n",
    "\n",
    "### 3. Conceptual Mapping\n",
    "The mathematical formulation is identical, demonstrating why SA is fundamentally an attention mechanism:\n",
    "\n",
    "| Function in Luong Attention | Function in Self-Attention | Role |\n",
    "| :--- | :--- | :--- |\n",
    "| Decoder Hidden State ($S_i$) | **Query ($Q$) Vector** | Asks for similarity (\"How similar are you to me?\") |\n",
    "| Encoder Hidden State ($H_j$) | **Key ($K$) Vector** | Acts as the reference point for comparison |\n",
    "| Encoder Hidden State ($H_j$) | **Value ($V$) Vector** | Provides the content for the final weighted sum |\n",
    "\n",
    "Since the essential mathematical operations—calculating similarity via a dot product, normalizing via Softmax, and producing a contextual output via a weighted sum—are reused in a different setting, Self-Attention is structurally an Attention mechanism.\n",
    "\n",
    "## II. Why Self-Attention is Called \"Self\"\n",
    "\n",
    "The defining difference that mandates the use of \"Self\" is the source of the sequences involved in the calculation.\n",
    "\n",
    "### 1. Inter-Sequence Attention (Traditional Models)\n",
    "Traditional attention mechanisms, such as Luong and Bahdanau, calculate attention **between two different sequences**.\n",
    "\n",
    "*   For machine translation, this involves calculating alignment scores between an **English sequence** (Input/Encoder) and a **Hindi sequence** (Output/Decoder).\n",
    "*   This is known as **Inter-Sequence Attention**.\n",
    "\n",
    "### 2. Intra-Sequence Attention (Self-Attention)\n",
    "Self-Attention calculates the similarity scores and alignment weights **within the same sequence**.\n",
    "\n",
    "*   In Self-Attention, the model starts with a single sentence (e.g., \"Turn off the lights\").\n",
    "*   The Query vectors (the \"questions\") are derived from this sequence, and the Key vectors (the \"reference points\") are also derived from **the exact same sequence**.\n",
    "*   Since the sentence is querying itself to establish internal context, this is called **Intra-Sequence Attention**, hence the name **Self-Attention**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
