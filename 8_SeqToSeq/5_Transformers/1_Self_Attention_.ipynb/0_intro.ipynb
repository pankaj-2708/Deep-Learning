{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a796b6f3",
   "metadata": {},
   "source": [
    "## I. Foundation: The Necessity of Vectorization\n",
    "\n",
    "For any Natural Language Processing (NLP) application (such as machine translation, sentiment analysis, or Named Entity Recognition), the most important fundamental requirement is the ability to **convert words into numbers**. Computers do not understand words; they only understand numbers.\n",
    "\n",
    "This process is known in NLP as **vectorization**.\n",
    "\n",
    "### A. Early Vectorization Techniques\n",
    "\n",
    "Initial efforts focused on converting words and text into numerical representations:\n",
    "\n",
    "1.  **One-Hot Encoding:**\n",
    "    *   Requires determining the total unique words (vocabulary).\n",
    "    *   Each unique word is represented by a vector of zeros, with a '1' placed at the index corresponding to that word (e.g., MAT = 100, CAT = 010, RAT = 001).\n",
    "    *   This method is considered inefficient.\n",
    "2.  **Bag of Words (BoW):**\n",
    "    *   A slight improvement over one-hot encoding.\n",
    "    *   Represents a sentence by counting how many times each unique word from the vocabulary appears in that sentence (e.g., if MAT appears twice and CAT appears once, the representation might be).\n",
    "3.  **TF-IDF:** Mentioned as a further improvement upon techniques like Bag of Words.\n",
    "\n",
    "## II. The Power and Flaw of Word Embeddings\n",
    "\n",
    "**Word Embeddings** were introduced as a far more advanced technique compared to earlier methods, capable of converting words into numbers in a much better way.\n",
    "\n",
    "### A. Semantic Meaning and Representation\n",
    "Word Embeddings are effective because they capture **semantic meaning**.\n",
    "\n",
    "*   **Process:** They convert a word into an **N-dimensional vector** (e.g., N could be 64, 256, or 512). This is achieved by training a neural network on a very large training dataset, such as all Wikipedia articles, allowing the network to understand how each word is used in context.\n",
    "*   **Contextual Insight:** The embedding vector captures the word's inherent meaning and the types of contexts in which it is generally used.\n",
    "*   **Geometric Similarity:** If two words are semantically very **similar** (e.g., King and Queen), their corresponding N-dimensional vectors will also be very similar and positioned close to each other in the N-dimensional space. Conversely, dissimilar words (e.g., King and Cricketer) will have vectors that appear very different.\n",
    "*   **Dimensional Meaning:** Each dimension within the vector may represent a specific aspect (e.g., one dimension might represent \"royalty,\" another \"athleticism,\" and another \"humanity\"). Factors representing \"royalty\" would be high for King and Queen but low for Cricketer.\n",
    "\n",
    "### B. The Critical Flaw: Static and Average Meaning\n",
    "Despite their power, traditional Word Embeddings have a significant limitation: they are **static**.\n",
    "\n",
    "1.  **Average Meaning:** The neural network that creates the embedding captures the **average meaning** of a particular word across the entire training dataset.\n",
    "    *   *Example:* If the training data contains 10,000 sentences, and the word \"Apple\" appears as a fruit 9,000 times (high 'test' component) and as a phone company 1,000 times (high 'technology' component), the resulting static vector will be heavily biased towards the average meaning (i.e., fruit), showing a very high value for 'test' and a very low value for 'technology'.\n",
    "2.  **Context Insensitivity:** Since the embedding is trained once, the same static vector is used repeatedly, regardless of the context in which the word appears.\n",
    "    *   *Problem:* If a sentence uses \"Apple\" as a technology company (\"Apple launched a new phone\"), the static embedding (which is primarily optimized for \"fruit\") is inappropriate and problematic for accurate translation or analysis.\n",
    "\n",
    "## III. Self-Attention: Generating Contextual Embeddings\n",
    "\n",
    "To address the flaw of static embeddings, the **Self-Attention** mechanism was developed.\n",
    "\n",
    "Self-Attention is a mechanism that generates **smart contextual embeddings** from static embeddings, making them much better to use for any kind of NLP application.\n",
    "\n",
    "### A. The Goal of Contextual Embeddings\n",
    "Ideally, the embedding values should **dynamically change** based on the context of the sentence.\n",
    "\n",
    "*   If the word \"Apple\" appears near words like \"launch\" and \"phone,\" the embedding model should automatically increase the 'technology' component and decrease the 'fruit/test' component.\n",
    "*   The system should be smart enough not to be confused by other irrelevant words nearby (like \"orange\" in the same sentence).\n",
    "\n",
    "### B. Self-Attention as a Function\n",
    "Self-Attention can be viewed as a function or a box:\n",
    "\n",
    "1.  **Input:** The entire sentence is passed in, represented by the **static embeddings** of every word (e.g., embeddings for \"Apple,\" \"launch,\" \"phone,\" \"orange\").\n",
    "2.  **Calculation:** Inside the Self-Attention box, internal calculations run.\n",
    "3.  **Output:** For every input embedding, a new output embedding is generated.\n",
    "4.  **Result:** These output embeddings are the **smart contextual embeddings**, which accurately reflect how a particular word is used in that specific sentence's context.\n",
    "\n",
    "These resulting contextual embeddings are the essential component used in subsequent advanced architectures like the **Transformer**.\n",
    "\n",
    "The next step in understanding Self-Attention involves delving into *how* these calculations work, specifically involving concepts like **Query, Key, and Value vectors**.\n",
    "\n",
    "***\n",
    "*Analogy:* If a traditional dictionary entry (Word Embedding) provides only the *average, most common definition* of a word, the **Self-Attention Mechanism** acts like an experienced editor. The editor reads the entire paragraph (the sentence) and, using the context, dynamically highlights and prioritizes the *exact shade of meaning* required for that specific sentence, creating a precise, contextualized definition (Contextual Embedding) ready for translation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
