{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1950651",
   "metadata": {},
   "source": [
    "## I. Context: The Decoder Architecture\n",
    "\n",
    "The study of the Transformer is divided into two halves: the Encoder (already covered) and the Decoder. While the Decoder is architecturally more complex, many components repeat from the Encoder, including **Multi-Head Attention**, **Positional Encoding**, **Add & Norm layers**, and the **Feed-Forward Layer**.\n",
    "\n",
    "However, the Decoder introduces two major new blocks necessary for its primary function (generating sequential output):\n",
    "\n",
    "1.  **Masked Multi-Head Attention (Masked Self-Attention):** This is a different \"flavour\" of Self-Attention.\n",
    "2.  **Cross-Attention:** A mechanism where attention is calculated between the Encoder output and the Decoder input.\n",
    "\n",
    "This lecture focuses entirely on explaining **Masked Self-Attention**.\n",
    "\n",
    "## II. The Core Conceptual Problem: Autoregression\n",
    "\n",
    "The fundamental design challenge of the Transformer Decoder is summarized in one sentence:\n",
    "\n",
    "> **The Transformer Decoder is an Autoregressive model at Inference time, but it is Non-Autoregressive at Training time**.\n",
    "\n",
    "### A. Autoregressive Definition\n",
    "\n",
    "An **Autoregressive model** is a class of models that generate data points in a sequence by **conditioning each new data point on the previously generated points**.\n",
    "\n",
    "*   This concept originated in time series (e.g., stock prediction), where Friday's stock value depends on Wednesday's and Thursday's values.\n",
    "*   In NLP, models like the basic LSTM Encoder-Decoder are autoregressive: to predict the next word (\"मिलकर\"), the model requires the previously predicted word (\"आपसे\") as an input for the next time step.\n",
    "*   **Necessity:** For sequential data (like translation or text generation), autoregression is necessary because words inherently depend on the sequence that preceded them. You cannot generate the entire output paragraph in one go.\n",
    "\n",
    "### B. Inference vs. Training Time\n",
    "\n",
    "1.  **Inference (Prediction):** At prediction time, the model **must be Autoregressive**. The model must predict the output word-by-word sequentially because the input for the next time step is the unknown output from the current time step. This is a \"compulsion\".\n",
    "2.  **Training:** Ideally, the model should behave the same way during training and inference. However, if the Decoder is Autoregressive during training (processing tokens sequentially):\n",
    "    *   **Problem:** The training becomes **very slow**. For a 300-word paragraph, the Decoder's complex internal operations would have to run 301 times sequentially for just one training example.\n",
    "\n",
    "## III. The Breakthrough: Non-Autoregressive Training\n",
    "\n",
    "The Transformer architecture achieves **Non-Autoregressive training** (i.e., fast, parallel processing) by leveraging a technique called **Teacher Forcing**.\n",
    "\n",
    "### A. Teacher Forcing and Parallelism\n",
    "During training, the model uses the **Teacher Forcing** concept:\n",
    "\n",
    "*   Regardless of whether the model predicted the previous word correctly or incorrectly, the input fed to the next time step is always the **correct word from the dataset**.\n",
    "*   Since the correct target sentence (\"आप कैसे हैं\") is entirely available in the dataset, the sequential dependency is broken.\n",
    "*   Because the required inputs (Start token, \"आप,\" \"कैसे,\" \"हैं\") are all available simultaneously, the entire target sentence can be fed into the Decoder, and all output steps can be calculated **in parallel**.\n",
    "*   **Result:** Training speed becomes **much faster**.\n",
    "\n",
    "### B. The Conflict: Data Leakage\n",
    "While Non-Autoregressive parallel processing solves the speed problem, it introduces a severe issue: **Data Leakage** (or \"cheating\").\n",
    "\n",
    "*   When all output tokens (e.g., \"आप,\" \"कैसे,\" \"हैं\") are processed simultaneously in the **Self-Attention block**, the calculation for the **current token's** contextual embedding uses the information from **future tokens**.\n",
    "*   *Example:* The contextual embedding for the word \"**आप**\" (position 1) is derived using contributions from \"कैसे\" (position 2) and \"हैं\" (position 3).\n",
    "*   **Why this is cheating:** During real-world inference, the future words (\"कैसे,\" \"हैं\") are unknown. Allowing the model to see future tokens during training (data leakage) causes it to perform well on the training data but perform **very poorly** on real-world prediction data.\n",
    "\n",
    "## IV. The Solution: Masked Self-Attention (MSA)\n",
    "\n",
    "The purpose of **Masked Self-Attention** is to **prevent data leakage** during parallel training while still preserving the speed benefits of Non-Autoregressive execution.\n",
    "\n",
    "The goal is to mathematically zero out the contributions of future tokens to the current token's contextual embedding.\n",
    "\n",
    "### A. Tracing the Error to Attention Weights\n",
    "The contribution of any word to another is determined by the final **Attention Weights ($W_{ij}$)**, which are derived from the raw attention score matrix (the result of the $QK^T$ dot product, scaled and before Softmax).\n",
    "\n",
    "The weights that must be eliminated (set to zero) are those linking a word to any token that follows it:\n",
    "\n",
    "| Query (Current Word) | Key (Context Words) | Status | Action Required |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **आप** (Pos 1) | कैसे (Pos 2), हैं (Pos 3) | **Future Tokens** | Must be zeroed. |\n",
    "| **कैसे** (Pos 2) | हैं (Pos 3) | **Future Token** | Must be zeroed. |\n",
    "| **हैं** (Pos 3) | None | **None** | No zeroing needed. |\n",
    "\n",
    "### B. The Masking Mechanism\n",
    "\n",
    "The masking process is a single, crucial step added before the Softmax operation:\n",
    "\n",
    "1.  **Calculate Scaled Scores:** The standard Self-Attention steps are performed: $QK^T$ is calculated, scaled by $\\sqrt{d_k}$, resulting in a raw attention score matrix.\n",
    "2.  **Create Mask Matrix:** A **Mask Matrix** of the exact same dimensions is created.\n",
    "3.  **Inserting Negative Infinity:** In every position corresponding to a **future token** that needs to be ignored, the Mask Matrix inserts the value **$-\\infty$ (minus infinity)**. All other positions contain zero.\n",
    "4.  **Addition:** The raw attention score matrix is **added** to the Mask Matrix. This replaces the scores that pointed to future tokens with $-\\infty$.\n",
    "5.  **Softmax Application:** The resulting matrix (containing $-\\infty$) is then passed through the **Softmax** function.\n",
    "6.  **Zeroing Effect:** Mathematically, the result of $\\text{Softmax}(-\\infty)$ is **zero**.\n",
    "\n",
    "By setting the corresponding attention weights to zero, the future tokens' contribution to the current token's contextual embedding is successfully eliminated. This delivers the \"best of both worlds\": **fast parallel processing (Non-Autoregressive)** while ensuring the current token only uses information from the preceding sequence, preventing data leakage.\n",
    "\n",
    "***\n",
    "*In essence, Masked Self-Attention allows the Transformer Decoder to train like a marathon runner, using all its speed and parallelism, but strategically blindfolding it so it can only look backward at the words it has generated, ensuring an honest learning process for sequential prediction.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
