{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8c878cf-ea87-4038-988f-452c605319fc",
   "metadata": {},
   "source": [
    "## CNN Lecture 4: Padding & Strides\n",
    "\n",
    "### I. Padding\n",
    "\n",
    "Padding is an operation designed to address two primary problems that arise during standard convolution operations.\n",
    "\n",
    "#### A. Problems Solved by Padding\n",
    "1.  **Size Reduction and Information Loss (Resolution Loss)**:\n",
    "    *   When a filter is convolved over an image, the resulting **feature map** is smaller than the input image. For instance, a $5 \\times 5$ image processed by a $3 \\times 3$ filter results in a $3 \\times 3$ feature map.\n",
    "    *   Applying successive convolutional layers causes the image size to progressively shrink (e.g., $28 \\times 28 \\to 26 \\times 26 \\to 24 \\times 24$, etc.). This continuous reduction leads to information loss, which is undesirable.\n",
    "2.  **Uneven Feature Importance (Border Pixels)**:\n",
    "    *   Pixels located on the side or border of the image participate in fewer convolution operations compared to pixels in the centre.\n",
    "    *   For example, a central pixel might be part of three or four different convolution operations, giving it a greater \"say\" or importance in the final feature map. Border pixels, however, might only participate in one operation.\n",
    "    *   If crucial information resides in the side pixels, this information may not be adequately filtered out and preserved in the feature map.\n",
    "\n",
    "#### B. The Padding Mechanism\n",
    "*   Padding involves adding extra columns and rows (a boundary) around the input image.\n",
    "*   The pixel values added in padding are typically **zeroes**. This operation is therefore commonly referred to as **Zero Padding** in literature.\n",
    "*   The ultimate goal of padding is often to ensure that the size of the feature map matches the size of the input image.\n",
    "\n",
    "#### C. Feature Map Size Calculation with Padding\n",
    "The formula for calculating the shape of the resulting feature map, where $N$ is the image size, $F$ is the filter size, and $P$ is the padding amount, is:\n",
    "$$\\mathbf{(N + 2P - F + 1)}$$\n",
    "\n",
    "*   *Example:* For a $5 \\times 5$ image ($N=5$) and a $3 \\times 3$ filter ($F=3$), if padding $P=1$ is applied, the output size is $5 + 2(1) - 3 + 1 = 5$, resulting in a $5 \\times 5$ feature map.\n",
    "\n",
    "#### D. Keras Implementation of Padding\n",
    "When working with Keras, two main padding options are available:\n",
    "1.  **'Valid'**: This option applies **no padding** (it functions as standard convolution).\n",
    "2.  **'Same'**: Keras automatically determines the necessary padding amount ($P$) required to ensure the output feature map size is the **same** as the input image size.\n",
    "\n",
    "### II. Strides\n",
    "\n",
    "Strides define the movement of the convolutional filter across the input image.\n",
    "\n",
    "#### A. Definition and Movement\n",
    "*   The default stride value is **$1 \\times 1$**. This means the filter moves one pixel to the right after each operation, and then one pixel downwards when moving to the next row.\n",
    "*   Strides can be changed (e.g., to $2 \\times 2$). A stride of $2 \\times 2$ means the filter jumps two pixels horizontally and two pixels vertically for the next convolution operation.\n",
    "*   If the stride value is greater than one, the process is called **Strided Convolution**.\n",
    "\n",
    "#### B. Effect on Feature Map Size\n",
    "*   Increasing the stride value **decreases the size** of the resulting feature map.\n",
    "\n",
    "#### C. Feature Map Size Calculation with Strides\n",
    "The general formula for calculating the size of the feature map, including padding ($P$), filter size ($F$), image size ($N$), and stride ($S$), is:\n",
    "$$\\mathbf{(N + 2P - F) / S + 1}$$\n",
    "\n",
    "If padding is zero ($P=0$), the formula simplifies to:\n",
    "$$(N - F) / S + 1$$\n",
    "\n",
    "#### D. Special Case: Non-Integer Results\n",
    "When dividing by the stride ($S$) results in a decimal value, it signifies that there are insufficient pixels remaining to perform the final convolution operation.\n",
    "*   In this scenario, a **floor operation** is applied to the result of the division (rounding down to the nearest integer) before adding 1. This ensures that only completed convolution operations are considered in the final size.\n",
    "\n",
    "#### E. Why Strides Are Necessary\n",
    "Using strided convolution means the filter is skipping information, taking \"jumps\" across the image. Two primary reasons for using strided convolution are:\n",
    "1.  **High-Level Feature Extraction**: If the goal is to capture only **high-level, gross features** and ignore low-level features (the fine details or subtleties of the image), a larger stride is used. A smaller stride allows the CNN to capture these low-level features.\n",
    "2.  **Computational Efficiency**: When training is required on large datasets, increasing the stride speeds up the process because it rapidly reduces the size of the data being processed, saving computational power. (Note: This reason has become less critical recently due to advances in computing power, and models often use a default stride of 1.)\n",
    "\n",
    "#### F. Keras Implementation of Strides\n",
    "In Keras, strides are implemented using a `stride` parameter, which is a tuple (e.g., `(2, 2)`). This tuple specifies the movement in the horizontal direction (first number) and the vertical direction (second number). Using a $2 \\times 2$ stride on a $28 \\times 28$ input image rapidly reduces the size after consecutive convolutional layers (e.g., $28 \\times 28 \\to 14 \\times 14 \\to 7 \\times 7$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4cb6777-d2fe-45a0-aedb-d31aebc6b98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m9,248\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m9,248\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m9,248\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,640</span> (111.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m28,640\u001b[0m (111.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,640</span> (111.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m28,640\u001b[0m (111.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "\n",
    "# eg model\n",
    "model=Sequential()\n",
    "model.add(layers.Conv2D(32,(3,3),activation='relu',input_shape=(50,50,3)))\n",
    "model.add(layers.Conv2D(32,(3,3),activation='relu',padding='valid'))\n",
    "model.add(layers.Conv2D(32,(3,3),activation='relu',padding='same'))\n",
    "model.add(layers.Conv2D(32,(3,3),activation='relu',padding='same',strides=(2,2)))\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
