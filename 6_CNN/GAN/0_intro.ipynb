{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5550125-0cb0-4a95-b896-3c7a4e722add",
   "metadata": {},
   "source": [
    "## Lecture Notes: Generative Adversarial Networks (GANs)\n",
    "\n",
    "### I. Introduction \n",
    "\n",
    "*   **Upcoming Complex Topics:** The following sessions, including GANs, Object Detection, and Segmentation, involve more complex topics that will be covered sequentially.\n",
    "*   **Focus on GANs:** We will prioritize covering **GANs** today (theory) and tomorrow (coding).\n",
    "*   **Background:** GANs are a highly interesting topic focusing on **image generation**, which differs significantly from traditional classification tasks. The initial GAN paper was released in **2014**, around the time VGG and ResNet architectures were being set up.\n",
    "*   **Recommendation:** It is highly recommended to read the original research paper multiple times after the session to gain in-depth details, understand what worked, and what failed.\n",
    "\n",
    "### II. Understanding the Components of GANs\n",
    "\n",
    "GANs stands for **Generative Adversarial Networks**.\n",
    "\n",
    "#### 1. Generative (G)\n",
    "*   The core goal is to **generate entirely new images** (or data).\n",
    "*   This differs from **Data Augmentation**, where existing input images are merely transformed (e.g., cropping, rotation).\n",
    "*   Modern examples of generative models include DALL-E and Sora.\n",
    "\n",
    "#### 2. Networks (N)\n",
    "*   GANs utilize **two different neural network models**:\n",
    "    *   The **Generator (G)** Model.\n",
    "    *   The **Discriminator (D)** Model.\n",
    "*   Unlike previous architectures (like CNNs, VGGNet, ResNet) which typically involve a single network model, GANs are a **multi-network model**. Both the Generator and the Discriminator are neural networks that need to be trained.\n",
    "\n",
    "#### 3. Adversarial (A)\n",
    "*   The term \"adversarial\" refers to the clashing relationship between the two networks, forming a **Min Max Two-Player Game**. This is often illustrated using the **Police and Thief analogy**:\n",
    "\n",
    "| Component | Role / Analogy | Objective |\n",
    "| :--- | :--- | :--- |\n",
    "| **Generator (G)** | Thief | To generate **fake images** that are so realistic they **fool the Discriminator** into thinking they are real. |\n",
    "| **Discriminator (D)** | Police | To **accurately classify** whether an input image is **Real** or **Fake**. |\n",
    "\n",
    "*   **Ultimate Goal:** The training process aims to improve the Generator until it creates images so similar to the real data that the Discriminator is completely confused and cannot distinguish between them. This confusion is represented when the Discriminator predicts a probability of **0.5 (half)** for all inputs.\n",
    "\n",
    "### III. GAN Architecture and Data Flow\n",
    "\n",
    "#### 1. Generator Input: The Latent Vector (Z)\n",
    "*   The Generator's input is a **Latent Vector (Z)**, which is fundamentally a **Random Noise Vector** (a bunch of random numbers).\n",
    "*   These random values are **not trainable parameters**.\n",
    "*   **Conditional GANs (CGANs)** allow for controlling this input noise based on specified statistics or conditions (like gender or age), leading to easier convergence and guided generation.\n",
    "\n",
    "#### 2. Generator Operation\n",
    "*   The Generator transforms the input vector of numbers (noise) into an image.\n",
    "*   It achieves this transformation using **Convolutional Transpose** operations.\n",
    "*   Convolutional Transpose operations perform **upscaling** (the opposite of standard convolution, which reduces size).\n",
    "*   The **kernels** within these layers are the **trainable parameters**. As the model is trained across many epochs, these kernels update their values, eventually enabling the random input to produce highly realistic images (e.g., generating new, non-existent faces based on trained datasets).\n",
    "\n",
    "#### 3. Discriminator Operation\n",
    "*   The Discriminator is typically a neural network architecture similar to a **basic CNN classification algorithm**.\n",
    "*   Its output is a probability, usually using a **Sigmoid activation function**, classifying the input as either Class 0 (Fake) or Class 1 (Real).\n",
    "*   The input to the Discriminator is a batch **mixing 50% Real Images** (X) and **50% Fake Images** (G(Z)).\n",
    "\n",
    "#### 4. Naming Conventions (Notation)\n",
    "| Input/Output | Notation | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| Real Image Input | $X$ | Data from the original dataset. |\n",
    "| Random Noise Input | $Z$ | The Latent Vector input to G. |\n",
    "| Generator Output | $G(Z)$ | The generated (fake) image. |\n",
    "| D output (Real) | $D(X)$ | Probability that a real image is real. |\n",
    "| D output (Fake) | $D(G(Z))$ | Probability that a generated image is real. |\n",
    "\n",
    "### IV. Training and Loss Functions\n",
    "\n",
    "*   Since there are two neural networks, there are two distinct loss functions (Discriminator Loss and Generator Loss) that are updated via **backpropagation**.\n",
    "*   The weights of the two networks are **different** and are trained individually.\n",
    "\n",
    "#### 1. Training Loop Steps\n",
    "Training must continue for multiple epochs:\n",
    "1.  **Generate Fake Data:** Generate a batch of random noise (Z). Pass Z to the Generator (G) to produce fake images ($G(Z)$).\n",
    "2.  **Mix Data:** Get a batch of real images (X). Mix the real and fake images.\n",
    "3.  **Train Discriminator:**\n",
    "    *   **Freeze the Generator**.\n",
    "    *   Pass the mixed batch to the Discriminator (D) and train D to accurately classify between Real (Label 1) and Fake (Label 0).\n",
    "4.  **Train Generator:**\n",
    "    *   **Freeze the Discriminator**.\n",
    "    *   Calculate the Generator loss.\n",
    "    *   **Crucially:** When training the Generator, the fake images ($G(Z)$) are intentionally labeled as **Real (Label 1)**. This is done because the Generator's goal is to fool the Discriminator; by telling the system that its output should be classified as Real, it forces D to make a mistake, which updates G's weights.\n",
    "    *   üí° Why we label fakes as ‚Äú1‚Äù for G‚Äôs loss<br>\n",
    "            Because:<br>\n",
    "                We want <br>\n",
    "                    D(G(z)) to be close to 1 (the discriminator believing the fake is real).<br>\n",
    "                    To make this happen, we compute loss between  D(G(z)) and label = 1.<br>\n",
    "\n",
    "            That‚Äôs how the gradient will push G‚Äôs weights to make more realistic outputs.<br>\n",
    "5.  **Repeat:** Repeat this process until the Discriminator consistently outputs 0.5.\n",
    "\n",
    "#### 2. Loss Function Intuition (Min Max)\n",
    "*   The overall loss function for GANs is structurally similar to the **Binary Cross-Entropy Loss**.\n",
    "*   **Discriminator Objective (Maximization):** The Discriminator (D) seeks to **maximize** the overall loss (Max D). It maximizes the loss by becoming highly accurate, ensuring that $D(X)$ is close to 1 (real) and $D(G(Z))$ is close to 0 (fake). A high loss (far from the confusion point of 0.5) indicates D is successfully distinguishing Real from Fake.\n",
    "*   **Generator Objective (Minimization):** The Generator (G) seeks to **minimize** the overall loss (Min G). G minimizes the loss when it creates images so realistic that the Discriminator is forced to output 0.5, thus making a mistake (being fooled).\n",
    "*   This conflict is why the training corresponds to the **min max $V(D, G)$** objective.\n",
    "  <img src=\"./images/gan_loss (3).png\">\n",
    "  <img src=\"./images/gan_loss (2).png\">\n",
    "  <img src=\"./images/gan_loss (1).png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
