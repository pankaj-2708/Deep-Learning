{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4221d6fa-a7db-490f-9a14-8b2257ffef27",
   "metadata": {},
   "source": [
    "## Lecture Notes: Backpropagation in CNN (Part 1)\n",
    "\n",
    "### I. Context and Difficulty\n",
    "\n",
    "The study of backpropagation in CNNs is a **very important and conceptual topic**. It is inherently difficult and involves significant mathematics (maths), potentially requiring the viewer to watch the material multiple times and use a copy pen to follow the calculations. While deep learning libraries handle this automatically, having a deeper understanding allows for better use of the tools.\n",
    "\n",
    "This overall topic is divided into three parts:\n",
    "\n",
    "1.  **Part 1 (Current focus):** Understanding the overall application of backpropagation in the CNN architecture.\n",
    "2.  **Part 2 (Next video):** Discussing how backpropagation applies specifically to the **Convolution, Max Pooling, and Flattening operations**.\n",
    "3.  **Part 3 (Last video):** Applying backpropagation to a more complex CNN architecture.\n",
    "\n",
    "### II. Simple CNN Architecture and Data Flow\n",
    "\n",
    "To simplify the explanation, a basic CNN architecture is used.\n",
    "\n",
    "#### A. Architecture Overview\n",
    "\n",
    "1.  **Input:** An image ($X$), assumed to be $6 \\times 6$.\n",
    "2.  **Convolution:** The image is passed through a $3 \\times 3$ filter.\n",
    "    *   *Output Shape:* $4 \\times 4$ Feature Map.\n",
    "3.  **Activation:** The Feature Map passes through a **ReLU operation**.\n",
    "    *   *Output Shape:* Remains $4 \\times 4$.\n",
    "4.  **Pooling:** Max Pooling is applied (e.g., $2 \\times 2$ window and stride of 2).\n",
    "    *   *Output Shape:* $2 \\times 2$.\n",
    "5.  **Flattening:** The $2 \\times 2$ output is flattened, resulting in a 1D vector of length 4.\n",
    "6.  **Fully Connected Layer:** The 4 inputs are sent to a single output neuron.\n",
    "7.  **Prediction:** The output provides a prediction (e.g., $1 \\times 1$ scalar).\n",
    "\n",
    "#### B. Trainable Parameters\n",
    "\n",
    "There are only two locations in this simple architecture where **trainable parameters** (weights and biases) exist.\n",
    "\n",
    "| Location | Parameter | Count | Description |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Convolution Filter** | $W_1$ | 9 weights ($3 \\times 3$) | The values inside the $3 \\times 3$ filter. |\n",
    "| | $B_1$ | 1 bias (Shape $1 \\times 1$) | The filter's bias. |\n",
    "| **Fully Connected Layer** | $W_2$ | 4 weights (Shape $1 \\times 4$) | Weights connecting the 4 flattened inputs to the single neuron. |\n",
    "| | $B_2$ | 1 bias (Shape $1 \\times 1$) | The output neuron's bias. |\n",
    "| **Total** | | **15 trainable parameters** | Optimising these 15 parameters is the goal of backpropagation. |\n",
    "\n",
    "The Loss Function used is **Binary Cross-Entropy** (applicable for binary classification, like determining if an image is a dog or not).\n",
    "\n",
    "### III. Logical Flow and Forward Propagation\n",
    "\n",
    "To understand backpropagation, the entire CNN architecture is represented by a logical diagram.\n",
    "\n",
    "#### A. Logical Diagram Flow\n",
    "\n",
    "$$X \\xrightarrow{\\text{Conv}, W_1, B_1} Z_1 \\xrightarrow{\\text{ReLU}} A_1 \\xrightarrow{\\text{Max Pooling}} P_1 \\xrightarrow{\\text{Flatten}} F \\xrightarrow{\\text{FC, } W_2, B_2} Z_2 \\xrightarrow{\\text{Sigmoid}} A_2 (\\hat{y}) \\rightarrow L$$\n",
    "\n",
    "#### B. Forward Propagation Equations\n",
    "\n",
    "The forward propagation describes the calculation flow:\n",
    "\n",
    "1.  $$Z_1 = \\text{Convolution}(X, W_1) + B_1$$\n",
    "2.  $$A_1 = \\text{ReLU}(Z_1)$$\n",
    "3.  $$P_1 = \\text{Max Pooling}(A_1)$$\n",
    "4.  $$F = \\text{Flatten}(P_1)$$\n",
    "5.  $$Z_2 = \\text{Dot Product}(F, W_2) + B_2$$ (Note: $W_2$ is multiplied by $F$, or $F$ is multiplied by $W_2$, depending on order).\n",
    "6.  $$A_2 = \\text{Sigmoid}(Z_2)$$ (This is the prediction $\\hat{y}$).\n",
    "7.  $$L = \\text{Loss}(Y, A_2)$$.\n",
    "\n",
    "### IV. Backpropagation: The Objective\n",
    "\n",
    "The end goal of backpropagation is to apply the **Gradient Descent algorithm** to find the optimal values for $W_1, B_1, W_2, \\text{and } B_2$ that minimise the loss ($L$).\n",
    "\n",
    "To achieve this, four specific derivatives (gradients) must be calculated:\n",
    "\n",
    "1.  $$\\frac{\\partial L}{\\partial W_1}$$\n",
    "2.  $$\\frac{\\partial L}{\\partial B_1}$$\n",
    "3.  $$\\frac{\\partial L}{\\partial W_2}$$\n",
    "4.  $$\\frac{\\partial L}{\\partial B_2}$$\n",
    "\n",
    "#### A. Conceptual Segmentation\n",
    "\n",
    "The architecture can be logically separated into two components for easier analysis:\n",
    "\n",
    "1.  **ANN Part (Fully Connected):** Everything after the Flattening layer (from $F$ onwards).\n",
    "2.  **CNN Part:** Everything up to the Flattening layer (Convolution, ReLU, Max Pooling).\n",
    "\n",
    "Backpropagation starts from the loss function ($L$) and moves backwards.\n",
    "\n",
    "### V. Derivation of Gradients for the ANN Part ($W_2, B_2$)\n",
    "\n",
    "The calculation begins with the derivatives related to the Fully Connected (ANN) part. These calculations use the Chain Rule.\n",
    "\n",
    "#### A. Gradient for $W_2$ ($\\frac{\\partial L}{\\partial W_2}$)\n",
    "\n",
    "The chain rule states that changing $W_2$ affects $Z_2$, which affects $A_2$, which finally affects $L$.\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial A_2} \\times \\frac{\\partial A_2}{\\partial Z_2} \\times \\frac{\\partial Z_2}{\\partial W_2}$$\n",
    "\n",
    "The final calculated derivative for $W_2$ is:\n",
    "$$\\frac{\\partial L}{\\partial W_2} = (A_2 - Y_i) \\times F^T$$\n",
    "\n",
    "The resulting shape of $\\frac{\\partial L}{\\partial W_2}$ must match the shape of $W_2$ (which is $1 \\times 4$).\n",
    "\n",
    "#### B. Gradient for $B_2$ ($\\frac{\\partial L}{\\partial B_2}$)\n",
    "\n",
    "The chain rule for the bias $B_2$ follows a similar structure.\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial B_2} = \\frac{\\partial L}{\\partial A_2} \\times \\frac{\\partial A_2}{\\partial Z_2} \\times \\frac{\\partial Z_2}{\\partial B_2}$$\n",
    "\n",
    "The final calculated derivative for $B_2$ in the single instance case is:\n",
    "$$\\frac{\\partial L}{\\partial B_2} = A_2 - Y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4538452-658b-41cb-a706-30b6d7b5d97f",
   "metadata": {},
   "source": [
    "### CNN Architecture and Chain Rule\n",
    "A basic CNN architecture involves the following steps:\n",
    "Input $X$ $\\rightarrow$ Convolution ($Z_1$) $\\rightarrow$ ReLU ($A_1$) $\\rightarrow$ Max Pooling ($P_1$) $\\rightarrow$ Flatten ($F$) $\\rightarrow$ Fully Connected ($Z_2$) $\\rightarrow$ Sigmoid ($A_2$ or $Y_{hat}$) $\\rightarrow$ Loss ($L$).\n",
    "\n",
    "To calculate $\\frac{\\partial L}{\\partial W_1}$, the chain rule is applied:\n",
    "$$\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial A_2} \\times \\frac{\\partial A_2}{\\partial Z_2} \\times \\frac{\\partial Z_2}{\\partial F} \\times \\frac{\\partial F}{\\partial P_1} \\times \\frac{\\partial P_1}{\\partial A_1} \\times \\frac{\\partial A_1}{\\partial Z_1} \\times \\frac{\\partial Z_1}{\\partial W_1}$$\n",
    "\n",
    "*Note: The product of the initial terms $\\frac{\\partial L}{\\partial A_2} \\times \\frac{\\partial A_2}{\\partial Z_2}$ simplifies to $A_2 - Y$ (where $Y$ is the target label). The next term, $\\frac{\\partial Z_2}{\\partial F}$, is effectively the weights of the fully connected layer, $W_2$. The resulting product of the first three terms yields $\\frac{\\partial L}{\\partial F}$.*\n",
    "\n",
    "***\n",
    "\n",
    "## Backpropagation on Specific CNN Layers\n",
    "\n",
    "### 1. Backpropagation on the Flatten Layer\n",
    "The Flatten operation converts a 2D tensor (like the Max Pooling output $P_1$) into a 1D vector ($F$).\n",
    "\n",
    "*   **No Trainable Parameters:** The Flatten layer does not contain trainable parameters, so conventional differentiation is not used.\n",
    "*   **Reverse Operation:** Backpropagation is achieved by performing the exact reverse of the forward operation.\n",
    "*   **Procedure:** The incoming gradient ($\\frac{\\partial L}{\\partial F}$, which is a $1\\text{D}$ vector) is simply **reshaped** back into the multidimensional format of $P_1$ (e.g., converting a $4 \\times 1$ vector back to a $2 \\times 2$ matrix). This reshaped matrix represents $\\frac{\\partial L}{\\partial P_1}$.\n",
    "\n",
    "### 2. Backpropagation on the Max Pooling Layer\n",
    "The Max Pooling operation selects the maximum item in each window (e.g., $4 \\times 4$ input $A_1$ becomes $2 \\times 2$ output $P_1$).\n",
    "\n",
    "*   **No Trainable Parameters:** The Max Pooling layer also has **no trainable parameters**.\n",
    "*   **Reverse Operation (Unpooling):** The gradient is propagated backward by reversing the pooling operation.\n",
    "*   **Procedure:** Only the numbers that were **maximum** in their respective windows during the forward pass contributed to the final prediction and, therefore, the loss.\n",
    "    *   The incoming gradient matrix ($\\frac{\\partial L}{\\partial P_1}$, e.g., $2 \\times 2$) is used to create a larger matrix (the shape of $A_1$, e.g., $4 \\times 4$).\n",
    "    *   The values of the incoming gradient are placed precisely in the locations (indices) where the maximum items were located in the original matrix $A_1$ (before pooling).\n",
    "    *   All other positions in the resulting matrix are set to **zero**, as those non-maximum numbers had no contribution to the loss prediction.\n",
    "\n",
    "Mathematically, $\\frac{\\partial L}{\\partial A_1}(x, y)$ equals $\\frac{\\partial L}{\\partial P_1}(m, n)$ if $A_1(x, y)$ was the maximum element in its pooling region, and 0 otherwise.\n",
    "\n",
    "### 3. Backpropagation on the ReLU Activation Layer\n",
    "The relationship between $A_1$ (ReLU output) and $Z_1$ (convolution output) is used to find $\\frac{\\partial A_1}{\\partial Z_1}$.\n",
    "\n",
    "*   **Differentiation of ReLU:** The derivative of ReLU is used to calculate $\\frac{\\partial A_1}{\\partial Z_1}$.\n",
    "    *   If the corresponding item in $Z_1$ is **greater than zero**, the derivative is **1**.\n",
    "    *   If the corresponding item in $Z_1$ is **less than or equal to zero**, the derivative is **0**.\n",
    "*   **Procedure:** To obtain $\\frac{\\partial L}{\\partial Z_1}$, the gradient coming from the Max Pooling layer ($\\frac{\\partial L}{\\partial A_1}$) is **multiplied element-wise** by the ReLU derivative matrix ($\\frac{\\partial A_1}{\\partial Z_1}$).\n",
    "\n",
    "### 4. Backpropagation on the Convolution Layer\n",
    "This involves calculating the gradients for the bias ($B_1$) and the weights ($W_1$).\n",
    "\n",
    "#### A. Gradient with Respect to Bias ($\\frac{\\partial L}{\\partial B_1}$)\n",
    "The bias $B_1$ is added uniformly to every element of the convolution output $Z_1$.\n",
    "\n",
    "*   **Procedure:** Because $Z_{1, ij}$ (any individual output element) has a derivative of 1 with respect to $B_1$, the total derivative $\\frac{\\partial L}{\\partial B_1}$ is simply the **summation of all the values** in the incoming gradient matrix ($\\frac{\\partial L}{\\partial Z_1}$).\n",
    "\n",
    "#### B. Gradient with Respect to Weights ($\\frac{\\partial L}{\\partial W_1}$)\n",
    "Calculating the derivative for the weights involves applying the chain rule to individual weights and their corresponding inputs ($X$).\n",
    "\n",
    "*   **Key Insight (Pattern Recognition):** While the expansion of the derivative results in complex summation terms, the overall operation to find $\\frac{\\partial L}{\\partial W_1}$ simplifies significantly.\n",
    "*   **Procedure:** The derivative of the loss with respect to the filter weights ($\\frac{\\partial L}{\\partial W_1}$) is obtained by performing a **convolution operation of the input image ($X$) with the incoming loss gradient matrix ($\\frac{\\partial L}{\\partial Z_1}$)**.\n",
    "\n",
    "***\n",
    "Do you want to focus on how the Max Pooling layer determines which gradient values to retain and which to set to zero?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
