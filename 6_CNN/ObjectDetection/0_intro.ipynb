{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3562c664-52d0-49be-8ba5-5ee169c0e3d5",
   "metadata": {},
   "source": [
    "## Object Detection and R-CNN Architecture\n",
    "\n",
    "### I. Introduction and Course Outline\n",
    "\n",
    "The session focuses on advanced computer vision topics.\n",
    "\n",
    "#### A. Course Content and Timeline\n",
    "1.  **Object Detection Series:** The series will cover R-CNN, Fast R-CNN, and Faster R-CNN.\n",
    "    *   **R-CNN:** It is a \"very old world architecture\" and an \"old technique\" developed at a time period different from the present.\n",
    "    *   **Advanced Models:** Fast R-CNN and Faster R-CNN are considered slightly more advanced models compared to R-CNN.\n",
    "2.  **State-of-the-Art (SOTA):** YOLO (You Only Look Once) is the current state-of-the-art technique. The course will cover the basic understanding of YOLO, the ideas behind its modifications, and possibly versions V5 or V10.\n",
    "3.  **Historical Timeline (Object Detection):** Object detection models began appearing around 2013-2014, following perfected image classification techniques (like VGG16).\n",
    "    *   1998: LeNet (Image Classification foundation).\n",
    "    *   2014: R-CNN released.\n",
    "    *   2015: Fast R-CNN.\n",
    "    *   2018: YOLO V3.\n",
    "    *   Current: YOLO V10.\n",
    "\n",
    "#### B. Projects and Deployment\n",
    "1.  **YOLO Project:** A multi-class object detection project will be undertaken using YOLO.\n",
    "2.  **Deployment:** A dedicated session will cover deploying a Casava leaf detection model on a cloud API, primarily Google GCP.\n",
    "3.  **Final Project (Self-Driving Car):** This project will be the last but one topic.\n",
    "    *   **Scope:** The project will incorporate only one or two techniques.\n",
    "    *   **Comparison to Industry Models:** Complex commercial systems like Tesla's use a \"very strong hybrid model\" combining segmentation, lane detection, classification, object detection, and signal finding to predict acceleration, braking, and turning.\n",
    "\n",
    "### II. Foundational Concepts: Classification, Localization, and Detection\n",
    "\n",
    "#### A. Object Detection Definition\n",
    "Object detection is a part of computer vision where the goal is not only to classify the image but also to classify the objects present inside the image and build a bounding box for every object.\n",
    "\n",
    "#### B. Classification vs. Localization vs. Detection\n",
    "1.  **Classification (Normal CNN):** Classifies what kind of image is present (e.g., house, dog, person). It provides a class prediction/confidence score (e.g., probability 0.9) but does not specify location or count of objects.\n",
    "2.  **Object Localization:** Classification combined with a single label plus the object location. It is used when there is **only one object** in focus in the image.\n",
    "3.  **Object Detection (Goal):** Requires detecting **all objects and all classes** within an image, irrespective of the object's shape or size. Output includes the label, confidence score, and bounding box for every prediction.\n",
    "\n",
    "#### C. Segmentation vs. Object Detection\n",
    "1.  **Object Detection:** Works on the **object level** using a bounding box. The bounding box often covers background pixels that are not part of the object.\n",
    "2.  **Segmentation:** An advanced topic that works on the **pixel level**, giving the output as a mask.\n",
    "3.  **Hybrid Models:** Segmentation and object detection models can be mixed to create a hybrid model.\n",
    "\n",
    "### III. Bounding Box Details\n",
    "\n",
    "#### A. Shape and Alignment\n",
    "1.  **Shape:** Bounding boxes are generally **rectangular or square** because they are easier to calculate distances.\n",
    "2.  **Axis Alignment:** The focus is on bounding boxes that are **aligned to the X and Y axes** (axis-aligned bounding boxes), meaning they are parallel to the axes.\n",
    "\n",
    "#### B. Goal and Perfection\n",
    "The goal is to have the bounding box as **close to the object's border as possible** to minimize covering background data, creating the \"best bounding box\". The box must perfectly cover the whole content of the object.\n",
    "\n",
    "#### C. Representations (Formats)\n",
    "Any representation must allow derivation of all four coordinates.\n",
    "1.  **Corner Coordinates:** $X_1, Y_1$ (top-left) and $X_2, Y_2$ (bottom-right).\n",
    "    *   Center $X$ is calculated as $(X_1 + X_2)/2$.\n",
    "    *   Center $Y$ is calculated as $(Y_1 + Y_2)/2$.\n",
    "2.  **Center Coordinates & Dimensions (YOLO Format):** Center $X$, Center $Y$, Width $W$, and Height $H$.\n",
    "3.  **Format Conversions:** Multiple formats exist (e.g., Coco format, YOLO format), requiring transformations between them.\n",
    "\n",
    "### IV. Early Detection Models and Limitations\n",
    "\n",
    "#### A. CNN Modification for Single Object Detection\n",
    "This modification is suitable only if the image contains a single object.\n",
    "1.  **Architecture:** The standard CNN is followed by a flatten layer.\n",
    "2.  **Branch 1 (Classification):** Predicts the confidence/objectness score (e.g., 0.9 probability) using a dense layer and **cross-entropy loss**.\n",
    "3.  **Branch 2 (Regression):** A separate fully connected layer is added with **four hidden units** to predict the four coordinates (X, Y, W, H). This uses **Mean Square Error (MSE) loss**.\n",
    "\n",
    "#### B. Sliding Window Technique (SWT)\n",
    "SWT was an attempt to solve the multi-object problem by passing subsets (windows) of the image to the model.\n",
    "1.  **Class Modification:** Requires adding a **background class** to the focused classes (total classes $C+1$), as many windows will not contain a focused object.\n",
    "2.  **Feasibility Issue:** SWT is not feasible because possible combinations are in the millions.\n",
    "    *   Example: A 200x200 image could generate around **400 million boxes**.\n",
    "    *   This requires \"insane amounts of training\" time and GPU resources.\n",
    "3.  **Solution Needed:** Techniques were required to decrease the number of inputs/regions sent to the model for predictions.\n",
    "\n",
    "# V. R-CNN Architecture (Regional CNN)\n",
    "\n",
    "R-CNN (Regional CNN) was developed to reduce the number of inputs drastically ‚Äî from millions of potential regions to around **2,000 proposals per image**.\n",
    "It consists of **three main modules**, each responsible for a different stage of object detection.\n",
    "\n",
    "---\n",
    "\n",
    "## A. Module 1: Selective Search (Region Proposals)\n",
    "\n",
    "1. **Purpose:** Generates *category-independent* region proposals that might contain objects.\n",
    "2. **Reduction:** Reduces the number of boxes from millions to approximately **2,000 proposals per image**, achieving a drastic **99.9% reduction**.\n",
    "3. **Process:**\n",
    "\n",
    "   * The image is segmented into many small regions.\n",
    "   * Similar regions are merged based on various **similarity factors**:\n",
    "\n",
    "     * Color similarity\n",
    "     * Texture similarity\n",
    "     * Size similarity\n",
    "     * Shape similarity\n",
    "     * A linear combination of the above factors\n",
    "   * **Output:** Selective Search produces candidate regions but **does not assign any class labels**.\n",
    "\n",
    "---\n",
    "\n",
    "## B. Module 2: Feature Extraction and Classification\n",
    "\n",
    "R-CNN uses a **pretrained CNN** (e.g., AlexNet or VGG) for feature extraction and **SVMs** for final classification.\n",
    "\n",
    "### 1. Preprocessing and Resizing\n",
    "\n",
    "* **Labeling issue:** The ~2,000 region proposals are initially **unlabeled**.\n",
    "* **Fixed input size:** Since CNNs require fixed-size inputs (e.g., 227√ó227), all proposals are resized using techniques like *dilation* to preserve the aspect ratio.\n",
    "\n",
    "### 2. Labeling Region Proposals using IOU\n",
    "\n",
    "To train the model, each region proposal is assigned a label based on **IOU (Intersection over Union)** with ground-truth boxes.\n",
    "\n",
    "* **IOU > 0.5:** The proposal is labeled as the object‚Äôs class (e.g., Person, Car).\n",
    "* **IOU < 0.3:** Labeled as **background**.\n",
    "* **0.3 ‚â§ IOU ‚â§ 0.5:** Ignored during training.\n",
    "* **Multiple overlaps:** If a proposal overlaps multiple objects, it takes the label of the one with the highest IOU.\n",
    "* **Sampling:** Because most regions are background (~90%), only a subset of background samples are used to balance the dataset.\n",
    "\n",
    "üîπ **Purpose:** This IOU labeling step *does not perform classification* ‚Äî it only creates **training data** (features + labels) for the classifier.\n",
    "\n",
    "### 3. Training with CNN Features and SVMs\n",
    "\n",
    "Once proposals are labeled:\n",
    "\n",
    "1. Each region is passed through the CNN to extract a **4096-dimensional feature vector** (from fully connected layers).\n",
    "2. These features, along with their IOU-based labels, are used to **train a set of Support Vector Machines (SVMs)**.\n",
    "\n",
    "* **Why SVMs?**\n",
    "\n",
    "  * CNNs were pretrained only for *image-level classification* (e.g., ImageNet).\n",
    "  * R-CNN uses SVMs as separate classifiers because:\n",
    "\n",
    "    * They perform well on **high-dimensional feature spaces**.\n",
    "    * They are effective for **binary classification with limited data**.\n",
    "    * Training was easier in separate stages (CNN ‚Üí SVM ‚Üí Regressor) given 2014‚Äôs hardware and software limits.\n",
    "\n",
    "* **Architecture:** One **SVM per object class** (e.g., 20 classes = 20 SVMs).\n",
    "  Each SVM is trained as:\n",
    "  `object class` vs. `background`.\n",
    "\n",
    "* **SVM Training Threshold:**\n",
    "\n",
    "  * IOU > 0.3 ‚Üí Positive sample (object)\n",
    "  * IOU < 0.3 ‚Üí Negative sample (background)\n",
    "\n",
    "üí° **Remember:** IOU labeling creates the *supervised dataset*, while SVMs **learn to classify** those proposals for unseen images.\n",
    "\n",
    "---\n",
    "\n",
    "## C. Module 3: Bounding Box Regressor (Refinement)\n",
    "\n",
    "A separate **bounding box regression model** refines the coordinates predicted by Selective Search.\n",
    "\n",
    "### 1. Goal\n",
    "\n",
    "Convert an approximate region proposal (P) into a more accurate box (G) that tightly encloses the object.\n",
    "\n",
    "### 2. Inputs and Targets\n",
    "\n",
    "* Proposal box: ( P = (x_p, y_p, w_p, h_p) )\n",
    "* Ground truth box: ( G = (x_g, y_g, w_g, h_g) )\n",
    "\n",
    "**Target offset values (regression targets) for training:**\n",
    "$[\n",
    "t_x^* = \\frac{(x_g - x_p)}{w_p}, \\quad\n",
    "t_y^* = \\frac{(y_g - y_p)}{h_p}, \\quad\n",
    "t_w^* = \\log\\left(\\frac{w_g}{w_p}\\right), \\quad\n",
    "t_h^* = \\log\\left(\\frac{h_g}{h_p}\\right)\n",
    "]$\n",
    "\n",
    "These represent the relative corrections the model should predict to align the proposal with the ground truth.\n",
    "\n",
    "### 3. Mechanism\n",
    "\n",
    "* The CNN extracts features from each proposal.\n",
    "* A fully connected layer predicts four values: ( t_x', t_y', t_w', t_h' ).\n",
    "* During inference, these predicted offsets are applied to the proposal box:\n",
    "  $[\n",
    "  x' = t_x' \\cdot w_p + x_p, \\quad\n",
    "  y' = t_y' \\cdot h_p + y_p, \\quad\n",
    "  w' = w_p \\cdot e^{t_w'}, \\quad\n",
    "  h' = h_p \\cdot e^{t_h'}\n",
    "  ]$\n",
    "  This gives the refined bounding box.\n",
    "\n",
    "### 4. Training Condition\n",
    "\n",
    "The regressor is trained only on proposals with **IOU ‚â• 0.6** with ground truth boxes.\n",
    "\n",
    "### 5. Summary Table for Regression\n",
    "\n",
    "|           Symbol           | Meaning                     | Phase                | Formula                 |\n",
    "| :------------------------: | --------------------------- | -------------------- | ----------------------- |\n",
    "| t_x^*, t_y^*, t_w^*, t_h^* | Ground truth target offsets | Training             | From P, G               |\n",
    "|   t_x', t_y', t_w', t_h'   | Predicted offsets           | Training & Inference | From CNN                |\n",
    "|       x', y', w', h'       | Refined box                 | Inference            | Apply predicted offsets |\n",
    "\n",
    "---\n",
    "\n",
    "### üß≠ Summary of R-CNN Pipeline\n",
    "\n",
    "| Stage | Component              | Purpose                               | Output                        |\n",
    "| ----- | ---------------------- | ------------------------------------- | ----------------------------- |\n",
    "| 1     | Selective Search       | Generate ~2,000 region proposals      | Candidate regions (no labels) |\n",
    "| 2     | CNN                    | Extract high-level features           | 4096-D feature vectors        |\n",
    "| 3     | SVM                    | Classify features into object classes | Object labels                 |\n",
    "| 4     | Bounding Box Regressor | Refine box coordinates                | Final bounding boxes          |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Limitations of R-CNN\n",
    "\n",
    "* **Very slow inference:** Each of 2,000 regions is processed by the CNN independently.\n",
    "* **Large storage needs:** All region features are stored on disk before SVM training.\n",
    "* **Multi-stage training:** CNN, SVMs, and Regressor are trained separately (not end-to-end).\n",
    "\n",
    "These issues were later solved by **Fast R-CNN** and **Faster R-CNN**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dcf621-d922-4158-b15e-da23bde0163b",
   "metadata": {},
   "source": [
    "# second lecture \n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Object detection is the task of identifying objects within an image along with their bounding boxes. Unlike image classification (which predicts a single label for an image) or object localization (which predicts a bounding box for a single object), object detection identifies multiple objects and their locations.\n",
    "\n",
    "Key concepts:\n",
    "\n",
    "* **Classification**: Single object prediction in the image.\n",
    "* **Localization**: Predicting bounding box of one object in the image.\n",
    "* **Object Detection**: Multiple objects with bounding boxes.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bounding Boxes\n",
    "\n",
    "Bounding boxes can be represented in multiple ways:\n",
    "\n",
    "1. **Corner Points Representation**: `(x_min, y_min, x_max, y_max)`\n",
    "2. **Center + Width & Height**: `(x_center, y_center, width, height)`\n",
    "\n",
    "**Coordinate Systems**:\n",
    "\n",
    "* Pixel values or normalized coordinates.\n",
    "* Axis-aligned boxes are used for simplicity (no rotation).\n",
    "\n",
    "**Labeling Formats**:\n",
    "\n",
    "* **COCO format**: `[x_center, y_center, width, height]`\n",
    "* **YOLO format**: `[x_center, y_center, width, height]` (normalized)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Object Detection Methods\n",
    "\n",
    "### 3.1 Sliding Window\n",
    "\n",
    "* Slide multiple windows of different aspect ratios across the image.\n",
    "* Each window is classified as **object** or **background** using CNN.\n",
    "* Works well for single-object images.\n",
    "* **Limitations**: Computationally expensive for multiple objects.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 Region-Based CNN (RCNN)\n",
    "\n",
    "RCNN improves efficiency by using region proposals instead of sliding windows over the entire image.\n",
    "\n",
    "**Pipeline**:\n",
    "\n",
    "1. **Region Proposals**:\n",
    "\n",
    "   * Generated using **Selective Search**.\n",
    "   * Divides image into multiple regions based on similarity (color, texture, size).\n",
    "   * Produces candidate bounding boxes likely to contain objects.\n",
    "\n",
    "   ```\n",
    "   Example ASCII:\n",
    "   Original Image\n",
    "   +---------------------+\n",
    "   |       Object 1      |\n",
    "   |   +-------+         |\n",
    "   |   | Box A |         |\n",
    "   |   +-------+         |\n",
    "   |   +-------+         |\n",
    "   |   | Box B |         |\n",
    "   |   +-------+         |\n",
    "   +---------------------+\n",
    "   ```\n",
    "\n",
    "2. **Feature Extraction**:\n",
    "\n",
    "   * Resize region proposals to fixed size.\n",
    "   * Pass through pre-trained CNN (e.g., VGG16) to extract features.\n",
    "\n",
    "3. **Object Classification**:\n",
    "\n",
    "   * Use **class-specific SVMs** to classify proposals into object classes.\n",
    "   * **Negative hard mining** used to improve training.\n",
    "\n",
    "4. **Bounding Box Regression**:\n",
    "\n",
    "   * CNN predicts offsets to refine bounding boxes.\n",
    "   * Learnable parameters adjust box to tightly fit object.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 Non-Maximum Suppression (NMS)\n",
    "\n",
    "Multiple proposals often predict overlapping boxes for the same object. **NMS** filters out redundant boxes.\n",
    "\n",
    "**Steps**:\n",
    "\n",
    "1. Sort predicted boxes by object confidence score (descending).\n",
    "2. Select the highest-scoring box.\n",
    "3. Compute **Intersection over Union (IoU)** with remaining boxes.\n",
    "4. Remove boxes with IoU > threshold (e.g., 0.5).\n",
    "5. Repeat for the next highest box.\n",
    "\n",
    "**Diagram**:\n",
    "\n",
    "```\n",
    "Boxes: [B1, B2, B3, B4]\n",
    "Scores: [0.9, 0.85, 0.34, 0.1]\n",
    "\n",
    "Step 1: Select B1 (score 0.9)\n",
    "Step 2: Remove overlapping boxes B2, B3 if IoU > 0.5\n",
    "Step 3: Select next highest remaining box B4\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Evaluation Metrics\n",
    "\n",
    "### 4.1 Precision & Recall\n",
    "\n",
    "* **Precision**: Fraction of predicted boxes that are correct.\n",
    "  [\\text{Precision} = \\frac{TP}{TP + FP}]\n",
    "* **Recall**: Fraction of ground truth boxes correctly detected.\n",
    "  [\\text{Recall} = \\frac{TP}{TP + FN}]\n",
    "\n",
    "**Example**:\n",
    "\n",
    "* Ground truth boxes: 3\n",
    "* Predicted correct boxes: 2\n",
    "* Precision = 2/3, Recall = 2/3\n",
    "\n",
    "### 4.2 Average Precision (AP) & Mean Average Precision (mAP)\n",
    "\n",
    "* **AP**: Area under the precision-recall curve for one class.\n",
    "* **mAP**: Mean of APs across all classes.\n",
    "* IoU thresholds: Usually evaluated at 0.5 to 0.95 with step 0.05.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Fast RCNN\n",
    "\n",
    "**Limitations of RCNN**:\n",
    "\n",
    "* Multi-stage training (CNN ‚Üí SVM ‚Üí Bounding Box Regression).\n",
    "* Slow at test time (47s per image with VGG16).\n",
    "* High storage requirements for region proposals.\n",
    "\n",
    "**Fast RCNN Improvements**:\n",
    "\n",
    "1. **Single-stage training**: CNN, classification, and bounding box regression in one network.\n",
    "2. **ROI Pooling**: Converts region proposals of varying sizes into fixed-size feature maps.\n",
    "3. **Faster feature extraction**: Pass full image once through CNN, then extract features for all proposals.\n",
    "\n",
    "**Pipeline**:\n",
    "\n",
    "1. Pass full image through CNN ‚Üí get convolutional feature map.\n",
    "2. ROI pooling ‚Üí fixed-size features for each proposal.\n",
    "3. Fully connected layers ‚Üí two branches:\n",
    "\n",
    "   * Classification\n",
    "   * Bounding box regression\n",
    "\n",
    "**Advantages**:\n",
    "\n",
    "* Faster training and inference.\n",
    "* Less storage required.\n",
    "* Single-stage network with end-to-end learning.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Summary of RCNN Variants\n",
    "\n",
    "| Model     | Feature Extraction | Bounding Box | SVM | Training Time | Test Time |\n",
    "| --------- | ------------------ | ------------ | --- | ------------- | --------- |\n",
    "| RCNN      | CNN per proposal   | Yes          | Yes | 2.5 GPU-days  | 47s/img   |\n",
    "| Fast RCNN | CNN once           | Yes          | No  | 9.5 hrs       | 1.8s/img  |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "1. Girshick, R. ‚ÄúRich feature hierarchies for accurate object detection and semantic segmentation.‚Äù CVPR 2014.\n",
    "2. Girshick, R. ‚ÄúFast R-CNN.‚Äù ICCV 2015.\n",
    "3. Ren, S., et al. ‚ÄúFaster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.‚Äù NIPS 2015.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629b0af9-9b35-4d6c-91ae-c303859bca05",
   "metadata": {},
   "source": [
    "# lecture 3\n",
    "\n",
    "## Detailed Lecture Notes: Faster R-CNN: Theory, Data Visualisation, and Evolution\n",
    "\n",
    "### I. Revision of R-CNN Fundamentals\n",
    "\n",
    "#### A. Non-Max Suppression (NMS)\n",
    "NMS is used to **suppress bounding boxes that are not relevant**. The goal is to choose only one accurate bounding box per object when multiple boxes are predicted.\n",
    "\n",
    "1.  **Process:** All bounding boxes are sorted based on their **objectness score** in descending order.\n",
    "2.  The box with the highest score is selected.\n",
    "3.  The **Intersection Over Union (IOU)** is calculated between the highest scoring box and all other bounding boxes.\n",
    "4.  Bounding boxes overlapping significantly (e.g., IOU greater than 0.5) are removed, as they are likely detecting the same object.\n",
    "5.  This process is repeated for the next highest remaining bounding box until all relevant boxes are selected.\n",
    "6.  NMS is applied independently for every different class of objects present in the image.\n",
    "\n",
    "#### B. Bounding Box Evaluation Metrics\n",
    "The primary metric used to evaluate bounding box accuracy is **Mean Average Precision (mAP)**.\n",
    "\n",
    "*   **Precision:** Measures how many of the predicted boxes are accurately correct, relative to all predicted boxes.\n",
    "*   **Recall:** Measures how many of the ground truths are accurately predicted, relative to all ground truths.\n",
    "*   **Average Precision (AP):** Defined as the **area under the precision-recall curve**.\n",
    "*   **mAP:** Calculated by finding the average precision for every class of objects in the dataset.\n",
    "*   **mAP at varying IOU:** Object detection models are often evaluated by calculating mAP across a range of IOU threshold values (e.g., starting at 0.5, stepping up by 0.05, until 0.95), and then calculating the mean of all those average precisions. This is referred to as mAP@[0.5:0.95].\n",
    "\n",
    "### II. Evolution to Fast R-CNN\n",
    "\n",
    "#### A. Motivation\n",
    "The original R-CNN was a complex, multi-stage network that was very slow, taking approximately **48.5 seconds** per image prediction (almost 1 minute). The network had multiple complex components, including CNNs and SVMs.\n",
    "\n",
    "#### B. Fast R-CNN Architecture\n",
    "Fast R-CNN aimed to reduce complexity and time:\n",
    "\n",
    "*   **Change in Order:** Instead of running Selective Search first, the image is passed through the CNN architecture first to generate a **feature map**. Selective Search is then applied to this downsized feature map.\n",
    "*   **ROI Pooling:** This concept was introduced because Selective Search generates proposals of many different shapes, preventing the use of a unified fully connected layer. ROI (Region of Interest) pooling takes these input proposals and transforms them into a **single, fixed shape** using max pooling.\n",
    "*   **Architecture Structure:** CNN $\\rightarrow$ ROI Pooling $\\rightarrow$ Flatten $\\rightarrow$ Fully Connected Layers $\\rightarrow$ Classification/Bounding Box branches.\n",
    "*   **Speed Improvement:** Fast R-CNN drastically reduced the prediction time to **0.3 seconds**.\n",
    "\n",
    "#### C. The Bottleneck\n",
    "Even in Fast R-CNN, the **Selective Search** algorithm remained the primary bottleneck, still taking **1.5 seconds** to extract region proposals. This resulted in a total prediction time of about 1.8 seconds (0.5 FPS), which is **not real-time**.\n",
    "\n",
    "### III. Faster R-CNN Architecture\n",
    "\n",
    "<img src=\"../images/faterrc.png\">\n",
    "\n",
    "#### A. Motivation and Key Improvement\n",
    "Faster R-CNN sought to solve the Selective Search bottleneck to achieve a total detection time of less than 1 second, necessary for real-time applications like video object detection.\n",
    "\n",
    "The core architectural change was the introduction of the **Region Proposal Network (RPN)** to replace Selective Search.\n",
    "\n",
    "#### B. RPN and Shared Architecture\n",
    "\n",
    "<img src=\"../images/rpn.png\">\n",
    "\n",
    "Faster R-CNN utilizes a **shared CNN architecture** (e.g., VGG16, ResNet, MobileNet).\n",
    "\n",
    "*   The image is sent to the CNN once.\n",
    "*   The resulting feature map is branched out to two components: the RPN and the downstream ROI Pooling layer.\n",
    "*   The RPN loss and the classification loss are calculated separately, but the overall loss is used to **update the shared weights** of the backbone CNN architecture.\n",
    "*   **RPN vs. Selective Search:** Selective Search is based on color, texture, and shape similarity and is **not a learnable algorithm**. RPN, in contrast, consists of convolution layers, making it rely on **learnable parameters** that adapt based on the data and objects.\n",
    "\n",
    "#### C. Anchor Boxes\n",
    "Anchor boxes are central to how RPN generates proposals.\n",
    "\n",
    "1.  **Intuition:** The premise is that a single pixel (or cell) in the feature map, derived from passing through multiple convolutions, could be responsible for detecting an object.\n",
    "2.  **Generation:** At every sliding window position (pixel) in the feature map, multiple anchor boxes are generated.\n",
    "    *   This is typically done using **three different scales** and **three different aspect ratios** (e.g., square, horizontal rectangle, vertical rectangle).\n",
    "    *   This yields $K=9$ anchor boxes at each sliding position.\n",
    "3.  **Output:** For a feature map of size $W \\times H$, the total number of anchor boxes generated is $W \\times H \\times K$ (e.g., over 20,000 potential boxes for a 60x40 map).\n",
    "\n",
    "#### D. RPN Output and Training\n",
    "The RPN applies a convolution (e.g., 3x3) across the feature map to generate predictions for these anchor boxes.\n",
    "\n",
    "RPN produces two outputs for the $K$ anchor boxes:\n",
    "1.  **Objectness Scores (2K channels):** This determines the probability that the anchor box contains *any* object (foreground) or is just background. (Although $K+1$ channels might suffice, the original design used 2K channels).\n",
    "2.  **Bounding Box Regression (4K outputs):** Four coordinates (e.g., X, Y, width, height) are predicted for each of the $K$ anchor boxes.\n",
    "\n",
    "To train the RPN, the generated proposals must be labeled:\n",
    "*   **Foreground/Positive Label:** Anchor boxes with an IOU greater than 0.7 with any ground truth box are labeled as a specific object class.\n",
    "*   **Background/Negative Label:** Anchor boxes with an IOU less than 0.3 with *all* ground truth boxes are labeled as background.\n",
    "*   Boxes with IOU between 0.3 and 0.7 are usually discarded.\n",
    "\n",
    "A batch (e.g., 256 proposals, split into 128 positive and 128 negative examples) is created and used to train the ROI pooling layer and subsequent fully connected layers.\n",
    "\n",
    "#### E. Performance Summary (R-CNN Family)\n",
    "| Architecture | Proposal Method | Total Time (Latency) | Note |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| R-CNN | Selective Search | 48.5 seconds | Multi-stage, slowest. |\n",
    "| Fast R-CNN | Selective Search | 1.8 seconds | 25 times faster than R-CNN; bottlenecked by SS. |\n",
    "| Faster R-CNN | RPN (Learnable) | $\\approx$ 200 milliseconds (0.2 seconds) | Almost real-time; RPN replaces SS. |\n",
    "\n",
    "All three R-CNN architectures (R-CNN, Fast R-CNN, Faster R-CNN) are considered **two-stage networks**.\n",
    "\n",
    "### IV. Data Visualization and Preparation\n",
    "\n",
    "#### A. Data Set and Annotation\n",
    "The practical session focused on creating and utilizing data for multi-class object detection, specifically using an aquarium dataset.\n",
    "\n",
    "*   **Tools:** Roboflow was used to download images, perform **manual labeling**, and output data files.\n",
    "*   **Classes:** The example dataset included 7 classes: fish, stingray, jellyfish, penguin, shark, puffin, and starfish.\n",
    "*   **Annotation:** Bounding boxes must be manually drawn and labeled for each object (e.g., turtle, goldfish).\n",
    "\n",
    "#### B. COCO Data Format\n",
    "The session utilized the **COCO JSON format** for annotations. The JSON file is stored as a dictionary containing fields for licenses, categories, images, and annotations.\n",
    "\n",
    "*   **Categories:** Used to map the Category ID (an integer) to the Class Name (e.g., 'fish').\n",
    "*   **Images:** Used to map the Image ID to the actual Image File Name.\n",
    "*   **Annotations:** Contain the bounding box information, linked via Image ID and Category ID.\n",
    "*   **Bounding Box Format:** COCO defines bounding boxes using the coordinates: $x_{min}, y_{min}, \\text{width}, \\text{height}$.\n",
    "\n",
    "#### C. Data Visualization\n",
    "To visualize the data, the JSON annotations are loaded, and the image, category, and annotation information is mapped into dictionaries for fast lookup (Order of 1 time complexity).\n",
    "\n",
    "Custom functions were written to:\n",
    "1.  Access the bounding box coordinates ($x_{min}, y_{min}, \\text{width}, \\text{height}$).\n",
    "2.  Calculate the coordinates needed to draw a rectangle (e.g., $x_{min}$ and $y_{min}$ for the top-left corner, and $x_{min} + \\text{width}$ and $y_{min} + \\text{height}$ for the bottom-right).\n",
    "3.  Draw the bounding box and the corresponding class name on the image.\n",
    "\n",
    "#### D. Tools for Handling Data\n",
    "It was noted that while writing custom code to load and manipulate JSON data from scratch is possible, packages like **pycocotools** exist that allow users to directly load annotation files in the COCO format and skip the manual parsing and mapping.\n",
    "\n",
    "### V. Future Topics\n",
    "The next sessions are planned to cover:\n",
    "*   Training a Faster R-CNN model on the custom dataset.\n",
    "*   Starting the theory and architecture of **YOLO (You Only Look Once)**.\n",
    "*   YOLO is a **single-stage network** that solves the complexity and latency issues of two-stage detection architectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
