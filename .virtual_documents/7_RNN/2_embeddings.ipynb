from keras import layers,Sequential
from keras.layers import Dense,Embedding
import pandas as pd


# creating a demo vocab
df=pd.DataFrame({'Sentence':['I am a good boy','he is a very very bad boy'],'Sentiment':[1,0]})
df


# encoding voabulary to a numerical data
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer()
tokenizer.fit_on_texts(df['Sentence'])
word_index = tokenizer.word_index
word_index


# converting sentences to numericals
sequences=tokenizer.texts_to_sequences(df['Sentence'])
sequences


# pdding them to make of equal length
padded_sequences=pad_sequences(sequences,padding='post')
padded_sequences


vocab_size = len(word_index) + 1  # +1 because index starts at 1
output_dim = 3
input_length = 5

model = Sequential()

# embedding will internally one hot encode our each word so each word will be of shape vocab_size,1 then it multiply this encoded to weight matrix then it get a vector of shape output dim ,1 and it do this for each word in sentence so it will get a vector of shape batch_size,input_length,output_dim and then we flatten it now the shape will become batchsize,output_dim*input_length
model.add(Embedding(input_dim=vocab_size, output_dim=output_dim, input_length=input_length))
model.add(layers.Flatten())
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(padded_sequences,df[['Sentiment']].values,epochs=5,batch_size=1)


model.summary()


embedding_layer = model.layers[0]  # first layer is Embedding
embeddings = embedding_layer.get_weights()[0]  # shape: (vocab_size, output_dim)










# CBOW implementation


sequences


#generating input output pairs
# window_size is the no of words before and after the target as input
# like if window size is one and sequence = i am the king then possible input output pairs are
# X = [[am],        y = i
#      [i, the],    y = am
#      [am, king],  y = the
#      [the]]       y = king

import numpy as np

def generate_pairs(sequences,window_size):
  X=[]
  y=[]
  for i in range(len(sequences)):
    for j in range(len(sequences[i])):
      target=sequences[i][j]
      start=max(0,j-window_size)
      end=min(len(sequences[i]),j+window_size)
      a=[]
      a+=sequences[i][start:j]
      a+=sequences[i][j+1:end+1]
      X.append(a)
      y.append(target)
  X=pad_sequences(X,padding='post')
  y=np.array(y)
  return np.array(X),y


X,y=generate_pairs(sequences,2)


from keras.models import Model
from keras.layers import Embedding,Dense,Input,Lambda
import tensorflow.keras.backend as K

inp=Input(shape=(X.shape[1],))

x=Embedding(input_dim=len(word_index)+1,output_dim=2,input_length=X.shape[1])(inp)
x = Lambda(lambda x: K.mean(x, axis=1))(x)

output=Dense(len(np.unique(y))+1,activation='softmax')(x)


model=Model(inputs=inp,outputs=output)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
model.summary()


model.fit(X,y,epochs=5,batch_size=2)





# like for sentence I am the king and window size = 1
# am → I
# am → the
# the → am
# the → king

def generate_pairs(sequences,window_size):
  X=[]
  y=[]
  for i in range(len(sequences)):
    for j in range(len(sequences[i])):
      target=sequences[i][j]
      start=max(0,j-window_size)
      end=min(len(sequences[i]),j+window_size)
      a=[]
      a+=sequences[i][start:j]
      a+=sequences[i][j+1:end+1]
      for k in a:
        X.append(k)
        y.append(target)
  return np.array(X),np.array(y)


X,y=generate_pairs(sequences,2)


X,y


from keras.models import Model
from keras.layers import Embedding,Dense,Input,Lambda
import tensorflow.keras.backend as K

inp=Input(shape=(1,))

x=Embedding(input_dim=len(word_index)+1,output_dim=2,input_length=1)(inp)
x = Lambda(lambda x: K.mean(x, axis=1))(x)

output=Dense(len(np.unique(y))+1,activation='softmax')(x)


model=Model(inputs=inp,outputs=output)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
model.summary()


!pip install gensim


# building using genitsm
from gensim.models import Word2Vec

# Example corpus
corpus = [
    "I love deep learning".lower().split(),
    "Deep learning is fun".lower().split(),
    "I am learning NLP".lower().split()
]

# corpus = list of list of words
print(corpus)


# CBOW model: sg=0 (skip-gram=1, CBOW=0)
model = Word2Vec(sentences=corpus,
                 vector_size=50,   # embedding dimension
                 window=2,        # context window size
                 min_count=1,     # ignore words with freq < 1
                 sg=0,            # 0 = CBOW, 1 = Skip-Gram
                 workers=4)       # parallel threads



# Vector for a word
vector = model.wv['learning']
print("Vector shape:", vector.shape)
print(vector)



similar_words = model.wv.most_similar('learning', topn=5)
print(similar_words)


# Save
model.save("cbow_model.gensim")

# Load
from gensim.models import Word2Vec
model = Word2Vec.load("cbow_model.gensim")


# GOOGLE news word2vec pretrained model

#This model uses Skip-Gram with negative sampling, not CBOW.


# !wget -c "https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
# no longer available here


from gensim.models import KeyedVectors
# first download model and paste its address in model path
model_path = "GoogleNews-vectors-negative300.bin.gz"
model = KeyedVectors.load_word2vec_format(model_path, binary=True)
