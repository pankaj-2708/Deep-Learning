{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a76d0316-89d8-4f48-b49d-952d3c20dc01",
   "metadata": {},
   "source": [
    "# Vanishing Gradient Problem in ANN | Exploding Gradient Problem\n",
    "lec 20 dl campusx\n",
    "\n",
    "\n",
    "#### 1. Introduction to the Vanishing Gradient Problem\n",
    "\n",
    "*   **Definition**: The Vanishing Gradient Problem is encountered when training **deep Artificial Neural Networks (ANNs)** using gradient-based learning methods like Gradient Descent and Backpropagation.\n",
    "*   **Core Issue**: During the training process, the updates to the network's weights become **very small, effectively stopping the weights from changing**. In the worst case, this can completely halt the neural network's training.\n",
    "*   **Mathematical Origin**: This problem stems from a simple mathematical logic: if you multiply many numbers that are **less than one**, the resulting product will be a very small number, smaller than any of the individual numbers.\n",
    "\n",
    "#### 2. Why the Vanishing Gradient Problem Occurs\n",
    "\n",
    "The problem is particularly prevalent in **deep neural networks** (networks with many hidden layers).\n",
    "\n",
    "*   **Backpropagation and Weight Updates**:\n",
    "    *   In Backpropagation, weights (`w`) are updated using the rule: `w_new = w_old - (learning_rate * ∂L/∂w)`.\n",
    "    *   The term `∂L/∂w` represents the derivative of the loss function with respect to a specific weight, indicating how much a change in that weight affects the loss.\n",
    "*   **The Chain Rule and Small Derivatives**:\n",
    "    *   For deep networks, calculating `∂L/∂w` for weights in earlier layers (closer to the input) involves multiplying many derivative terms together due to the **chain rule**.\n",
    "    *   When activation functions like **Sigmoid** or **tanh** are used, their derivatives are often small.\n",
    "        *   The derivative of the Sigmoid function, for instance, is always between 0 and 0.25, and generally between 0 and 1.\n",
    "    *   When these small derivative values (which are typically less than one) are multiplied together across many layers, the resulting `∂L/∂w` for the early layers becomes an **extremely small (vanishingly small)** number.\n",
    "*   **Impact on Weight Updates**:\n",
    "    *   If `∂L/∂w` is a very small number (e.g., `0.0001`), even with a typical learning rate, the change `(learning_rate * ∂L/∂w)` will be negligible.\n",
    "    *   Consequently, `w_new` will be almost identical to `w_old`, meaning the weights essentially don't change.\n",
    "    *   If weights don't change, the loss will not decrease, and the model will fail to train.\n",
    "*   **Historical Context**: This problem was a major hurdle in the early days of deep learning, hindering the effective training of deeper networks. Sigmoid and tanh functions compress a large input space into a small output space (e.g., Sigmoid maps any input to a 0-1 range), contributing to this issue.\n",
    "\n",
    "#### 3. How to Detect the Vanishing Gradient Problem\n",
    "\n",
    "There are two primary ways to identify if the Vanishing Gradient Problem is occurring:\n",
    "\n",
    "1.  **Monitor the Loss Function**:\n",
    "    *   Observe the loss reported after each epoch during training.\n",
    "    *   If the **loss stops decreasing** or shows negligible changes after an initial period, it's a strong indicator of vanishing gradients.\n",
    "    *   *Demonstration*: A code example shows that after ~100 epochs, the loss gets stuck at a value (e.g., 0.69) and does not reduce further.\n",
    "\n",
    "2.  **Monitor Weight Changes**:\n",
    "    *   **Track the values of your weights** (e.g., `w_11`) across epochs.\n",
    "    *   If weights are not changing, or are changing by an extremely small amount (e.g., `0.000001%`), it confirms vanishing gradients.\n",
    "    *   *Demonstration*: A code example compares initial weights to weights after one epoch of training, showing that the calculated gradient is extremely small, and the percentage change in weights is negligible (e.g., 0.0001%). After 100 epochs, the change in weights is still very small.\n",
    "\n",
    "#### 4. Solutions to the Vanishing Gradient Problem\n",
    "\n",
    "Five common techniques are presented to address the Vanishing Gradient Problem:\n",
    "\n",
    "1.  **Reduce Model Complexity**:\n",
    "    *   **Strategy**: If your neural network has too many hidden layers (i.e., it's too deep), simplifying it by **reducing the number of layers** can help.\n",
    "    *   **Reasoning**: Fewer layers mean fewer derivative terms to multiply, which reduces the chance of the overall gradient becoming vanishingly small.\n",
    "    *   **Trade-off**: While effective, this is often not ideal because deep networks are built to capture complex patterns. Reducing complexity might prevent the model from learning intricate features in the data.\n",
    "    *   *Demonstration*: A code example shows that reducing the number of hidden layers allows the loss to decrease further (e.g., to 0.39) and results in noticeable changes in weights between old and new values, indicating the problem is resolved.\n",
    "\n",
    "2.  **Use Different Activation Functions**:\n",
    "    *   **Strategy**: Replace Sigmoid or tanh with activation functions whose derivatives do not vanish over a wide range.\n",
    "    *   **Example: ReLU (Rectified Linear Unit)**:\n",
    "        *   **Formula**: `max(0, x)`.\n",
    "        *   **Graph**: Zero for negative inputs, `x` for positive inputs.\n",
    "        *   **Derivative**: For positive inputs, the derivative is `1`. For negative inputs, it's `0`.\n",
    "        *   **Benefit**: When the derivative is `1`, multiplying many such terms together does not make the overall gradient smaller, thus avoiding the vanishing gradient issue.\n",
    "        *   **Problem (Dying ReLU)**: If a ReLU neuron's input is always negative, its derivative becomes `0`, and it stops learning.\n",
    "        *   **Improvements**: Variants like Leaky ReLU address the dying ReLU problem.\n",
    "\n",
    "3.  **Proper Weight Initialisation**:\n",
    "    *   **Strategy**: Instead of random weight initialisation, use specific techniques like **Glorot (Xavier) initialisation** or **He initialisation**.\n",
    "    *   **Reasoning**: These methods initialise weights in a way that helps maintain the magnitude of gradients across layers, preventing them from becoming too small or too large.\n",
    "    *   *Note*: This topic will be covered in future videos.\n",
    "\n",
    "4.  **Batch Normalisation**:\n",
    "    *   **Strategy**: Insert Batch Normalisation layers between hidden layers.\n",
    "    *   **Reasoning**: Batch Normalisation normalises the inputs to each layer, stabilising the activations and preventing them from becoming too extreme, which in turn helps maintain healthy gradient flow.\n",
    "    *   *Note*: This is a newer technique and will be discussed in detail in a dedicated video.\n",
    "\n",
    "5.  **Use Residual Networks (ResNets)**:\n",
    "    *   **Strategy**: Employ Residual Networks, which are a special type of network architecture.\n",
    "    *   **Reasoning**: ResNets use \"skip connections\" that allow gradients to bypass one or more layers, flowing directly to earlier layers. This direct path helps prevent gradients from vanishing even in very deep networks.\n",
    "    *   *Note*: This will be covered when discussing Convolutional Neural Networks (CNNs).\n",
    "\n",
    "#### 5. Introduction to the Exploding Gradient Problem\n",
    "\n",
    "*   **Definition**: The Exploding Gradient Problem is the **opposite of the Vanishing Gradient Problem**. It is more commonly observed in **Recurrent Neural Networks (RNNs)**.\n",
    "*   **Principle**: If you multiply numbers that are **greater than one**, the resulting product becomes an extremely large number.\n",
    "*   **Cause**: When derivatives calculated during Backpropagation are frequently **much greater than one**.\n",
    "*   **Impact on Weight Updates**: If `∂L/∂w` is a very large number (e.g., `100`), and the learning rate is `0.1`, then `(learning_rate * ∂L/∂w)` becomes `10`. If an initial weight was `1`, the new weight becomes `1 - 10 = -9`, a drastically different value.\n",
    "    *   This leads to **unstable and random behaviour** of the weights, causing the model to diverge and fail to train.\n",
    "*   **Solution**: A common technique to handle exploding gradients is **Gradient Clipping**, which limits the magnitude of gradients.\n",
    "*   *Note*: This problem will be explored in more detail with RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9426726-3a87-414f-84d0-81f417982521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
