{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c36dc380-ff21-42bb-9789-0da304c6fb59",
   "metadata": {},
   "source": [
    "## Problems with Recurrent Neural Networks (RNNs)\n",
    "\n",
    "### I. Context and Rationale\n",
    "\n",
    "RNNs are a neural network architecture specifically suited for **sequential data**. Sequential data involves data points that depend on preceding data points, such as textual data or time series data.\n",
    "\n",
    "However, RNNs are not as widely used as other architectures because they struggle with specific problems. Understanding these problems explains why advanced architectures like **LSTMs** became necessary.\n",
    "\n",
    "The discussion relies on an understanding of how RNNs are trained using **Backpropagation Through Time (BPTT)**.\n",
    "\n",
    "### II. The Two Major Problems\n",
    "\n",
    "Recurrent Neural Networks fundamentally struggle with two key issues that prevent optimal training:\n",
    "\n",
    "1.  **Problem of Long-Term Dependency**.\n",
    "2.  **Problem of Unstable Training** (Stagnated Training).\n",
    "\n",
    "***\n",
    "\n",
    "## III. Problem 1: Long-Term Dependency (The Vanishing Gradient)\n",
    "\n",
    "### A. The Dependency Issue\n",
    "\n",
    "The long-term dependency problem describes the RNN's inability to retain information from steps that occurred far in the past.\n",
    "\n",
    "*   **Memory Analogy:** The network suffers from a kind of **short-term memory loss**; for instance, it may forget things quickly, similar to the character played by Amir Khan in *Ghajini*.\n",
    "*   **Practical Example:** In applications like predicting the next word in a long sentence (e.g., \"The language spoken in Maharashtra is Marathi... I don't understand [Marathi]\"), an RNN will likely fail because the dependency between the current prediction and the required word is too far in the past. It performs well only in short sentences.\n",
    "\n",
    "### B. Causal Mechanism: Vanishing Gradient Problem\n",
    "\n",
    "The long-term dependency issue arises specifically due to the **Vanishing Gradient Problem**.\n",
    "\n",
    "### C. Mathematical Explanation via BPTT\n",
    "\n",
    "During BPTT, the network calculates the partial derivatives (gradients) of the Loss ($L$) with respect to shared recurrent weights, such as the Input Weight ($W_I$) and the Hidden Weight ($W_H$).\n",
    "\n",
    "1.  **Chaining Dependencies:** Since these weights are used repeatedly across all time steps when the RNN is \"unfolded,\" the total derivative involves summing the contributions from every time step ($T=1$ to $N$).\n",
    "2.  **Long-Term Path:** Calculating the contribution of a weight from an early time step (long-term dependency) requires traversing a computational path that involves the multiplication of many derivatives. For 100 time steps, the dependency term looks like a chain of 100 multiplied derivatives.\n",
    "3.  **The Multiplication Effect:** If the derivative of the activation function used in the hidden unit is calculated (e.g., using $\\tanh$) and the result is a number between zero and one (a small number).\n",
    "4.  **Gradient Vanishing:** When many small numbers (between 0 and 1) are multiplied together over a long sequence, the resulting long-term dependency term becomes **very close to zero**.\n",
    "5.  **Training Impact:** When the total derivative is calculated, the contributions from the long-term dependencies vanish, leaving only the **short-term dependencies** (recent inputs) to dominate the calculation. This means the weights are updated primarily based on recent inputs, and the RNN cannot learn long-range connections.\n",
    "\n",
    "### D. Possible Solutions to Vanishing Gradients\n",
    "\n",
    "Several techniques can be used to potentially reduce this problem:\n",
    "\n",
    "*   **Activation Functions:** Use different activation functions, such as **ReLU**, whose derivative does not necessarily fall within the zero-to-one range.\n",
    "*   **Weight Initialization:** Implement better weight initialization techniques. For example, initializing the recurrent weight matrix as an **Identity Matrix** ensures that the information is not corrupted when passed forward.\n",
    "*   **Architectural Changes:** Use slightly different architectures, such as **Skip Connections**.\n",
    "\n",
    "***\n",
    "\n",
    "## IV. Problem 2: Unstable Training (The Exploding Gradient)\n",
    "\n",
    "### A. The Training Issue\n",
    "\n",
    "Unstable training occurs when the RNN cannot be trained properly; the loss function does not improve, and the network fails to produce accurate results. This issue can cause the training process to completely halt.\n",
    "\n",
    "### B. Causal Mechanism: Exploding Gradient Problem\n",
    "\n",
    "This problem is caused by the **Exploding Gradient Problem**.\n",
    "\n",
    "### C. Mathematical Explanation\n",
    "\n",
    "The Exploding Gradient Problem is the opposite scenario to Vanishing Gradients:\n",
    "\n",
    "1.  **Derivatives Become Too Large:** If the derivative of the activation function (especially if ReLU is used, as it can yield any positive number) is large, and the recurrent weights are also large.\n",
    "2.  **Rapid Multiplication:** When these large values are multiplied repeatedly down the long computational chain (as seen in BPTT), the resulting gradients become **very large**â€”potentially approaching infinity.\n",
    "3.  **Training Impact:** An extremely large gradient causes the weight update step (via gradient descent) to dramatically overshoot the minimum, making the new weights ($W_{in}$ and $W_H$) infinite or excessively large. This prevents the model from training correctly.\n",
    "4.  **Other Causes:** A **high learning rate** can also contribute to the Exploding Gradient Problem.\n",
    "\n",
    "### D. Solution to Exploding Gradients\n",
    "\n",
    "*   **Gradient Clipping:** The primary solution is **Gradient Clipping**. This technique caps the maximum allowed value of the gradients. If a calculated gradient exceeds this predefined maximum, it is clipped back down, thereby preventing the weights from becoming excessively large or infinite.\n",
    "\n",
    "***\n",
    "\n",
    "## V. Conclusion\n",
    "\n",
    "Since RNNs struggle with both long-term dependencies (Vanishing Gradients) and unstable training (Exploding Gradients), researchers moved towards more sophisticated architectures like **LSTMs**. LSTMs are designed specifically to solve these stability and long-term memory issues and are widely used in the real world."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl_py_9)",
   "language": "python",
   "name": "dl_py_9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
