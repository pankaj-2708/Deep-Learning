{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c1eb35-828a-468b-bf2e-d604d94bf00b",
   "metadata": {},
   "source": [
    "# LSTM: Long Short-Term Memory (Part 1: The What?)\n",
    "\n",
    "## I. Introduction and Importance\n",
    "\n",
    "*   This video continues the Deep Learning playlist, resuming after a pause.\n",
    "*   The last topic covered was **Recurrent Neural Networks (RNN)**.\n",
    "*   **Long Short-Term Memory (LSTM)** is a highly important topic, heavily used in the industry.\n",
    "*   LSTMs are a foundational step for understanding modern advancements like **ChatGPT, LLMs, Transformers, and Attention mechanisms**.\n",
    "*   **Note on Difficulty:** LSTMs are generally labelled as a difficult topic. This is because it is often challenging to understand not only *how* LSTMs work but also *why* they function the way they do.\n",
    "*   **Roadmap for LSTM Series:**\n",
    "    1.  **What?** (Core idea and concept).\n",
    "    2.  **How?** (Architecture and underlying mathematics).\n",
    "    3.  **Why?** (Understanding the mechanism's purpose).\n",
    "    4.  Practical coding project.\n",
    "\n",
    "## II. Recap: The Need for LSTMs\n",
    "\n",
    "### A. Artificial Neural Networks (ANNs)\n",
    "\n",
    "*   ANNs have a simple architecture consisting of input layers, hidden layers, and an output layer, all fully connected.\n",
    "*   **Limitation:** ANNs cannot handle **sequential data** (like text or time series) where chronology matters and past data points impact future points.\n",
    "*   ANNs process an entire sequence (e.g., a sentence converted into vectors/numbers) simultaneously, meaning the order of words loses its significance.\n",
    "\n",
    "### B. Recurrent Neural Networks (RNNs)\n",
    "\n",
    "*   RNNs were developed to solve the sequential data problem by introducing the concept of **State**.\n",
    "*   **RNN Architecture:** Uses a similar structure to ANNs but incorporates a state concept where each hidden unit provides feedback to itself and to others in the next time step.\n",
    "*   Data is processed one word/token at a time.\n",
    "*   The output of one time step ($H_t$) acts as an input for the next time step.\n",
    "\n",
    "## III. The Problem with RNNs and the Origin of LSTMs\n",
    "\n",
    "*   The main issue arises when the sequence (or chain) becomes **very long** (e.g., 50 words).\n",
    "*   When a decision needs to be made late in the sequence (e.g., filling in a blank at the end of a long paragraph), the RNN often **forgets** the initial inputs upon which the decision depends.\n",
    "*   This forgetting is due to a mathematical issue called the **Vanishing/Exploding Gradient Problem**.\n",
    "*   **Origin of LSTMs:** LSTMs were created specifically because RNNs were **not able to handle long sequences** due to the Vanishing/Exploding Gradient Problem.\n",
    "\n",
    "## IV. Core Idea of LSTM: Separating Memory\n",
    "\n",
    "*   In RNNs, the system has only **one path (line)** available to retain information about past data.\n",
    "*   This single line is tasked with maintaining both **Short-Term Context** (recent inputs) and **Long-Term Context** (information from the distant past).\n",
    "*   Mathematically, this combined responsibility fails, and the Short-Term Context typically **dominates**, causing the system to forget the important old information.\n",
    "*   **LSTM Solution:** Introduce a second, separate path to handle long-term context.\n",
    "    *   **Lower Path:** Maintains the **Short-Term Memory** (the hidden state, $H_t$).\n",
    "    *   **Upper Path:** Maintains the **Long-Term Memory** (the **Cell State**, $C_t$).\n",
    "*   If a piece of information is determined to be important in an early step, it is placed on the Long-Term Memory chain and will carry forward until the end of the sequence, unless it is explicitly removed.\n",
    "\n",
    "### Analogy: The Human Brain\n",
    "\n",
    "*   When processing sequential data (like a story), the mind processes it word-by-word (Short-Term Context).\n",
    "*   The mind constantly works to build **Long-Term Context** by deciding which currently processed events should be deemed important enough to be retained over time (e.g., the primary hero, the time period, the location).\n",
    "*   Simultaneously, the mind removes previously stored long-term context that is no longer relevant (e.g., if a hero dies, they are removed from the important list).\n",
    "\n",
    "## V. LSTM Architecture and Communication\n",
    "\n",
    "*   The LSTM architecture is designed to manage the communication between the Short-Term Memory ($H_t$) and the Long-Term Memory (Cell State, $C_t$).\n",
    "*   If the Short-Term Memory detects new important information, it communicates this to the Long-Term Memory for addition.\n",
    "*   If the Short-Term Memory decides something needs to be removed from the Long-Term Memory, it communicates this request as well.\n",
    "*   This necessary communication makes the LSTM architecture more **complex/difficult** than the simple RNN architecture.\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20250404172141987003/gate_of_lstm.webp\">\n",
    "\n",
    "### The Three Gates\n",
    "\n",
    "The complex mechanism within the LSTM cell handles this communication using three primary components, called **Gates**:\n",
    "\n",
    "1.  **Forget Gate**\n",
    "2.  **Input Gate**\n",
    "3.  **Output Gate**\n",
    "\n",
    "#### Gate Functionality (One-Line Description)\n",
    "\n",
    "| Gate | Function |\n",
    "| :--- | :--- |\n",
    "| **Forget Gate** | Decides, based on the current input and short-term context, what information to **remove** (or forget) from the Long-Term Memory. |\n",
    "| **Input Gate** | Decides, based on the current input, what **new information** should be **added** to the Long-Term Memory. |\n",
    "| **Output Gate** | Produces the final output based on the current input and the contents of the Long-Term Memory. It is also responsible for creating the Short-Term Memory state for the next step. |\n",
    "\n",
    "## VI. Summary: LSTM as a Computer\n",
    "\n",
    "LSTMs can be viewed functionally like a computer (Input $\\rightarrow$ Processing $\\rightarrow$ Output).\n",
    "\n",
    "### Inputs (at Time $t$)\n",
    "\n",
    "The system receives three pieces of input:\n",
    "\n",
    "1.  **$C_{t-1}$:** The Long-Term Memory (Cell State) from the previous time step.\n",
    "2.  **$H_{t-1}$:** The Short-Term Memory from the previous time step.\n",
    "3.  **Current Input Word** ($X_t$).\n",
    "\n",
    "### Outputs (at Time $t$)\n",
    "\n",
    "The system provides two outputs:\n",
    "\n",
    "1.  **$C_t$:** The Long-Term Memory (Cell State) for the next state.\n",
    "2.  **$H_t$:** The Short-Term Memory (Hidden State) for the next state.\n",
    "\n",
    "### Internal Processing (Two Main Steps)\n",
    "\n",
    "The processing inside the cell involves two crucial tasks:\n",
    "\n",
    "1.  **Update the Long-Term Memory** (removing old information and adding new information).\n",
    "2.  **Create the Short-Term Memory** for the next state."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl_py_9)",
   "language": "python",
   "name": "dl_py_9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
