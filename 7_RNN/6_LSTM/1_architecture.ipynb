{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa74beb3-6384-4069-a2d6-b6e4312f93e3",
   "metadata": {},
   "source": [
    "# LSTM Architecture and Mechanism\n",
    "\n",
    "## Core Concepts and Architecture Overview\n",
    "\n",
    "The complexity of the LSTM architecture, when compared to a standard RNN architecture, stems from its ability to maintain and manage **two types of context**:\n",
    "\n",
    "1.  **Long-Term Context/Memory:** Called the **Cell State** ($C_t$).\n",
    "2.  **Short-Term Context/Memory:** Called the **Hidden State** ($H_t$).\n",
    "\n",
    "The complex design primarily serves to set up communication and interaction between the Long-Term and Short-Term memory components.\n",
    "\n",
    "### Inputs and Outputs\n",
    "\n",
    "For a particular timestamp (e.g., $t_1$), the LSTM cell takes **three inputs**:\n",
    "\n",
    "1.  **Previous Cell State** ($C_{t-1}$).\n",
    "2.  **Previous Hidden State** ($H_{t-1}$).\n",
    "3.  **Input for the current time stamp** ($X_t$).\n",
    "\n",
    "The LSTM cell produces **two outputs**:\n",
    "\n",
    "1.  **Current Cell State** ($C_t$).\n",
    "2.  **Current Hidden State** ($H_t$).\n",
    "\n",
    "### Processing Goals\n",
    "\n",
    "The processing part of the LSTM cell has two main jobs:\n",
    "\n",
    "1.  **Updating the Cell State** ($C_{t-1}$ to $C_t$). This update involves deciding what existing information to **remove** (if unnecessary) and what **new information to add** (if necessary), both based on the current input.\n",
    "2.  **Calculating the Hidden State** ($H_t$), which is then passed to the next timestamp.\n",
    "\n",
    "## Mathematical Components and Operations\n",
    "\n",
    "### Vectors and Dimensions\n",
    "\n",
    "The core states and inputs within the LSTM are represented mathematically as **vectors** (collections of numbers).\n",
    "\n",
    "*   **Cell State ($C_t$) and Hidden State ($H_t$):** Both are vectors. The dimension (shape) of the input vectors and output vectors must be exactly the same.\n",
    "*   **Input ($X_t$):** The input is also a vector, representing the word converted into numbers (e.g., via text vectorisation).\n",
    "*   **Gate Outputs ($F_t, I_t, \\tilde{C}_t, O_t$):** The outputs of the various gates (Forget, Input, Candidate Cell State, Output) are also vectors. All six vectors ($C_t, H_t, F_t, I_t, \\tilde{C}_t, O_t$) have the exact same shape or number of dimensions.\n",
    "\n",
    "### Mathematical Operations (Red and Yellow Units)\n",
    "\n",
    "The red units in the architecture represent point-wise mathematical operations.\n",
    "\n",
    "*   **Point-wise Multiplication ($\\odot$):** Multiplies corresponding elements of two vectors of the same shape, resulting in a new vector.\n",
    "*   **Point-wise Addition ($+$):** Adds corresponding elements of two vectors of the same shape.\n",
    "*   **Activation Functions (Yellow Units):** These units represent Neural Network layers containing activation functions (Sigmoid $\\sigma$ or Tanh $\\tanh$).\n",
    "    *   **Tanh ($\\tanh$):** Squashes values between -1 and 1.\n",
    "    *   **Sigmoid ($\\sigma$):** Squashes values between 0 and 1.\n",
    "\n",
    "The number of nodes or units in these Neural Network layers is flexible but must be consistently maintained throughout the cell, typically matching the dimension of the state vectors.\n",
    "\n",
    "## The Three Gates\n",
    "\n",
    "The core function of the LSTM cell is governed by three primary gates: the Forget Gate, the Input Gate, and the Output Gate.\n",
    "\n",
    "### 1. The Forget Gate ($F_t$)\n",
    "\n",
    "<img src=\"https://i.ibb.co/mCqFrf5Z/image.png\">\n",
    "\n",
    "The Forget Gate determines **what to remove** from the previous Cell State ($C_{t-1}$).\n",
    "\n",
    "*   **Inputs:** Current Input ($X_t$) and Previous Hidden State ($H_{t-1}$).\n",
    "*   **Mechanism:** This gate uses a Neural Network layer with a **Sigmoid** activation function. The output, $F_t$, is a vector with values between 0 and 1.\n",
    "*   **Equation (Calculation of $F_t$):**\n",
    "    $$F_t = \\sigma(W_f \\cdot [H_{t-1}, X_t] + B_f)$$\n",
    "    (Where $W_f$ is the weight matrix, $[H_{t-1}, X_t]$ is the concatenation of the vectors, and $B_f$ is the bias).\n",
    "*   **Application (Forgetting):** $F_t$ is then used in a **point-wise multiplication** with the Previous Cell State ($C_{t-1}$).\n",
    "    $$F_t \\odot C_{t-1}$$\n",
    "*   **Significance:** The term \"gate\" is used because $F_t$ has the power to decide how much information from $C_{t-1}$ flows forward.\n",
    "    *   If $F_t$ produces 0 for a specific dimension, 100% of that information is removed (forgotten).\n",
    "    *   If $F_t$ produces 1, 100% of that information passes through.\n",
    "\n",
    "### 2. The Input Gate ($I_t$ and $\\tilde{C}_t$)\n",
    "\n",
    "<img src=\"https://i.ibb.co/d0nCL2JX/image.png\">\n",
    "\n",
    "The Input Gate decides **what new important information to add** to the Cell State. This process involves two main stages.\n",
    "\n",
    "**Stage 1: Candidate Cell State ($\\tilde{C}_t$)**\n",
    "\n",
    "*   This stage calculates new candidate values that might be worthy of adding to the Cell State.\n",
    "*   **Mechanism:** It uses a Neural Network layer with a **Tanh** activation function.\n",
    "*   **Equation:**\n",
    "    $$\\tilde{C}_t = \\tanh(W_c \\cdot [H_{t-1}, X_t] + B_c)$$\n",
    "    ($\\tilde{C}_t$ is the Candidate Cell State).\n",
    "\n",
    "**Stage 2: Input Filter ($I_t$)**\n",
    "\n",
    "*   This stage determines which parts of the Candidate Cell State ($\\tilde{C}_t$) are actually important enough to be added.\n",
    "*   **Mechanism:** It uses a Neural Network layer with a **Sigmoid** activation function.\n",
    "*   **Equation:**\n",
    "    $$I_t = \\sigma(W_i \\cdot [H_{t-1}, X_t] + B_i)$$\n",
    "    ($I_t$ is the Input Filter).\n",
    "*   **Application (Filtering):** The filtered candidate information is calculated via **point-wise multiplication**:\n",
    "    $$I_t \\odot \\tilde{C}_t$$\n",
    "\n",
    "### 3. Cell State Update\n",
    "\n",
    "\n",
    "The results from the Forget Gate and the Input Gate are combined using **point-wise addition** to generate the new Cell State ($C_t$).\n",
    "\n",
    "*   **Final Cell State Equation:**\n",
    "    $$C_t = (F_t \\odot C_{t-1}) + (I_t \\odot \\tilde{C}_t)$$\n",
    "\n",
    "This structure is crucial because it allows information to potentially pass through the Cell State path relatively untouched (if $F_t=1$ and $I_t \\odot \\tilde{C}_t$ is near zero), which helps to prevent the **vanishing gradient problem** often encountered in standard RNNs, ensuring long-term memory is not lost.\n",
    "\n",
    "### 4. The Output Gate ($O_t$)\n",
    "\n",
    "<img src=\"https://i.ibb.co/pr0ZGWQf/image.png\">\n",
    "\n",
    "The Output Gate determines the **value of the Current Hidden State** ($H_t$) for the current time step. It filters the newly updated Cell State ($C_t$).\n",
    "\n",
    "**Stage 1: Tuning the Cell State**\n",
    "\n",
    "*   The updated Cell State ($C_t$) is passed through a **Tanh** function.\n",
    "\n",
    "**Stage 2: Output Filter ($O_t$)**\n",
    "\n",
    "*   This filter decides which parts of the (tuned) Cell State should be output as the Hidden State.\n",
    "*   **Mechanism:** It uses a Neural Network layer with a **Sigmoid** activation function.\n",
    "*   **Equation:**\n",
    "    $$O_t = \\sigma(W_o \\cdot [H_{t-1}, X_t] + B_o)$$\n",
    "\n",
    "**Stage 3: Hidden State Calculation**\n",
    "\n",
    "*   The final Hidden State ($H_t$) is calculated by **point-wise multiplication** of the Output Filter ($O_t$) and the tuned Cell State ($\\tanh(C_t)$).\n",
    "*   **Final Hidden State Equation:**\n",
    "    $$H_t = O_t \\odot \\tanh(C_t)$$\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/1*goJVQs-p9kgLODFNyhl9zA.gif\">\n",
    "\n",
    "<video><source src=\"https://packaged-media.redd.it/afzlbpt2ncg81/pb/m2-res_1080p.mp4?m=DASHPlaylist.mpd&v=1&e=1760396400&s=7b04600131aa454e5c0b5854952a5df6ff271ee7\" type=\"video/mp4\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cfc31c-9b93-4f82-b44a-49a06bd47aaa",
   "metadata": {},
   "source": [
    "# Understanding LSTM Outputs and How to Combine Them (Keras)\n",
    "\n",
    "## üß† 1Ô∏è‚É£ Available LSTM Output Options in Keras\n",
    "\n",
    "An LSTM layer can output: - Full sequence of hidden states\n",
    "(`return_sequences=True`) - Final hidden & cell states\n",
    "(`return_state=True`) - Or both at once.\n",
    "\n",
    "### üîπ OPTION A --- Default (single output)\n",
    "\n",
    "``` python\n",
    "LSTM(units)\n",
    "```\n",
    "\n",
    "**Returns:** only the final hidden state ‚Üí `h_t` (shape:\n",
    "`(batch, units)`)\n",
    "\n",
    "‚úÖ Used for: - Classification tasks - Sequence summary (e.g., sentiment\n",
    "analysis)\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### üîπ OPTION B --- Return sequence of outputs\n",
    "\n",
    "``` python\n",
    "LSTM(units, return_sequences=True)\n",
    "```\n",
    "\n",
    "**Returns:** sequence of all hidden states ‚Üí `(batch, timesteps, units)`\n",
    "\n",
    "‚úÖ Used for: - Many-to-many models (e.g., sequence tagging,\n",
    "translation) - Feeding into another LSTM (stacked LSTMs)\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### üîπ OPTION C --- Return final states\n",
    "\n",
    "``` python\n",
    "LSTM(units, return_state=True)\n",
    "```\n",
    "\n",
    "**Returns:**\\\n",
    "- `output` ‚Üí final hidden state (`h_t`)\\\n",
    "- `h` ‚Üí hidden state\\\n",
    "- `c` ‚Üí cell state\n",
    "\n",
    "‚úÖ Used for: - Encoder--Decoder models (Seq2Seq) - Combining long-term &\n",
    "short-term memory manually\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### üîπ OPTION D --- Both sequences and states\n",
    "\n",
    "``` python\n",
    "LSTM(units, return_sequences=True, return_state=True)\n",
    "```\n",
    "\n",
    "**Returns:**\\\n",
    "- `outputs` ‚Üí all hidden states\\\n",
    "- `h` ‚Üí final hidden state\\\n",
    "- `c` ‚Üí final cell state\n",
    "\n",
    "‚úÖ Used for: - Custom RNN architectures - Attention mechanisms where\n",
    "both full sequence and final state are needed\n",
    "\n",
    "------------------------------------------------------------------------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl_py_9)",
   "language": "python",
   "name": "dl_py_9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
