{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f8b6f52-8544-47d8-a44a-ea575a9dd1e5",
   "metadata": {},
   "source": [
    "## I. Introduction to Backpropagation Through Time (BPTT)\n",
    "\n",
    "*   **Learning in Neural Networks:** In standard neural networks, learning—the adjustment of weights—occurs through the process of **Backpropagation**.\n",
    "*   **RNN Specificity:** In Recurrent Neural Networks (RNNs), this concept is extended and referred to as **Backpropagation Through Time**.\n",
    "*   **Context:** These concepts are studied as part of continuing deep learning efforts focused on understanding how RNNs handle sequential data.\n",
    "\n",
    "***\n",
    "\n",
    "## II. Data Setup and RNN Architecture\n",
    "\n",
    "### A. Example Scenario: Sentiment Analysis\n",
    "\n",
    "*   We use a practical example of **Sentiment Analysis**, where the input is text, and the output determines the sentiment (e.g., 1 for positive, 0 for negative).\n",
    "*   **Toy Dataset:** The input data consists of short reviews, each composed of three words.\n",
    "    *   Example reviews include: \"Cat Cat Mat,\" \"Rat Rat Mat\".\n",
    "    *   Each review has an associated sentiment label (e.g., Positive or Negative).\n",
    "\n",
    "### B. Data Representation\n",
    "\n",
    "*   **Vector Conversion:** To process text, unique words in the vocabulary must be converted into numerical vectors.\n",
    "*   If the vocabulary contains three unique words (Cat, Mat, Rat), each word is represented by a three-dimensional vector (e.g., Cat =, Mat =, Rat =).\n",
    "*   The input to the RNN is therefore 3-dimensional.\n",
    "\n",
    "### C. The RNN Cell and Weights\n",
    "\n",
    "*   The architecture consists of units (or cells).\n",
    "*   Since the example is a binary classification problem, the output layer has a single node, which produces the prediction ($\\hat{Y}$).\n",
    "*   **Weights (Parameters):** The model contains shared weights across all time steps:\n",
    "    *   $W_I$ (Input Weight).\n",
    "    *   $W_H$ (Hidden Weight).\n",
    "    *   $W_{Output}$ (Output Weight).\n",
    "    *   *Note:* Biases are generally present but are often ignored temporarily in the discussion for simplicity.\n",
    "\n",
    "***\n",
    "\n",
    "## III. Forward Propagation (Unfolding in Time)\n",
    "\n",
    "To understand backpropagation, one must first trace the **forward propagation** path.\n",
    "\n",
    "*   **Sequential Processing:** Reviews are inserted into the RNN **word by word**.\n",
    "*   **Unfolding:** For a review with three words, the Recurrent Neural Network is **unfolded in time** into three steps.\n",
    "\n",
    "1.  **Step 1 (First Word, $X_{11}$):** The first word is sent into the system. It uses an initial input (often zeros, $O_0$). This calculates the first hidden state, $O_1$.\n",
    "    *   *Equation:* $O_1 = f(X_{11} W_I + O_0 W_H)$, where $f$ is an activation function.\n",
    "\n",
    "2.  **Step 2 (Second Word, $X_{12}$):** The second word is sent in. It uses $O_1$ as its recurrent input. This calculates $O_2$.\n",
    "    *   *Equation:* $O_2 = f(X_{12} W_I + O_1 W_H)$.\n",
    "\n",
    "3.  **Step 3 (Third Word, $X_{13}$):** The third word is sent in. It uses $O_2$ as its recurrent input. This calculates $O_3$.\n",
    "    *   *Equation:* $O_3 = f(X_{13} W_I + O_2 W_H)$.\n",
    "\n",
    "4.  **Final Output ($\\hat{Y}$):** Since there are no more words, $O_3$ is used to calculate the final prediction ($\\hat{Y}$).\n",
    "    *   *Equation:* $\\hat{Y} = g(O_3 W_{Output})$, where $g$ is typically a Sigmoid activation function for binary classification.\n",
    "\n",
    "### Loss Calculation\n",
    "\n",
    "*   The calculated prediction ($\\hat{Y}$) is compared with the true label ($Y$) to determine the **Loss ($L$)**.\n",
    "*   For binary classification, the loss is typically calculated using binary cross-entropy:\n",
    "    *   $L = -Y \\log(\\hat{Y}) - (1 - Y) \\log(1 - \\hat{Y})$.\n",
    "\n",
    "***\n",
    "\n",
    "## IV. Backpropagation and Gradient Descent\n",
    "\n",
    "The primary goal of backpropagation is to **minimise the loss** by finding optimal values for the weights ($W_I$, $W_H$, $W_{Output}$). This involves applying the gradient descent update step.\n",
    "\n",
    "### A. The Gradient Descent Update\n",
    "\n",
    "Weights are updated iteratively using the learning rate and the calculated partial derivatives (gradients):\n",
    "$$\n",
    "W_{new} = W_{old} - \\text{Learning Rate} \\times \\frac{\\partial L}{\\partial W}\n",
    "$$\n",
    "The core task is calculating the three required partial derivatives.\n",
    "\n",
    "### B. Calculating $\\frac{\\partial L}{\\partial W_{Output}}$\n",
    "\n",
    "*   This derivative is the easiest to calculate because $W_{Output}$ is near the end of the forward propagation path.\n",
    "*   The relationship is: $L \\rightarrow \\hat{Y} \\rightarrow W_{Output}$.\n",
    "*   Using the Chain Rule:\n",
    "    $$\n",
    "    \\frac{\\partial L}{\\partial W_{Output}} = \\frac{\\partial L}{\\partial \\hat{Y}} \\times \\frac{\\partial \\hat{Y}}{\\partial W_{Output}}\n",
    "    $$\n",
    "*   This involves simple differentiation of the Loss equation and the final output activation function.\n",
    "\n",
    "### C. Calculating Derivatives for Recurrent Weights ($\\frac{\\partial L}{\\partial W_I}$ and $\\frac{\\partial L}{\\partial W_H}$)\n",
    "\n",
    "Calculating the derivatives for the Input ($W_I$) and Hidden ($W_H$) weights is significantly more complex because **these weights are reused across every time step**.\n",
    "\n",
    "#### 1. The Challenge of Unfolding\n",
    "\n",
    "*   Because the RNN is **unfolded in time**, the relationship between the final Loss ($L$) and a recurrent weight ($W_I$ or $W_H$) involves **multiple separate computational paths**.\n",
    "*   For a 3-word review, there are three distinct paths from $W_I$ to $L$ (one path for each time step where $W_I$ was used: $T=1, T=2, T=3$).\n",
    "*   If the review had 10 words, there would be 10 separate paths, making manual calculation difficult.\n",
    "\n",
    "#### 2. Calculating $\\frac{\\partial L}{\\partial W_I}$ (Input Weight)\n",
    "\n",
    "Since $W_I$ contributes to the loss at every time step, the final derivative is the **sum of the derivatives across all time steps** ($k$).\n",
    "\n",
    "*   The relationship path is $L \\rightarrow \\hat{Y} \\rightarrow O_k \\rightarrow W_I$.\n",
    "*   The derivative must be calculated by summing the contributions from each time step ($k=1$ to $N$, where $N$ is the total time steps):\n",
    "    $$\n",
    "    \\frac{\\partial L}{\\partial W_I} = \\sum_{k=1}^N \\left( \\frac{\\partial L}{\\partial \\hat{Y}} \\times \\frac{\\partial \\hat{Y}}{\\partial O_k} \\times \\frac{\\partial O_k}{\\partial W_I} \\right)\n",
    "    $$\n",
    "\n",
    "#### 3. Calculating $\\frac{\\partial L}{\\partial W_H}$ (Hidden Weight)\n",
    "\n",
    "*   The calculation for $W_H$ is similarly complex.\n",
    "*   $W_H$ is involved in the hidden state calculations ($O_k$) at every step.\n",
    "*   The derivative must also be calculated by summing the contributions across all time steps, following the various paths (e.g., $L \\rightarrow O_3$ directly via $W_H$, and indirectly via $O_2$ where $W_H$ was also used, etc.).\n",
    "\n",
    "### D. Iterative Learning\n",
    "\n",
    "Once all three derivatives ($\\frac{\\partial L}{\\partial W_{Output}}$, $\\frac{\\partial L}{\\partial W_I}$, and $\\frac{\\partial L}{\\partial W_H}$) are successfully calculated:\n",
    "\n",
    "1.  The gradient descent update step is applied, yielding new values for $W_I$, $W_H$, and $W_{Output}$.\n",
    "2.  These new values are then used when processing the next piece of data (the second review, for example).\n",
    "3.  This process of forward propagation, loss calculation, and backpropagation continues until the minimum loss value is achieved for the entire dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl_py_9)",
   "language": "python",
   "name": "dl_py_9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
