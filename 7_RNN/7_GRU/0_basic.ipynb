{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c3f07da-ccf0-4721-a510-5aedfca38091",
   "metadata": {},
   "source": [
    "# Gated Recurrent Unit (GRU): Lecture Notes\n",
    "\n",
    "## I. Introduction and Context\n",
    "\n",
    "### A. What are GRUs?\n",
    "*   Gated Recurrent Units (GRUs) are a type of **Recurrent Neural Network (RNN) architecture**.\n",
    "*   They are primarily used to **process sequential data**.\n",
    "*   GRUs are a relatively recent development, having been introduced in **2014**, compared to LSTMs (Long Short-Term Memory) which came out around 1997.\n",
    "\n",
    "### B. Why GRUs Exist (Addressing Limitations of Previous RNNs)\n",
    "*   **Simple RNNs:** Suffer from the **vanishing and exploding gradient problems**, meaning they cannot retain long-term context when the sequence (e.g., a sentence) becomes very long.\n",
    "*   **LSTMs:** Successfully solved the vanishing/exploding gradient problem by maintaining two different contexts: **Long-Term Memory (Cell State)** and **Short-Term Memory (Hidden State)**, utilizing a very complex architecture involving **three gates** (Forget, Input, and Output).\n",
    "*   **The Flaw of LSTMs:** While effective, LSTMs have a **complex architecture** and a **high number of parameters**. A high parameter count means that training takes longer, especially when working with large datasets.\n",
    "*   **The GRU Solution:** GRUs solve this training time issue by offering a **simpler architecture** and possessing **fewer parameters** compared to LSTMs. This means training time is reduced.\n",
    "*   **Performance:** Despite their simpler structure and fewer gates, GRU performance is **comparable** to LSTMs. In fact, there are certain datasets and problems where GRUs have been empirically proven to **outperform LSTMs**.\n",
    "\n",
    "## II. GRU Architecture: Components and Setup\n",
    "\n",
    "<img src=\"https://miro.medium.com/0*mXUQSGG-WCt2UE-Y\">\n",
    "\n",
    "### A. The Goal of the GRU Cell\n",
    "*   For any time stamp $t$, the GRU aims to calculate the **Current Hidden State ($H_t$)**.\n",
    "*   The GRU receives two inputs at each time step:\n",
    "    1.  The **Previous Hidden State ($H_{t-1}$)**.\n",
    "    2.  The **Current Time Stamp Input ($X_t$)**.\n",
    "\n",
    "### B. Key Components (Vectors)\n",
    "All components mentioned below are fundamentally **vectors** (sets of numbers).\n",
    "\n",
    "| Term | Notation | Description/Role |\n",
    "| :--- | :--- | :--- |\n",
    "| **Previous Hidden State** | $H_{t-1}$ | The memory/context carried from the previous time step. |\n",
    "| **Current Hidden State** | $H_t$ | The calculated memory/context for the current time step. |\n",
    "| **Current Input** | $X_t$ | The vectorized input (e.g., a word) at time $t$. |\n",
    "| **Reset Gate** | $R_t$ | Controls which parts of $H_{t-1}$ should be forgotten/reset based on $X_t$. |\n",
    "| **Update Gate** | $Z_t$ | Determines the balance between the Past Memory ($H_{t-1}$) and the Candidate Memory ($\\tilde{H}_t$). |\n",
    "| **Candidate Hidden State** | $\\tilde{H}_t$ (or $H_t$ with tilde/bar) | The proposed new memory state, heavily influenced by $X_t$, before final balancing. |\n",
    "\n",
    "**Dimensionality Constraint:** Excluding $X_t$ (which can have a different dimension), the vectors $H_{t-1}$, $H_t$, $R_t$, $Z_t$, and $\\tilde{H}_t$ must **always have the same dimension**. This dimension is determined by the **number of hidden units/nodes** specified in the architecture (a hyperparameter).\n",
    "\n",
    "### C. Input Vectorization ($X_t$)\n",
    "*   Since the GRU cannot process words directly, the input text must be **converted into vectors** (numbers).\n",
    "*   Vectorization techniques include simple methods like Bag of Words (BOE) or more advanced methods like Word2Vec embeddings. $X_t$ is the resulting vector for the word/unit being processed at time $t$.\n",
    "\n",
    "### D. Operational Elements\n",
    "1.  **Neural Network Layers (Fully Connected Layers):** Represented by yellow boxes in diagrams.\n",
    "    *   The layers corresponding to the gates ($R_t, Z_t$) use the **Sigmoid** ($\\sigma$) activation function, ensuring outputs are between 0 and 1.\n",
    "    *   The layer calculating the Candidate Hidden State ($\\tilde{H}_t$) uses the **$\\tanh$** activation function.\n",
    "    *   The number of nodes must be **identical** across all three neural network layers.\n",
    "2.  **Element-wise Operations:** Represented by pink circles.\n",
    "    *   **Multiplication ($\\times$):** Point-wise multiplication (element-wise).\n",
    "    *   **Addition ($+$):** Point-wise addition (element-wise).\n",
    "    *   **One-Minus ($1-$):** Calculates $(1 - Z_t)$ for element-wise use.\n",
    "\n",
    "## III. The GRU Architecture: Conceptual Flow\n",
    "\n",
    "<img src=\"https://i.ibb.co/YB2TNnnc/image.png\">\n",
    "\n",
    "### A. The Big Idea: Single State Memory\n",
    "*   Unlike LSTMs, the GRU's \"big idea\" is that it does **not need two separate states** (cell state and hidden state) to carry long-term and short-term context.\n",
    "*   It uses a **single Hidden State** ($H$) to manage both types of context through careful manipulation by its two gates.\n",
    "*   **Intuition:** The Hidden State acts as the **memory of the system**, where different vector dimensions represent different aspects of the context (e.g., power, conflict, tragedy, or revenge in a story).\n",
    "\n",
    "### B. Two-Step Transition Process\n",
    "The transition from Past Memory ($H_{t-1}$) to Current Memory ($H_t$) occurs in two steps:\n",
    "\n",
    "1.  **Step 1: Creation of Candidate Memory ($\\tilde{H}_t$)**\n",
    "    *   The past memory ($H_{t-1}$) is combined with the current input ($X_t$) to create a $\\tilde{H}_t$ (Candidate Hidden State).\n",
    "    *   This step uses the **Reset Gate ($R_t$)** to modulate (reset or reduce) irrelevant parts of $H_{t-1}$ based on $X_t$.\n",
    "2.  **Step 2: Balancing and Final Output ($H_t$)**\n",
    "    *   The GRU cannot directly use $\\tilde{H}_t$ as $H_t$ because $\\tilde{H}_t$ is often **too heavily reliant** on the current input ($X_t$).\n",
    "    *   The **Update Gate ($Z_t$)** is used to dynamically balance the Past Memory ($H_{t-1}$) and the Candidate Memory ($\\tilde{H}_t$).\n",
    "    *   The resulting balance forms the final $H_t$.\n",
    "\n",
    "\n",
    "## IV. Detailed Architecture: Calculation Steps\n",
    "\n",
    "The calculation of $H_t$ involves four chronological steps:\n",
    "\n",
    "### Step 1: Calculate Reset Gate ($R_t$)\n",
    "*   **Purpose:** To determine which dimensions of the *past memory* ($H_{t-1}$) should be substantially forgotten (reset) based on the significance of the current input ($X_t$).\n",
    "*   **Calculation:** $H_{t-1}$ and $X_t$ are concatenated and passed through a Sigmoid activation layer.\n",
    "    $$R_t = \\sigma(W_R \\cdot [H_{t-1}, X_t] + B_R)$$\n",
    "\n",
    "### Step 2: Calculate Candidate Hidden State ($\\tilde{H}_t$)\n",
    "This step involves two sub-operations:\n",
    "1.  **Modulation of Past Memory:** The Reset Gate $R_t$ performs an element-wise multiplication with $H_{t-1}$ to create the **Modulated/Resetted Past Memory**:\n",
    "    $$R_t \\odot H_{t-1}$$\n",
    "2.  **Proposal Generation:** The modulated memory is concatenated with the Current Input $X_t$ and passed through a $\\tanh$ activation layer. This proposes the new memory state $\\tilde{H}_t$.\n",
    "    $$\\tilde{H}_t = \\tanh(W_C \\cdot [ (R_t \\odot H_{t-1}), X_t] + B_C)$$\n",
    "\n",
    "### Step 3: Calculate Update Gate ($Z_t$)\n",
    "*   **Purpose:** To determine the precise **balance** (weighting) to be applied to the Past Memory ($H_{t-1}$) versus the Candidate Memory ($\\tilde{H}_t$).\n",
    "*   **Calculation:** $H_{t-1}$ and $X_t$ are concatenated and passed through a Sigmoid activation layer.\n",
    "    $$Z_t = \\sigma(W_Z \\cdot [H_{t-1}, X_t] + B_Z)$$\n",
    "\n",
    "### Step 4: Calculate Current Hidden State ($H_t$)\n",
    "*   **Purpose:** To combine the Past Memory and Candidate Memory using the balance determined by $Z_t$.\n",
    "*   **Mechanism:** If $Z_t$ is high, more weight is given to the new Candidate Memory ($\\tilde{H}_t$); if $Z_t$ is low, more weight is given to the Past Memory ($H_{t-1}$).\n",
    "*   **Calculation:** This uses the $1-Z_t$ operation and element-wise addition and multiplication.\n",
    "    $$H_t = (1 - Z_t) \\odot H_{t-1} + Z_t \\odot \\tilde{H}_t$$\n",
    "\n",
    "## V. Key Differences Between LSTMs and GRUs\n",
    "\n",
    "| Feature | LSTM (Long Short-Term Memory) | GRU (Gated Recurrent Unit) | Citation |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Number of Gates** | Three (Input, Forget, Output). | Two (Reset, Update). | |\n",
    "| **Memory Units** | Two separate states: Cell State and Hidden State. | One single state: Hidden State only. | |\n",
    "| **Architecture Complexity**| Complex. | Simpler. | |\n",
    "| **Parameter Count** | Higher. The formula contains four $W$ weights. | Lower. The formula contains three $W$ weights. | |\n",
    "| **Computational Speed** | Higher computational complexity; generally slower. | Faster to compute, especially on smaller datasets or when resources are limited. | |\n",
    "| **Performance** | Better on many tasks, especially more complex tasks and larger datasets. | Can perform comparably to LSTMs; often train faster due to fewer parameters. | |\n",
    "| **Usage Recommendation**| Used when empirical testing shows better results, or when tasks are complex. | Often the **first choice** when starting out due to simplicity. | |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl_py_9)",
   "language": "python",
   "name": "dl_py_9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
