{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "341a2c50-4b28-485d-a757-5cd687950927",
   "metadata": {},
   "source": [
    "# Bidirectional Recurrent Neural Networks (BiRNNs)\n",
    "\n",
    "## I. Context and Motivation\n",
    "\n",
    "### A. Background (Unidirectional RNNs)\n",
    "*   Previous RNN architectures studied include the **Vanilla RNN**, **Long Short-Term Memory (LSTM)**, and **Gated Recurrent Units (GRU)**, along with the concept of **Deep RNNs** (stacked cells).\n",
    "*   Standard RNNs (including LSTMs and GRUs) are **unidirectional**.\n",
    "*   In a standard RNN design, the information flows in **one direction** (e.g., left to right).\n",
    "*   The final output ($\\hat{Y}$) at any time step depends on the **past inputs** ($X_1, X_2, ..., X_t$).\n",
    "\n",
    "### B. The Failure Scenario\n",
    "*   Standard unidirectional designs **fail** when a situation requires that **future inputs affect past outputs**.\n",
    "*   It might seem counter-intuitive that a future event could affect a past output, but this occurs frequently in sequence processing, especially Natural Language Processing (NLP).\n",
    "\n",
    "### C. Example: Named Entity Recognition (NER)\n",
    "*   NER is an NLP task where entities (like Person, Location, or Organization) are extracted from a sentence.\n",
    "*   **Ambiguity Problem:** Consider two sentences:\n",
    "    1.  \"I love **Amazon**. It's a great **website**.\"\n",
    "    2.  \"I love **Amazon**. It's a beautiful **river**.\"\n",
    "*   If a standard RNN processes the first sentence word by word (left to right), when it reaches the word \"**Amazon**,\" it faces ambiguity. Amazon could be an **Organization (ORG)** or a **Location (LOC)** (due to the Amazon River).\n",
    "*   The RNN **cannot determine** the correct entity type until it reads the **future context** (e.g., the word \"website\" or \"river\").\n",
    "*   This is a scenario where knowledge of the **future input** is required to correctly classify the **current output**.\n",
    "\n",
    "## II. Architecture and Operation\n",
    "\n",
    "### A. The Core Idea\n",
    "*   A BiRNN resolves the dependency on future context by processing the input from **both sides**.\n",
    "*   A BiRNN uses **two different, separate RNNs**:\n",
    "    1.  **Forward RNN:** Reads the sentence from **Left to Right** (आगे से पढ़ना शुरू करता है).\n",
    "    2.  **Backward RNN:** Reads the sentence from **Right to Left** (पीछे से पढ़ना शुरू करता है).\n",
    "\n",
    "### B. Processing Flow\n",
    "1.  **Forward Pass:** The forward RNN processes inputs sequentially ($X_{i1}, X_{i2}, X_{i3}, X_{i4}$). It propagates its hidden state in the typical direction.\n",
    "2.  **Backward Pass:** The backward RNN receives input in reverse order (e.g., $X_{i4}, X_{i3}, X_{i2}, X_{i1}$). It propagates its hidden state backward (moving from right to left).\n",
    "3.  **Concatenation:** At every time step ($t$), the outputs (Hidden States) from both the forward and backward RNNs are **concatenated (जोड़ देंगे)**. This combined output is used to calculate the final prediction ($Y_t$).\n",
    "\n",
    "### C. Resolution of Ambiguity\n",
    "*   Because of this bidirectional flow, calculating the output ($Y_t$) for an early word (like \"Amazon\" at $t=1$) includes contributions from the input words that appear later in the sequence (\"website\").\n",
    "*   This provides the necessary context to determine whether \"Amazon\" is an organization or a location.\n",
    "\n",
    "## III. Mathematical Formulation\n",
    "\n",
    "A BiRNN requires separate calculations for the forward and backward hidden states and then combines them.\n",
    "\n",
    "### A. Forward Hidden State ($\\vec{H}_t$)\n",
    "*   This follows the normal RNN equation, depending on the previous time step ($t-1$):\n",
    "    $$\\vec{H}_t = \\tanh(W^F \\cdot \\vec{H}_{t-1} + U \\cdot X_t + B^F)$$\n",
    "\n",
    "### B. Backward Hidden State ($\\overleftarrow{H}_t$)\n",
    "*   This calculation is unique because the input affecting $t$ comes from $t+1$:\n",
    "    $$\\overleftarrow{H}_t = \\tanh(W^B \\cdot \\overleftarrow{H}_{t+1} + U \\cdot X_t + B^B)$$\n",
    "    *   *Note:* $W^F$ (forward weight) and $W^B$ (backward weight) are different.\n",
    "\n",
    "### C. Final Output ($Y_t$)\n",
    "*   The final output uses a sigmoid layer over the concatenation of the two calculated hidden states:\n",
    "    $$Y_t = \\sigma(V \\cdot [\\vec{H}_t, \\overleftarrow{H}_t] + B_{\\text{out}})$$\n",
    "\n",
    "## IV. Implementation and Applicability\n",
    "\n",
    "### A. Extension to Gated Architectures\n",
    "*   The concept of bidirectional traversal is **applicable to any type of RNN cell**, similar to how the Deep RNN concept applies to all cell types.\n",
    "*   While the concept is called BiRNN, it is most frequently applied to LSTMs or GRUs.\n",
    "    *   **BiLSTM** (Bidirectional LSTM)\n",
    "    *   **BiGRU** (Bidirectional GRU)\n",
    "\n",
    "### B. Implementation (Keras)\n",
    "*   Keras provides a **Bidirectional wrapper** that allows the easy creation of a BiRNN, BiLSTM, or BiGRU.\n",
    "*   The user wraps the chosen RNN layer (e.g., Simple RNN, LSTM, or GRU) inside the `Bidirectional` wrapper.\n",
    "\n",
    "### C. Parameter Count\n",
    "*   Because a BiRNN uses **two separate RNNs** (forward and backward), the total number of weights and biases in the bidirectional layer **doubles** compared to a simple unidirectional layer.\n",
    "\n",
    "## V. Application Areas\n",
    "\n",
    "BiRNNs generally provide good results in applications where context from both sides of the sequence is valuable:\n",
    "\n",
    "1.  **Named Entity Recognition (NER):** Requires future context to resolve current word ambiguity.\n",
    "2.  **Parts of Speech (POS) Tagging:** Determining the grammatical category of a word.\n",
    "3.  **Machine Translation:** Translation-based applications.\n",
    "4.  **Sentiment Analysis:** Can outperform normal RNNs in performance.\n",
    "5.  **Time Series Forecasting:** Used in problems like stock price or weather prediction, where analyzing data forward and backward can help.\n",
    "\n",
    "## VI. Drawbacks\n",
    "\n",
    "Using BiRNNs introduces several complications and trade-offs:\n",
    "\n",
    "1.  **Increased Complexity and Overfitting:** The number of weights and biases **doubles**. This increases the complexity and the risk of **overfitting**. Techniques like **dropout** and **regularization** must be used to mitigate overfitting.\n",
    "2.  **Increased Training Time:** Due to the higher number of parameters, the training time increases, especially with large datasets.\n",
    "3.  **Real-time Latency Issues:** BiRNNs require the **entire data** to be available before processing can begin (to allow the backward pass to start from the end).\n",
    "    *   In real-time applications (e.g., real-time speech recognition), this results in **latency issues** (slow response) because the system must wait for the user to finish speaking the entire sentence before applying the BiRNN.\n",
    "    *   If the data is not available in one go, BiRNNs are not suitable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl_py_9)",
   "language": "python",
   "name": "dl_py_9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
