{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c04437-c03a-4782-baa0-8acb6a217c58",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs): Handling Sequential Data\n",
    "\n",
    "### 1. Introduction and Context\n",
    "\n",
    "RNNs represent the **third class of neural networks** studied, following Artificial Neural Networks (ANNs) and Convolutional Neural Networks (CNNs).\n",
    "\n",
    "*   **Definition:** RNNs (Recurrent Neural Networks) are a **special type of sequential model** specifically designed to work on **sequential data**.\n",
    "*   **Primary Use Cases:** They are heavily utilized for sequence-related tasks and in the area of **Natural Language Processing (NLP)**.\n",
    "\n",
    "| Neural Network Type | Typical Data Type |\n",
    "| :--- | :--- |\n",
    "| **ANNs** (Artificial Neural Networks) | Tabular data |\n",
    "| **CNNs** (Convolutional Neural Networks) | Grid-like data, images, or videos |\n",
    "| **RNNs** (Recurrent Neural Networks) | Sequential data |\n",
    "\n",
    "### 2. Understanding Sequential Data\n",
    "\n",
    "Sequential data is data where the **sequence matters**. The context of past elements is retained to understand the meaning of the future elements.\n",
    "\n",
    "**Examples of Sequential Data:**\n",
    "\n",
    "1.  **Text/Language:** When reading a sentence (e.g., \"Hey my name is Nitish\"), words follow each other sequentially, and the order is crucial for semantic meaning.\n",
    "2.  **Time Series Data:** Graphs showing data over time (e.g., stock market data from 2001, 2002, 2003) where past trends determine future movements.\n",
    "3.  **Audio:** Waveforms generated from spoken words.\n",
    "4.  **Biological Data:** DNA sequences.\n",
    "\n",
    "The ability of RNNs to model sequential data is often cited as the starting point of a revolution in NLP.\n",
    "\n",
    "### 3. Why ANNs Fail on Sequential Data\n",
    "\n",
    "The primary reason RNNs are necessary is that ANNs cannot effectively handle the characteristics of sequential data, particularly text.\n",
    "\n",
    "#### A. Problem of Varying Input Size\n",
    "\n",
    "Sequential data, like sentences, rarely has a uniform length (i.e., the number of words varies).\n",
    "\n",
    "*   **The Conflict:** Standard ANNs are fixed-size models. You cannot use a network where the input size (e.g., the number of words input) can be different for every example.\n",
    "*   **Attempted Workaround: Zero Padding:** To fix the varying size problem, one might find the maximum number of words in any sentence and pad all shorter sentences with **zero vectors**.\n",
    "    *   *Example:* If the maximum sentence has 5 words, a 3-word sentence is padded with two additional zero vectors.\n",
    "*   **The Flaws of Zero Padding:**\n",
    "    1.  **Inefficiency and Computation:** If the vocabulary is large (e.g., 10,000 words) and the maximum sentence length is large (e.g., 100 words), zero padding results in **unnecessary computation** and dramatically increases the number of weights (e.g., input shape of 10 lakh leading to 1 crore weights).\n",
    "    2.  **Prediction Instability:** If a user inputs a text longer than the maximum length used during training (e.g., 200 words input when the model was trained for a maximum of 100), prediction problems will occur.\n",
    "\n",
    "#### B. Failure to Capture Sequence and Semantic Meaning (The Biggest Issue)\n",
    "\n",
    "ANNs are fundamentally incapable of understanding context because they process input words all at once.\n",
    "\n",
    "*   **Loss of Information:** When all words enter the network simultaneously, the network **loses information about the sequence** (which word came first and which came later).\n",
    "*   **Lack of Memory:** ANNs, by design, **do not have the capability to retain memory** of past inputs.\n",
    "*   This inability to remember the sequence prevents ANNs from capturing the **semantic meaning** or hidden context within the text.\n",
    "*   Because ANNs cannot capture the required sequential information, their accuracy would be poor on these tasks, necessitating the creation of RNNs.\n",
    "\n",
    "### 4. Applications and Use Cases of RNNs\n",
    "\n",
    "RNNs are used in many modern, commercially relevant applications.\n",
    "\n",
    "| Application | Description |\n",
    "| :--- | :--- |\n",
    "| **Sentiment Analysis** | Receiving a piece of text (e.g., a review) and classifying its sentiment as positive or negative. E-commerce companies use this to analyze product reviews. |\n",
    "| **Sentence Completion / Next Word Prediction** | Automatically suggesting the subsequent words or sentences as a user types (e.g., features seen in Gmail or phone keypads). |\n",
    "| **Image Caption Generator** | Uploading an image and receiving an accurate descriptive text (a useful, \"magical\" application, for instance, in creating constant textual commentary for the visually impaired using a camera feed). |\n",
    "| **Machine Translation** | Converting text from one language to another (e.g., Google Translate), often involving language detection. This is a \"big deal\" for international communication. |\n",
    "| **Question and Answering (Q&A) Systems** | Receiving a large paragraph of text and then being able to answer specific, related questions about that text. The concepts are derived from RNNs, though advanced versions may use models like BERT. Useful in complex fields like medicine. |\n",
    "| **Time Series Forecasting and Speech Classification** | Other broader applications of RNNs. |\n",
    "\n",
    "### 5. Roadmap for Future RNN Studies\n",
    "\n",
    "The subsequent study of RNNs will follow a structured path:\n",
    "\n",
    "1.  **Simple RNN:** Discussing the simplest RNN architecture, coding it in Keras, and examining small examples.\n",
    "2.  **Backpropagation in Time:** Studying how backpropagation works specifically within the RNN architecture.\n",
    "3.  **RNN Problems:** Analyzing common issues like **Vanishing Gradient** and **Exploding Gradient**.\n",
    "4.  **Advanced Architectures:** To solve the standard RNN problems, the course will cover **LSTM (Long Short-Term Memory)** and **GRU (Gated Recurrent Unit)**.\n",
    "5.  **Types of RNN Architectures:** Studying different structural types of RNNs.\n",
    "6.  **Deep RNNs:** Discussing models where multiple layers are stacked.\n",
    "7.  **Bidirectional RNNs:** The final advanced topic in the series."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl_py_9)",
   "language": "python",
   "name": "dl_py_9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
